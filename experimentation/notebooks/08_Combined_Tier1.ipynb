{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Combined Tier 1 Paradigms: Deep Research Agent\n",
    "\n",
    "This notebook implements the **Combined Tier 1 Architecture** that unifies four paradigms:\n",
    "\n",
    "1. **Cascading Knowledge Cache** - Global search layer wrapping all operations\n",
    "2. **Agile Sprints** - Information gathering with retrospectives\n",
    "3. **Iterative Refinement V2** - Skeleton-based document generation with patches\n",
    "4. **Quality Gates** - Strategic checkpoints for quality assurance\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "The architecture follows a **funnel pattern**:\n",
    "```\n",
    "Wide Research → Structured Synthesis → Focused Refinement → Polished Output\n",
    "```\n",
    "\n",
    "### Five Phases:\n",
    "1. **Phase 1: Agile Research Sprints** - Comprehensive information gathering\n",
    "2. **Phase 2: Skeleton Generation** - Create document structure\n",
    "3. **Phase 3: Node Expansion** - Generate prose per section\n",
    "4. **Phase 4: Verification & Refinement** - Quality gates and patching\n",
    "5. **Phase 5: Final Assembly** - Compile polished report\n",
    "\n",
    "## Technology Stack\n",
    "- **LLM**: gpt-5-mini-2025-08-07\n",
    "- **Web Search**: Tavily API\n",
    "- **Embeddings**: OpenAI text-embedding-3-small\n",
    "- **Tracing**: LangSmith\n",
    "- **Framework**: LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "import asyncio\n",
    "import hashlib\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Annotated, TypedDict, Literal, Optional, Any\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from tavily import TavilyClient\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables\n",
    "env_path = Path(\"../.env\")\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Configure LangSmith tracing\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"deep_research_new\"\n",
    "\n",
    "print(\"Environment configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM, Tavily, and Embeddings\n",
    "MODEL_NAME = \"gpt-5-mini-2025-08-07\"\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0, max_retries=10)\n",
    "tavily_client = TavilyClient()\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# ===== CONFIGURATION PARAMETERS =====\n",
    "\n",
    "# Research Phase (Agile Sprints)\n",
    "MAX_RESEARCH_SPRINTS = 3\n",
    "QUERIES_PER_SPRINT = 5\n",
    "MIN_SOURCES_FOR_GATE1 = 15\n",
    "MIN_DOMAINS_FOR_GATE1 = 5\n",
    "\n",
    "# Skeleton Generation\n",
    "TARGET_WORDS_PER_NODE = 300\n",
    "MIN_SECTIONS = 5\n",
    "MAX_SECTIONS = 8\n",
    "\n",
    "# Knowledge Cache\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "HIGH_CONFIDENCE_THRESHOLD = 0.75\n",
    "LOW_CONFIDENCE_THRESHOLD = 0.40\n",
    "SPECIFICITY_ADJUSTMENT = 0.2\n",
    "TOP_K_RETRIEVAL = 5\n",
    "\n",
    "# Verification & Refinement\n",
    "MAX_REFINEMENT_ITERATIONS = 2\n",
    "QUALITY_THRESHOLD = 7.5\n",
    "MIN_EVIDENCE_SCORE = 6\n",
    "MAX_CASCADES_PER_ITERATION = 5\n",
    "\n",
    "# Token Management\n",
    "MAX_CONTEXT_CHARS = 12000\n",
    "MAX_FINDINGS_CHARS = 10000\n",
    "\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "print(f\"Research: max {MAX_RESEARCH_SPRINTS} sprints, {QUERIES_PER_SPRINT} queries/sprint\")\n",
    "print(f\"Refinement: max {MAX_REFINEMENT_ITERATIONS} iterations, threshold {QUALITY_THRESHOLD}/10\")\n",
    "print(f\"Cache: HIGH >= {HIGH_CONFIDENCE_THRESHOLD}, LOW < {LOW_CONFIDENCE_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2mhycmivay",
   "metadata": {},
   "source": [
    "## 2. Knowledge Cache Implementation\n",
    "\n",
    "The Knowledge Cache wraps ALL search operations throughout the agent, providing:\n",
    "- **Layer 1**: Deterministic deduplication (exact query match)\n",
    "- **Layer 2**: Semantic similarity retrieval (vector search)\n",
    "- **Layer 3**: LLM-augmented judgment (gap analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wejmds4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Knowledge Cache Data Models =====\n",
    "\n",
    "class CachedDocument(BaseModel):\n",
    "    \"\"\"A cached web document.\"\"\"\n",
    "    url: str\n",
    "    normalized_url: str\n",
    "    content: str\n",
    "    content_hash: str\n",
    "    title: str = \"\"\n",
    "    retrieval_timestamp: str\n",
    "    source_query: str = \"\"\n",
    "\n",
    "\n",
    "class CachedChunk(BaseModel):\n",
    "    \"\"\"A chunk of content with embedding.\"\"\"\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    embedding: List[float]\n",
    "    source_url: str\n",
    "    position: int\n",
    "\n",
    "\n",
    "class CacheDecision(BaseModel):\n",
    "    \"\"\"Record of a cache decision for observability.\"\"\"\n",
    "    query: str\n",
    "    layer_reached: Literal[\"L1\", \"L2\", \"L3\"]\n",
    "    decision: str\n",
    "    confidence_score: float = 0.0\n",
    "    action_taken: Literal[\"USE_CACHE\", \"SEARCH\", \"TARGETED_SEARCH\"]\n",
    "    reasoning: str = \"\"\n",
    "    timestamp: str = \"\"\n",
    "\n",
    "\n",
    "class KnowledgeBase:\n",
    "    \"\"\"Session-scoped knowledge base with cascading cache capabilities.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.url_registry: Dict[str, CachedDocument] = {}\n",
    "        self.query_cache: Dict[str, dict] = {}\n",
    "        self.chunks: List[CachedChunk] = []\n",
    "        self.chunk_embeddings: Optional[np.ndarray] = None\n",
    "        self.stats = {\n",
    "            \"total_queries\": 0, \"l1_hits\": 0, \"l2_high\": 0, \"l2_medium\": 0,\n",
    "            \"l2_low\": 0, \"l3_sufficient\": 0, \"l3_partial\": 0, \"l3_insufficient\": 0,\n",
    "            \"web_searches_executed\": 0, \"web_searches_avoided\": 0\n",
    "        }\n",
    "\n",
    "    def normalize_url(self, url: str) -> str:\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            host = parsed.netloc.lower()\n",
    "            if host.startswith(\"www.\"):\n",
    "                host = host[4:]\n",
    "            path = parsed.path.rstrip(\"/\")\n",
    "            return f\"https://{host}{path}\"\n",
    "        except:\n",
    "            return url.lower()\n",
    "\n",
    "    def normalize_query_light(self, query: str) -> str:\n",
    "        return \" \".join(query.lower().split())\n",
    "\n",
    "    def normalize_query_aggressive(self, query: str) -> str:\n",
    "        stop_words = {\"the\", \"a\", \"an\", \"is\", \"are\", \"of\", \"in\", \"to\", \"for\", \"and\", \"or\", \"what\", \"how\", \"why\"}\n",
    "        light = self.normalize_query_light(query)\n",
    "        terms = [t for t in light.split() if t not in stop_words and len(t) > 1]\n",
    "        return \" \".join(sorted(terms))\n",
    "\n",
    "    def compute_content_hash(self, content: str) -> str:\n",
    "        return hashlib.sha256(content.encode()).hexdigest()\n",
    "\n",
    "    def add_document(self, url: str, content: str, title: str = \"\", source_query: str = \"\"):\n",
    "        normalized_url = self.normalize_url(url)\n",
    "        doc = CachedDocument(\n",
    "            url=url, normalized_url=normalized_url, content=content,\n",
    "            content_hash=self.compute_content_hash(content), title=title,\n",
    "            retrieval_timestamp=datetime.now().isoformat(), source_query=source_query\n",
    "        )\n",
    "        self.url_registry[normalized_url] = doc\n",
    "        self._chunk_and_embed(doc)\n",
    "        return doc\n",
    "\n",
    "    def _chunk_and_embed(self, doc: CachedDocument):\n",
    "        content = doc.content\n",
    "        chunks_text = []\n",
    "        for i in range(0, len(content), CHUNK_SIZE - CHUNK_OVERLAP):\n",
    "            chunk_text = content[i:i + CHUNK_SIZE]\n",
    "            if len(chunk_text) > 50:\n",
    "                chunks_text.append(chunk_text)\n",
    "        if not chunks_text:\n",
    "            return\n",
    "        embeddings = embeddings_model.embed_documents(chunks_text)\n",
    "        for i, (text, embedding) in enumerate(zip(chunks_text, embeddings)):\n",
    "            chunk = CachedChunk(\n",
    "                chunk_id=f\"{doc.content_hash[:8]}_{i}\", text=text,\n",
    "                embedding=embedding, source_url=doc.url, position=i\n",
    "            )\n",
    "            self.chunks.append(chunk)\n",
    "        self._update_embedding_matrix()\n",
    "\n",
    "    def _update_embedding_matrix(self):\n",
    "        if self.chunks:\n",
    "            self.chunk_embeddings = np.array([c.embedding for c in self.chunks])\n",
    "\n",
    "    def add_query(self, query: str, result_urls: List[str], result_summary: str):\n",
    "        entry = {\n",
    "            \"original_query\": query, \"light_normalized\": self.normalize_query_light(query),\n",
    "            \"aggressive_normalized\": self.normalize_query_aggressive(query),\n",
    "            \"timestamp\": datetime.now().isoformat(), \"result_urls\": result_urls,\n",
    "            \"result_summary\": result_summary\n",
    "        }\n",
    "        self.query_cache[entry[\"light_normalized\"]] = entry\n",
    "        self.query_cache[entry[\"aggressive_normalized\"]] = entry\n",
    "        return entry\n",
    "\n",
    "    def lookup_query_exact(self, query: str) -> Optional[dict]:\n",
    "        return self.query_cache.get(self.normalize_query_light(query))\n",
    "\n",
    "    def semantic_search(self, query: str, top_k: int = TOP_K_RETRIEVAL) -> List[Tuple[CachedChunk, float]]:\n",
    "        if not self.chunks or self.chunk_embeddings is None:\n",
    "            return []\n",
    "        query_embedding = np.array(embeddings_model.embed_query(query))\n",
    "        similarities = np.dot(self.chunk_embeddings, query_embedding) / (\n",
    "            np.linalg.norm(self.chunk_embeddings, axis=1) * np.linalg.norm(query_embedding) + 1e-8\n",
    "        )\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        return [(self.chunks[idx], float(similarities[idx])) for idx in top_indices]\n",
    "\n",
    "    def get_stats_summary(self) -> str:\n",
    "        total = self.stats[\"total_queries\"]\n",
    "        if total == 0:\n",
    "            return \"No queries processed yet.\"\n",
    "        avoided = self.stats[\"web_searches_avoided\"]\n",
    "        hit_rate = avoided / total * 100 if total > 0 else 0\n",
    "        return f\"Total: {total} queries, {avoided} avoided ({hit_rate:.1f}% hit rate), {len(self.chunks)} chunks cached\"\n",
    "\n",
    "\n",
    "# Initialize global knowledge base\n",
    "knowledge_base = KnowledgeBase()\n",
    "print(\"Knowledge base initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t6srgqg8yfh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cascaded Search Function =====\n",
    "\n",
    "def search_web(query: str, max_results: int = 8) -> Tuple[str, List[str], List[str]]:\n",
    "    \"\"\"Execute web search using Tavily. Returns (summary, results, urls).\"\"\"\n",
    "    try:\n",
    "        if len(query) > 400:\n",
    "            query = query[:400]\n",
    "        response = tavily_client.search(query=query, max_results=max_results, include_answer=True)\n",
    "        results = []\n",
    "        urls = []\n",
    "        summary = response.get(\"answer\", \"\")\n",
    "        for r in response.get(\"results\", []):\n",
    "            url = r.get('url', '')\n",
    "            urls.append(url)\n",
    "            content = r.get('content', '')[:500]\n",
    "            title = r.get('title', 'No title')\n",
    "            results.append(f\"[{title}] {content}... (Source: {url})\")\n",
    "        return summary, results, urls\n",
    "    except Exception as e:\n",
    "        return f\"Search error: {str(e)}\", [], []\n",
    "\n",
    "\n",
    "def compute_confidence(top_results: List[Tuple[CachedChunk, float]], query: str) -> float:\n",
    "    \"\"\"Compute multi-signal confidence score.\"\"\"\n",
    "    if not top_results:\n",
    "        return 0.0\n",
    "    top_score = top_results[0][1]\n",
    "    score_gap = top_results[0][1] - top_results[1][1] if len(top_results) > 1 else top_score\n",
    "    query_terms = set(query.lower().split())\n",
    "    top_chunk_terms = set(top_results[0][0].text.lower().split())\n",
    "    term_overlap = len(query_terms & top_chunk_terms) / len(query_terms | top_chunk_terms) if query_terms | top_chunk_terms else 0\n",
    "    return 0.5 * top_score + 0.25 * min(score_gap * 2, 1.0) + 0.25 * term_overlap\n",
    "\n",
    "\n",
    "async def cascaded_search(query: str, kb: KnowledgeBase) -> Tuple[str, List[str], CacheDecision]:\n",
    "    \"\"\"Execute full cascading cache check and search if needed.\"\"\"\n",
    "    kb.stats[\"total_queries\"] += 1\n",
    "    timestamp = datetime.now().isoformat()\n",
    "\n",
    "    # Layer 1: Exact match\n",
    "    exact_match = kb.lookup_query_exact(query)\n",
    "    if exact_match:\n",
    "        kb.stats[\"l1_hits\"] += 1\n",
    "        kb.stats[\"web_searches_avoided\"] += 1\n",
    "        return exact_match[\"result_summary\"], exact_match[\"result_urls\"], CacheDecision(\n",
    "            query=query, layer_reached=\"L1\", decision=\"HIT\", confidence_score=1.0,\n",
    "            action_taken=\"USE_CACHE\", reasoning=\"Exact query match\", timestamp=timestamp\n",
    "        )\n",
    "\n",
    "    # Layer 2: Semantic search\n",
    "    results = kb.semantic_search(query)\n",
    "    if results:\n",
    "        confidence = compute_confidence(results, query)\n",
    "        if confidence >= HIGH_CONFIDENCE_THRESHOLD:\n",
    "            kb.stats[\"l2_high\"] += 1\n",
    "            kb.stats[\"web_searches_avoided\"] += 1\n",
    "            content = \"\\n\\n\".join([f\"[From: {c.source_url}]\\n{c.text}\" for c, _ in results[:3]])\n",
    "            urls = list(set([c.source_url for c, _ in results]))\n",
    "            return content, urls, CacheDecision(\n",
    "                query=query, layer_reached=\"L2\", decision=\"HIGH_CONF\", confidence_score=confidence,\n",
    "                action_taken=\"USE_CACHE\", reasoning=f\"High semantic similarity ({confidence:.2f})\", timestamp=timestamp\n",
    "            )\n",
    "        elif confidence >= LOW_CONFIDENCE_THRESHOLD:\n",
    "            kb.stats[\"l2_medium\"] += 1\n",
    "            # For medium confidence, still do a search but could use cached context\n",
    "        else:\n",
    "            kb.stats[\"l2_low\"] += 1\n",
    "\n",
    "    # Execute web search\n",
    "    summary, search_results, urls = search_web(query)\n",
    "    kb.stats[\"web_searches_executed\"] += 1\n",
    "\n",
    "    # Cache results\n",
    "    query_content = f\"Query: {query}\\n\\nAnswer: {summary}\\n\\nResults:\\n\" + \"\\n\\n\".join(search_results)\n",
    "    synthetic_url = f\"search://{kb.compute_content_hash(query)[:16]}\"\n",
    "    kb.add_document(synthetic_url, query_content, title=f\"Search: {query[:50]}\", source_query=query)\n",
    "    kb.add_query(query, urls, summary)\n",
    "\n",
    "    return query_content, urls, CacheDecision(\n",
    "        query=query, layer_reached=\"L2\", decision=\"LOW_CONF\",\n",
    "        confidence_score=compute_confidence(results, query) if results else 0.0,\n",
    "        action_taken=\"SEARCH\", reasoning=\"Executed web search\", timestamp=timestamp\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Cascaded search function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecg0xqpp7qm",
   "metadata": {},
   "source": [
    "## 3. State Definition and Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4syemwmgu7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Skeleton and Prose Models =====\n",
    "\n",
    "class SkeletonNode(BaseModel):\n",
    "    \"\"\"A node in the document skeleton hierarchy.\"\"\"\n",
    "    node_id: str = Field(description=\"Unique identifier like 'sec:intro'\")\n",
    "    title: str = Field(description=\"Section title\")\n",
    "    intent: str = Field(description=\"1-3 sentence description of purpose\")\n",
    "    target_word_count: int = Field(default=300)\n",
    "    dependencies: List[str] = Field(default_factory=list)\n",
    "    children: List[str] = Field(default_factory=list)\n",
    "    is_expanded: bool = Field(default=False)\n",
    "\n",
    "\n",
    "class SkeletonGenerationOutput(BaseModel):\n",
    "    \"\"\"Output schema for skeleton generation.\"\"\"\n",
    "    thesis: str = Field(description=\"One-sentence thesis statement\")\n",
    "    sections: List[SkeletonNode] = Field(description=\"All sections in document order\")\n",
    "\n",
    "\n",
    "class ProseGenerationOutput(BaseModel):\n",
    "    \"\"\"Output schema for prose generation.\"\"\"\n",
    "    bridge_in: str = Field(description=\"Transitional sentences from previous section\")\n",
    "    main_content: str = Field(description=\"Main prose content\")\n",
    "    bridge_out: str = Field(description=\"Transitional sentences to next section\")\n",
    "    summary: str = Field(description=\"1-2 sentence summary\")\n",
    "\n",
    "\n",
    "class Claim(BaseModel):\n",
    "    \"\"\"A verifiable assertion in the document.\"\"\"\n",
    "    claim_id: str\n",
    "    claim_text: str\n",
    "    source_node: str\n",
    "    verification_status: Literal[\"unverified\", \"verified\", \"contested\"] = \"unverified\"\n",
    "    supporting_evidence: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class ClaimExtractionOutput(BaseModel):\n",
    "    \"\"\"Output for claim extraction.\"\"\"\n",
    "    claims: List[Claim]\n",
    "\n",
    "\n",
    "class CritiqueIssue(BaseModel):\n",
    "    \"\"\"An issue identified during critique.\"\"\"\n",
    "    issue_id: str\n",
    "    scope: Literal[\"global\", \"section\", \"transition\"]\n",
    "    target_nodes: List[str]\n",
    "    issue_type: str\n",
    "    severity: Literal[\"critical\", \"major\", \"minor\"]\n",
    "    description: str\n",
    "    suggestion: str\n",
    "    search_query: str = \"\"\n",
    "\n",
    "\n",
    "class CritiqueResult(BaseModel):\n",
    "    \"\"\"Complete critique output.\"\"\"\n",
    "    overall_quality: float = Field(description=\"Quality score 1-10\")\n",
    "    issues: List[CritiqueIssue] = Field(default_factory=list)\n",
    "    summary: str\n",
    "\n",
    "\n",
    "print(\"Data models defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff6wt0f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Combined Tier 1 State =====\n",
    "\n",
    "class CombinedTier1State(TypedDict):\n",
    "    \"\"\"State for the Combined Tier 1 Deep Research Agent.\"\"\"\n",
    "    # Input\n",
    "    question: str\n",
    "\n",
    "    # ===== PHASE 1: Research Sprints =====\n",
    "    research_backlog: List[str]  # REPLACED each sprint\n",
    "    current_research_sprint: int\n",
    "    max_research_sprints: int\n",
    "    sprint_findings: Annotated[List[str], operator.add]  # ACCUMULATED\n",
    "    research_source_urls: Annotated[List[str], operator.add]\n",
    "    research_retrospective_notes: Annotated[List[str], operator.add]\n",
    "    research_summary: str\n",
    "    research_complete: bool\n",
    "\n",
    "    # ===== PHASE 2: Skeleton =====\n",
    "    skeleton: Dict[str, Any]\n",
    "    skeleton_validated: bool\n",
    "\n",
    "    # ===== PHASE 3: Node Expansion =====\n",
    "    prose_store: Dict[str, Dict[str, Any]]\n",
    "    claims_registry: Dict[str, Dict[str, Any]]\n",
    "    nodes_expanded: List[str]\n",
    "\n",
    "    # ===== PHASE 4: Verification =====\n",
    "    noise_map: List[Dict[str, Any]]\n",
    "    nodes_to_patch: List[str]\n",
    "    cascade_queue: List[str]\n",
    "    targeted_evidence: Dict[str, List[str]]\n",
    "    current_refinement_iteration: int\n",
    "    max_refinement_iterations: int\n",
    "    quality_scores: Annotated[List[float], operator.add]\n",
    "    verification_log: Annotated[List[str], operator.add]\n",
    "\n",
    "    # ===== PHASE 5: Output =====\n",
    "    final_report: str\n",
    "\n",
    "    # ===== METRICS =====\n",
    "    total_searches: int\n",
    "    cache_hits: int\n",
    "    cache_decisions: Annotated[List[Dict], operator.add]\n",
    "\n",
    "\n",
    "print(\"Combined state defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcm345wig8",
   "metadata": {},
   "source": [
    "## 4. Phase 1: Agile Research Sprints\n",
    "\n",
    "This phase decomposes the question, executes sprints with retrospectives, and produces a compressed research brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ctvzx7f1sm9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Phase 1 Prompts =====\n",
    "\n",
    "DECOMPOSE_PROMPT = \"\"\"You are a research planning expert. Decompose this research question into 5-7 specific sub-questions.\n",
    "\n",
    "Research Question: {question}\n",
    "\n",
    "Generate a prioritized list of specific, focused research questions that together will comprehensively answer the main question. Each should be independently searchable.\n",
    "\n",
    "Return as a numbered list (highest priority first):\n",
    "1. [Most critical sub-question]\n",
    "2. [Second priority]\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "SPRINT_SYNTHESIS_PROMPT = \"\"\"You are a research agent conducting Sprint {sprint_num} of {max_sprints}.\n",
    "\n",
    "Current research focus: {current_questions}\n",
    "\n",
    "Based on the search results below, extract key findings addressing these questions.\n",
    "Be specific and cite sources with URLs.\n",
    "\n",
    "Search Results:\n",
    "{search_results}\n",
    "\n",
    "Provide a comprehensive summary of findings (400-600 words) with specific facts and source URLs.\n",
    "\"\"\"\n",
    "\n",
    "RETROSPECTIVE_PROMPT = \"\"\"You are conducting a sprint retrospective for a research project.\n",
    "\n",
    "Original Question: {original_question}\n",
    "\n",
    "Sprint {sprint_num} of {max_sprints} has completed.\n",
    "\n",
    "Summary of findings so far:\n",
    "{findings_summary}\n",
    "\n",
    "Current remaining questions:\n",
    "{remaining_backlog}\n",
    "\n",
    "Provide a STRUCTURED response:\n",
    "\n",
    "## LEARNINGS\n",
    "Key insights from this sprint.\n",
    "\n",
    "## GAPS\n",
    "What is still unclear or needs investigation?\n",
    "\n",
    "## CONTINUE\n",
    "Should we continue with another sprint? Answer YES or NO.\n",
    "\n",
    "## NEW_QUESTIONS\n",
    "List 2-4 NEW questions that emerged (or \"None\"):\n",
    "- [New question 1]\n",
    "- [New question 2]\n",
    "\n",
    "## REPRIORITIZED_BACKLOG\n",
    "Reorder remaining questions by priority:\n",
    "1. [Highest priority]\n",
    "2. [Next priority]\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "COMPRESS_FINDINGS_PROMPT = \"\"\"Summarize these research findings into a concise research brief.\n",
    "\n",
    "All Findings:\n",
    "{all_findings}\n",
    "\n",
    "Create a bullet-point summary (max 600 words) capturing:\n",
    "- Key facts and statistics\n",
    "- Main themes and patterns\n",
    "- Important sources\n",
    "- Any contradictions identified\n",
    "\n",
    "Be concise but preserve critical information.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Phase 1 prompts defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tz8p5jb6rv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Phase 1 Node Functions =====\n",
    "\n",
    "async def decompose_question(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Decompose the research question into a backlog of sub-questions.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Phase 1a: Question Decomposition\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    prompt = DECOMPOSE_PROMPT.format(question=question)\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    lines = response.content.strip().split(\"\\n\")\n",
    "    backlog = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and (line[0].isdigit() or line.startswith(\"-\")):\n",
    "            clean = line.lstrip(\"0123456789.-) \").strip()\n",
    "            if clean:\n",
    "                backlog.append(clean)\n",
    "    \n",
    "    print(f\"  Created backlog with {len(backlog)} research questions\")\n",
    "    for i, q in enumerate(backlog[:5], 1):\n",
    "        print(f\"    {i}. {q[:60]}...\")\n",
    "    \n",
    "    return {\n",
    "        \"research_backlog\": backlog,\n",
    "        \"current_research_sprint\": 1,\n",
    "        \"max_research_sprints\": MAX_RESEARCH_SPRINTS,\n",
    "        \"research_complete\": False\n",
    "    }\n",
    "\n",
    "\n",
    "async def execute_research_sprint(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Execute a research sprint on top backlog items.\"\"\"\n",
    "    backlog = state.get(\"research_backlog\", [])\n",
    "    current_sprint = state.get(\"current_research_sprint\", 1)\n",
    "    max_sprints = state.get(\"max_research_sprints\", MAX_RESEARCH_SPRINTS)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 1b: Research Sprint {current_sprint}/{max_sprints}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if not backlog:\n",
    "        return {\"sprint_findings\": [\"No questions in backlog.\"]}\n",
    "    \n",
    "    # Take top questions for this sprint\n",
    "    current_questions = backlog[:QUERIES_PER_SPRINT]\n",
    "    \n",
    "    all_content = []\n",
    "    all_urls = []\n",
    "    all_decisions = []\n",
    "    \n",
    "    for i, question in enumerate(current_questions, 1):\n",
    "        print(f\"  [{i}/{len(current_questions)}] {question[:50]}...\")\n",
    "        content, urls, decision = await cascaded_search(question, knowledge_base)\n",
    "        all_content.append(f\"### {question}\\n{content}\")\n",
    "        all_urls.extend(urls)\n",
    "        all_decisions.append(decision.model_dump())\n",
    "        \n",
    "        action = \"CACHE\" if decision.action_taken == \"USE_CACHE\" else \"SEARCH\"\n",
    "        print(f\"      {action} | Layer: {decision.layer_reached}\")\n",
    "    \n",
    "    # Synthesize findings\n",
    "    combined_results = \"\\n\\n---\\n\\n\".join(all_content)\n",
    "    if len(combined_results) > 12000:\n",
    "        combined_results = combined_results[:12000] + \"\\n...[truncated]\"\n",
    "    \n",
    "    synthesis_prompt = SPRINT_SYNTHESIS_PROMPT.format(\n",
    "        sprint_num=current_sprint,\n",
    "        max_sprints=max_sprints,\n",
    "        current_questions=\"\\n\".join(f\"- {q}\" for q in current_questions),\n",
    "        search_results=combined_results\n",
    "    )\n",
    "    \n",
    "    synthesis = await llm.ainvoke([HumanMessage(content=synthesis_prompt)])\n",
    "    finding = f\"## Sprint {current_sprint} Findings\\n\\n{synthesis.content}\"\n",
    "    \n",
    "    # Update backlog (remove processed questions)\n",
    "    updated_backlog = backlog[QUERIES_PER_SPRINT:]\n",
    "    \n",
    "    print(f\"  Synthesized {len(synthesis.content)} chars, {len(all_urls)} sources\")\n",
    "    \n",
    "    return {\n",
    "        \"sprint_findings\": [finding],\n",
    "        \"research_source_urls\": all_urls,\n",
    "        \"cache_decisions\": all_decisions,\n",
    "        \"research_backlog\": updated_backlog\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Phase 1 node functions defined (part 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cijvkfsbm5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Phase 1 Node Functions (continued) =====\n",
    "\n",
    "def parse_reprioritized_backlog(response_content: str) -> List[str]:\n",
    "    \"\"\"Parse reprioritized backlog from retrospective.\"\"\"\n",
    "    backlog_match = re.search(r'## REPRIORITIZED_BACKLOG\\s*(.*?)(?=##|$)', response_content, re.DOTALL | re.IGNORECASE)\n",
    "    if backlog_match:\n",
    "        backlog_text = backlog_match.group(1)\n",
    "        questions = []\n",
    "        for line in backlog_text.strip().split(\"\\n\"):\n",
    "            line = line.strip()\n",
    "            if line and (line[0].isdigit() or line.startswith(\"-\")):\n",
    "                clean = re.sub(r'^[\\d\\.\\-\\)\\s]+', '', line).strip()\n",
    "                if clean and len(clean) > 10:\n",
    "                    questions.append(clean)\n",
    "        return questions\n",
    "    return []\n",
    "\n",
    "\n",
    "def parse_should_continue_research(response_content: str) -> bool:\n",
    "    \"\"\"Parse whether to continue from retrospective.\"\"\"\n",
    "    continue_match = re.search(r'## CONTINUE\\s*(.*?)(?=##|$)', response_content, re.DOTALL | re.IGNORECASE)\n",
    "    if continue_match:\n",
    "        text = continue_match.group(1).strip().lower()\n",
    "        negative_patterns = [r'^no\\b', r'should\\s+stop', r'sufficient', r'adequately']\n",
    "        for pattern in negative_patterns:\n",
    "            if re.search(pattern, text):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "\n",
    "async def research_retrospective(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Conduct retrospective after research sprint.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    current_sprint = state.get(\"current_research_sprint\", 1)\n",
    "    max_sprints = state.get(\"max_research_sprints\", MAX_RESEARCH_SPRINTS)\n",
    "    all_findings = \"\\n\\n\".join(state.get(\"sprint_findings\", []))\n",
    "    backlog = state.get(\"research_backlog\", [])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 1c: Sprint {current_sprint} Retrospective\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Summarize findings for token efficiency\n",
    "    if len(all_findings) > 4000:\n",
    "        compress_prompt = COMPRESS_FINDINGS_PROMPT.format(all_findings=all_findings[:8000])\n",
    "        compress_response = await llm.ainvoke([HumanMessage(content=compress_prompt)])\n",
    "        findings_summary = compress_response.content\n",
    "    else:\n",
    "        findings_summary = all_findings\n",
    "    \n",
    "    prompt = RETROSPECTIVE_PROMPT.format(\n",
    "        original_question=question,\n",
    "        sprint_num=current_sprint,\n",
    "        max_sprints=max_sprints,\n",
    "        findings_summary=findings_summary,\n",
    "        remaining_backlog=\"\\n\".join(f\"- {q}\" for q in backlog) if backlog else \"None\"\n",
    "    )\n",
    "    \n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    reprioritized = parse_reprioritized_backlog(response.content)\n",
    "    should_continue = parse_should_continue_research(response.content)\n",
    "    \n",
    "    if not reprioritized and backlog:\n",
    "        reprioritized = backlog\n",
    "    \n",
    "    print(f\"  Should continue: {should_continue}\")\n",
    "    print(f\"  Backlog size: {len(reprioritized)}\")\n",
    "    \n",
    "    return {\n",
    "        \"research_retrospective_notes\": [f\"### Sprint {current_sprint} Retrospective\\n{response.content}\"],\n",
    "        \"current_research_sprint\": current_sprint + 1,\n",
    "        \"research_backlog\": reprioritized,\n",
    "        \"research_summary\": findings_summary,\n",
    "        \"research_complete\": not should_continue\n",
    "    }\n",
    "\n",
    "\n",
    "async def quality_gate_1(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Check source sufficiency before moving to synthesis.\"\"\"\n",
    "    source_urls = state.get(\"research_source_urls\", [])\n",
    "    unique_urls = list(set(source_urls))\n",
    "    \n",
    "    # Count unique domains\n",
    "    domains = set()\n",
    "    for url in unique_urls:\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            domains.add(parsed.netloc)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Quality Gate 1: Source Sufficiency\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Unique sources: {len(unique_urls)} (min: {MIN_SOURCES_FOR_GATE1})\")\n",
    "    print(f\"  Unique domains: {len(domains)} (min: {MIN_DOMAINS_FOR_GATE1})\")\n",
    "    \n",
    "    passed = len(unique_urls) >= MIN_SOURCES_FOR_GATE1 and len(domains) >= MIN_DOMAINS_FOR_GATE1\n",
    "    print(f\"  Gate 1 {'PASSED' if passed else 'PASSED (relaxed)'}\")  # Always pass but note\n",
    "    \n",
    "    return {\"research_complete\": True}\n",
    "\n",
    "\n",
    "async def compress_findings(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Compress all findings into a research brief for skeleton generation.\"\"\"\n",
    "    all_findings = \"\\n\\n\".join(state.get(\"sprint_findings\", []))\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Phase 1d: Compressing Research Brief\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if len(all_findings) > MAX_FINDINGS_CHARS:\n",
    "        prompt = COMPRESS_FINDINGS_PROMPT.format(all_findings=all_findings[:MAX_FINDINGS_CHARS])\n",
    "        response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "        summary = response.content\n",
    "    else:\n",
    "        summary = all_findings\n",
    "    \n",
    "    print(f\"  Compressed to {len(summary)} chars\")\n",
    "    \n",
    "    return {\"research_summary\": summary}\n",
    "\n",
    "\n",
    "def should_continue_research(state: CombinedTier1State) -> Literal[\"execute_sprint\", \"quality_gate_1\"]:\n",
    "    \"\"\"Decide whether to continue research or move to quality gate.\"\"\"\n",
    "    current_sprint = state.get(\"current_research_sprint\", 1)\n",
    "    max_sprints = state.get(\"max_research_sprints\", MAX_RESEARCH_SPRINTS)\n",
    "    backlog = state.get(\"research_backlog\", [])\n",
    "    research_complete = state.get(\"research_complete\", False)\n",
    "    \n",
    "    if current_sprint > max_sprints:\n",
    "        print(f\"  Max sprints reached. Moving to Quality Gate 1.\")\n",
    "        return \"quality_gate_1\"\n",
    "    if not backlog:\n",
    "        print(f\"  Backlog empty. Moving to Quality Gate 1.\")\n",
    "        return \"quality_gate_1\"\n",
    "    if research_complete:\n",
    "        print(f\"  Research marked complete. Moving to Quality Gate 1.\")\n",
    "        return \"quality_gate_1\"\n",
    "    \n",
    "    print(f\"  Continuing to sprint {current_sprint}.\")\n",
    "    return \"execute_sprint\"\n",
    "\n",
    "\n",
    "print(\"Phase 1 node functions defined (part 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qwvjkdiw82",
   "metadata": {},
   "source": [
    "## 5. Phase 2: Skeleton Generation\n",
    "\n",
    "Creates a hierarchical document structure before writing prose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j2jwub7q73o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Phase 2: Skeleton Generation =====\n",
    "\n",
    "SKELETON_PROMPT = \"\"\"You are a research document architect. Create a document skeleton.\n",
    "\n",
    "Research Question: {question}\n",
    "\n",
    "Research Findings Summary:\n",
    "{research_summary}\n",
    "\n",
    "Create a hierarchical document structure with:\n",
    "1. A thesis statement (one sentence)\n",
    "2. 5-7 main sections for a comprehensive research report\n",
    "\n",
    "Each section needs:\n",
    "- node_id: Unique identifier like \"sec:intro\", \"sec:background\"\n",
    "- title: Descriptive section title\n",
    "- intent: 1-3 sentences describing what this section accomplishes\n",
    "- target_word_count: 250-400 words per section\n",
    "- dependencies: List of node_ids this section builds upon (empty for intro)\n",
    "- children: Empty list (flat structure)\n",
    "\n",
    "REQUIRED SECTIONS:\n",
    "1. Introduction - Present topic, context, thesis\n",
    "2. Background - Foundation knowledge\n",
    "3. Main Body (2-4 sections) - Key aspects in depth\n",
    "4. Analysis/Discussion - Synthesize findings\n",
    "5. Conclusion - Summary and future directions\n",
    "\n",
    "Node IDs must be unique. Dependencies must reference existing nodes only.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def validate_skeleton(skeleton: Dict[str, Any]) -> Tuple[bool, List[str]]:\n",
    "    \"\"\"Validate skeleton structure.\"\"\"\n",
    "    issues = []\n",
    "    nodes = skeleton.get(\"nodes\", {})\n",
    "    root_nodes = skeleton.get(\"root_nodes\", [])\n",
    "    \n",
    "    for root_id in root_nodes:\n",
    "        if root_id not in nodes:\n",
    "            issues.append(f\"Root node '{root_id}' not found\")\n",
    "    \n",
    "    for node_id, node in nodes.items():\n",
    "        for dep_id in node.get(\"dependencies\", []):\n",
    "            if dep_id not in nodes:\n",
    "                issues.append(f\"Node '{node_id}' depends on non-existent '{dep_id}'\")\n",
    "    \n",
    "    return len(issues) == 0, issues\n",
    "\n",
    "\n",
    "async def generate_skeleton(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Generate the document skeleton structure.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    research_summary = state.get(\"research_summary\", \"\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Phase 2: Skeleton Generation\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    prompt = SKELETON_PROMPT.format(question=question, research_summary=research_summary[:6000])\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(SkeletonGenerationOutput)\n",
    "    result = await structured_llm.ainvoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    # Build skeleton dictionary\n",
    "    skeleton = {\n",
    "        \"thesis\": result.thesis,\n",
    "        \"root_nodes\": [],\n",
    "        \"nodes\": {}\n",
    "    }\n",
    "    \n",
    "    child_ids = set()\n",
    "    for section in result.sections:\n",
    "        child_ids.update(section.children)\n",
    "    \n",
    "    for section in result.sections:\n",
    "        skeleton[\"nodes\"][section.node_id] = section.model_dump()\n",
    "        if section.node_id not in child_ids:\n",
    "            skeleton[\"root_nodes\"].append(section.node_id)\n",
    "    \n",
    "    if not skeleton[\"root_nodes\"]:\n",
    "        skeleton[\"root_nodes\"] = list(skeleton[\"nodes\"].keys())\n",
    "    \n",
    "    is_valid, issues = validate_skeleton(skeleton)\n",
    "    \n",
    "    print(f\"  Thesis: {result.thesis[:80]}...\")\n",
    "    print(f\"  Sections: {len(skeleton['nodes'])}\")\n",
    "    print(f\"  Valid: {is_valid}\")\n",
    "    if issues:\n",
    "        for issue in issues[:3]:\n",
    "            print(f\"    Warning: {issue}\")\n",
    "    \n",
    "    return {\n",
    "        \"skeleton\": skeleton,\n",
    "        \"skeleton_validated\": is_valid,\n",
    "        \"prose_store\": {},\n",
    "        \"claims_registry\": {}\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Phase 2 functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k2oo3hx60bb",
   "metadata": {},
   "source": [
    "## 6. Phase 3: Node Expansion\n",
    "\n",
    "Generates prose for each skeleton node with dependency awareness and bridge sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4t2x8m8gwq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Phase 3: Node Expansion =====\n",
    "\n",
    "PROSE_PROMPT = \"\"\"You are a research writer generating content for a specific section.\n",
    "\n",
    "DOCUMENT CONTEXT:\n",
    "Research Question: {question}\n",
    "Document Thesis: {thesis}\n",
    "\n",
    "SECTION TO WRITE:\n",
    "Node ID: {node_id}\n",
    "Title: {title}\n",
    "Intent: {intent}\n",
    "Target Length: ~{target_words} words\n",
    "\n",
    "PREVIOUS SECTION ENDING:\n",
    "{previous_bridge_out}\n",
    "\n",
    "DEPENDENCY SUMMARIES:\n",
    "{dependency_summaries}\n",
    "\n",
    "RESEARCH FINDINGS:\n",
    "{research_findings}\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. bridge_in (1-2 sentences): Transition from previous section\n",
    "2. main_content (~{target_words} words): Substantive prose with citations (Source: URL)\n",
    "3. bridge_out (1-2 sentences): Transition to next section\n",
    "4. summary (1-2 sentences): What this section establishes\n",
    "\n",
    "Be comprehensive, specific, and well-sourced.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_leaf_nodes(skeleton: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"Get all leaf node IDs in document order.\"\"\"\n",
    "    nodes = skeleton.get(\"nodes\", {})\n",
    "    root_nodes = skeleton.get(\"root_nodes\", [])\n",
    "    \n",
    "    def collect_leaves(node_ids: List[str]) -> List[str]:\n",
    "        leaves = []\n",
    "        for nid in node_ids:\n",
    "            node = nodes.get(nid, {})\n",
    "            children = node.get(\"children\", [])\n",
    "            if not children:\n",
    "                leaves.append(nid)\n",
    "            else:\n",
    "                leaves.extend(collect_leaves(children))\n",
    "        return leaves\n",
    "    \n",
    "    return collect_leaves(root_nodes)\n",
    "\n",
    "\n",
    "def topological_sort_nodes(skeleton: Dict[str, Any], node_ids: List[str]) -> List[str]:\n",
    "    \"\"\"Sort nodes by dependency order.\"\"\"\n",
    "    nodes = skeleton.get(\"nodes\", {})\n",
    "    node_id_set = set(node_ids)\n",
    "    remaining = set(node_ids)\n",
    "    sorted_nodes = []\n",
    "    \n",
    "    while remaining:\n",
    "        ready = []\n",
    "        for nid in remaining:\n",
    "            node = nodes.get(nid, {})\n",
    "            deps = set(node.get(\"dependencies\", []))\n",
    "            internal_deps = deps & node_id_set\n",
    "            if internal_deps.issubset(set(sorted_nodes)):\n",
    "                ready.append(nid)\n",
    "        \n",
    "        if not ready:\n",
    "            # Circular dependency - add remaining in order\n",
    "            for nid in node_ids:\n",
    "                if nid in remaining:\n",
    "                    sorted_nodes.append(nid)\n",
    "            break\n",
    "        \n",
    "        ready_ordered = [nid for nid in node_ids if nid in ready]\n",
    "        sorted_nodes.extend(ready_ordered)\n",
    "        remaining -= set(ready_ordered)\n",
    "    \n",
    "    return sorted_nodes\n",
    "\n",
    "\n",
    "async def expand_all_nodes(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Expand all leaf nodes in dependency order.\"\"\"\n",
    "    skeleton = state[\"skeleton\"]\n",
    "    research_summary = state.get(\"research_summary\", \"\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Phase 3: Node Expansion\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    leaf_nodes = get_leaf_nodes(skeleton)\n",
    "    sorted_nodes = topological_sort_nodes(skeleton, leaf_nodes)\n",
    "    \n",
    "    prose_store = {}\n",
    "    claims_registry = {}\n",
    "    \n",
    "    for i, node_id in enumerate(sorted_nodes):\n",
    "        node = skeleton[\"nodes\"][node_id]\n",
    "        print(f\"  [{i+1}/{len(sorted_nodes)}] {node_id}: {node['title']}\")\n",
    "        \n",
    "        # Get previous node's bridge_out\n",
    "        prev_bridge_out = \"\"\n",
    "        if i > 0:\n",
    "            prev_id = sorted_nodes[i-1]\n",
    "            if prev_id in prose_store:\n",
    "                prev_bridge_out = prose_store[prev_id].get(\"bridge_out\", \"\")\n",
    "        \n",
    "        # Get dependency summaries\n",
    "        dep_summaries = []\n",
    "        for dep_id in node.get(\"dependencies\", []):\n",
    "            if dep_id in prose_store:\n",
    "                dep_title = skeleton[\"nodes\"].get(dep_id, {}).get(\"title\", dep_id)\n",
    "                dep_summary = prose_store[dep_id].get(\"summary\", \"\")\n",
    "                dep_summaries.append(f\"{dep_title}: {dep_summary}\")\n",
    "        \n",
    "        prompt = PROSE_PROMPT.format(\n",
    "            question=question,\n",
    "            thesis=skeleton.get(\"thesis\", \"\"),\n",
    "            node_id=node_id,\n",
    "            title=node.get(\"title\", \"\"),\n",
    "            intent=node.get(\"intent\", \"\"),\n",
    "            target_words=node.get(\"target_word_count\", TARGET_WORDS_PER_NODE),\n",
    "            previous_bridge_out=prev_bridge_out if prev_bridge_out else \"(First section)\",\n",
    "            dependency_summaries=\"\\n\".join(dep_summaries) if dep_summaries else \"(No dependencies)\",\n",
    "            research_findings=research_summary[:4000]\n",
    "        )\n",
    "        \n",
    "        structured_llm = llm.with_structured_output(ProseGenerationOutput)\n",
    "        result = await structured_llm.ainvoke([HumanMessage(content=prompt)])\n",
    "        \n",
    "        prose_store[node_id] = {\n",
    "            \"node_id\": node_id,\n",
    "            \"main_content\": result.main_content,\n",
    "            \"bridge_in\": result.bridge_in,\n",
    "            \"bridge_out\": result.bridge_out,\n",
    "            \"summary\": result.summary,\n",
    "            \"revision_count\": 0\n",
    "        }\n",
    "        \n",
    "        print(f\"      Generated {len(result.main_content)} chars\")\n",
    "    \n",
    "    return {\n",
    "        \"prose_store\": prose_store,\n",
    "        \"claims_registry\": claims_registry,\n",
    "        \"nodes_expanded\": sorted_nodes\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Phase 3 functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9q8tqacevdw",
   "metadata": {},
   "source": [
    "## 7. Phase 4: Verification and Refinement\n",
    "\n",
    "Critique the document, apply Quality Gate 2, do targeted retrieval for weak claims, and apply patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h0agndxc5sc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Phase 4: Critique and Quality Gate 2 =====\n",
    "\n",
    "CRITIQUE_PROMPT = \"\"\"You are a critical reviewer evaluating a research document.\n",
    "\n",
    "ORIGINAL QUESTION: {question}\n",
    "DOCUMENT THESIS: {thesis}\n",
    "\n",
    "FULL DOCUMENT:\n",
    "{document_content}\n",
    "\n",
    "Analyze at THREE levels:\n",
    "\n",
    "## 1. GLOBAL ISSUES (entire document)\n",
    "- Thesis clarity and consistency\n",
    "- Overall argument flow\n",
    "- Terminology consistency\n",
    "\n",
    "## 2. SECTION ISSUES (per node) - MOST IMPORTANT\n",
    "- weak_claim: Claims lacking evidence\n",
    "- missing_evidence: Key assertions needing sources\n",
    "- logical_gap: Reasoning jumps\n",
    "- unclear: Ambiguous passages\n",
    "- depth: Insufficient detail\n",
    "\n",
    "## 3. TRANSITION ISSUES (between sections)\n",
    "- Abrupt topic shifts\n",
    "- Redundant transitions\n",
    "\n",
    "For each issue provide:\n",
    "- issue_id: Unique identifier\n",
    "- scope: \"global\", \"section\", or \"transition\"\n",
    "- target_nodes: Affected node IDs\n",
    "- issue_type: Category\n",
    "- severity: \"critical\", \"major\", or \"minor\"\n",
    "- description: What the problem is\n",
    "- suggestion: How to fix it\n",
    "- search_query: Query to find evidence (for evidence issues)\n",
    "\n",
    "SCORING (1-10):\n",
    "- 9-10: Publication ready\n",
    "- 7-8: Good, minor issues\n",
    "- 5-6: Needs improvement\n",
    "- 3-4: Significant problems\n",
    "\n",
    "Provide overall_quality score and list of issues.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "async def critique_document(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Perform structured critique producing the Noise Map.\"\"\"\n",
    "    skeleton = state[\"skeleton\"]\n",
    "    prose_store = state[\"prose_store\"]\n",
    "    question = state[\"question\"]\n",
    "    iteration = state.get(\"current_refinement_iteration\", 0)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 4a: Critique (Iteration {iteration})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Build document content\n",
    "    leaf_nodes = get_leaf_nodes(skeleton)\n",
    "    document_content = \"\"\n",
    "    for node_id in leaf_nodes:\n",
    "        if node_id in prose_store:\n",
    "            node = skeleton[\"nodes\"][node_id]\n",
    "            prose = prose_store[node_id]\n",
    "            document_content += f\"\\n\\n## {node['title']} [{node_id}]\\n\"\n",
    "            document_content += f\"{prose.get('bridge_in', '')}\\n\"\n",
    "            document_content += f\"{prose.get('main_content', '')}\\n\"\n",
    "            document_content += f\"{prose.get('bridge_out', '')}\\n\"\n",
    "    \n",
    "    if len(document_content) > 12000:\n",
    "        document_content = document_content[:12000] + \"\\n...[truncated]\"\n",
    "    \n",
    "    prompt = CRITIQUE_PROMPT.format(\n",
    "        question=question,\n",
    "        thesis=skeleton.get(\"thesis\", \"\"),\n",
    "        document_content=document_content\n",
    "    )\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(CritiqueResult)\n",
    "    result = await structured_llm.ainvoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    noise_map = [issue.model_dump() for issue in result.issues]\n",
    "    \n",
    "    # Identify nodes needing patches\n",
    "    nodes_to_patch = list(set(\n",
    "        node_id for issue in result.issues\n",
    "        for node_id in issue.target_nodes\n",
    "        if issue.severity in [\"critical\", \"major\"]\n",
    "    ))\n",
    "    \n",
    "    print(f\"  Quality Score: {result.overall_quality}/10\")\n",
    "    print(f\"  Issues: {len(result.issues)}\")\n",
    "    print(f\"  Nodes to patch: {len(nodes_to_patch)}\")\n",
    "    \n",
    "    return {\n",
    "        \"noise_map\": noise_map,\n",
    "        \"nodes_to_patch\": nodes_to_patch,\n",
    "        \"quality_scores\": [result.overall_quality],\n",
    "        \"current_refinement_iteration\": iteration\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Phase 4 critique defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87in7uvly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Phase 4: Targeted Retrieval and Patching =====\n",
    "\n",
    "PATCH_PROMPT = \"\"\"You are revising a section based on critique feedback.\n",
    "\n",
    "SECTION TO REVISE:\n",
    "Node ID: {node_id}\n",
    "Title: {title}\n",
    "Intent: {intent}\n",
    "\n",
    "CURRENT CONTENT:\n",
    "{current_content}\n",
    "\n",
    "ISSUES TO FIX:\n",
    "{issues_text}\n",
    "\n",
    "NEW EVIDENCE:\n",
    "{new_evidence}\n",
    "\n",
    "CONTEXT:\n",
    "Previous section ends with: {prev_bridge_out}\n",
    "Next section starts with: {next_bridge_in}\n",
    "\n",
    "Revise the section to address ALL issues, incorporate evidence with citations.\n",
    "Output: bridge_in, main_content, bridge_out, summary\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "async def targeted_retrieval(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Search for evidence to address issues.\"\"\"\n",
    "    noise_map = state.get(\"noise_map\", [])\n",
    "    nodes_to_patch = state.get(\"nodes_to_patch\", [])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Phase 4b: Targeted Retrieval\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if not nodes_to_patch:\n",
    "        print(\"  No nodes need patching\")\n",
    "        return {\"targeted_evidence\": {}}\n",
    "    \n",
    "    # Collect search queries per node\n",
    "    node_queries = {}\n",
    "    for issue in noise_map:\n",
    "        if issue.get(\"search_query\") and issue.get(\"severity\") in [\"critical\", \"major\"]:\n",
    "            for node_id in issue.get(\"target_nodes\", []):\n",
    "                if node_id in nodes_to_patch:\n",
    "                    if node_id not in node_queries:\n",
    "                        node_queries[node_id] = []\n",
    "                    node_queries[node_id].append(issue[\"search_query\"])\n",
    "    \n",
    "    targeted_evidence = {}\n",
    "    \n",
    "    for node_id, queries in node_queries.items():\n",
    "        print(f\"  Searching for: {node_id}\")\n",
    "        node_evidence = []\n",
    "        for query in queries[:2]:  # Limit queries per node\n",
    "            print(f\"    Query: {query[:40]}...\")\n",
    "            content, urls, decision = await cascaded_search(query, knowledge_base)\n",
    "            node_evidence.append(content[:1000])\n",
    "            action = \"CACHE\" if decision.action_taken == \"USE_CACHE\" else \"SEARCH\"\n",
    "            print(f\"      {action} | Layer: {decision.layer_reached}\")\n",
    "        targeted_evidence[node_id] = node_evidence\n",
    "    \n",
    "    print(f\"  Evidence gathered for {len(targeted_evidence)} nodes\")\n",
    "    \n",
    "    return {\"targeted_evidence\": targeted_evidence}\n",
    "\n",
    "\n",
    "async def apply_patches(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Apply patches to nodes with issues.\"\"\"\n",
    "    skeleton = state[\"skeleton\"]\n",
    "    prose_store = state[\"prose_store\"]\n",
    "    noise_map = state.get(\"noise_map\", [])\n",
    "    nodes_to_patch = state.get(\"nodes_to_patch\", [])\n",
    "    targeted_evidence = state.get(\"targeted_evidence\", {})\n",
    "    iteration = state.get(\"current_refinement_iteration\", 0)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Phase 4c: Apply Patches\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if not nodes_to_patch:\n",
    "        print(\"  No patches needed\")\n",
    "        return {\"prose_store\": prose_store, \"current_refinement_iteration\": iteration + 1}\n",
    "    \n",
    "    leaf_nodes = get_leaf_nodes(skeleton)\n",
    "    sorted_patch = topological_sort_nodes(skeleton, [n for n in nodes_to_patch if n in leaf_nodes])\n",
    "    \n",
    "    for node_id in sorted_patch[:3]:  # Limit patches per iteration\n",
    "        if node_id not in prose_store:\n",
    "            continue\n",
    "        \n",
    "        node = skeleton[\"nodes\"].get(node_id, {})\n",
    "        current = prose_store[node_id]\n",
    "        \n",
    "        print(f\"  Patching: {node_id}\")\n",
    "        \n",
    "        # Get issues for this node\n",
    "        node_issues = [i for i in noise_map if node_id in i.get(\"target_nodes\", [])]\n",
    "        issues_text = \"\\n\".join([\n",
    "            f\"- [{i['severity']}] {i['issue_type']}: {i['description']}\"\n",
    "            for i in node_issues\n",
    "        ])\n",
    "        \n",
    "        evidence = targeted_evidence.get(node_id, [])\n",
    "        evidence_text = \"\\n\\n\".join(evidence) if evidence else \"No additional evidence.\"\n",
    "        \n",
    "        # Get adjacent context\n",
    "        idx = leaf_nodes.index(node_id) if node_id in leaf_nodes else -1\n",
    "        prev_bridge = prose_store.get(leaf_nodes[idx-1], {}).get(\"bridge_out\", \"\") if idx > 0 else \"\"\n",
    "        next_bridge = prose_store.get(leaf_nodes[idx+1], {}).get(\"bridge_in\", \"\") if idx < len(leaf_nodes)-1 else \"\"\n",
    "        \n",
    "        prompt = PATCH_PROMPT.format(\n",
    "            node_id=node_id,\n",
    "            title=node.get(\"title\", \"\"),\n",
    "            intent=node.get(\"intent\", \"\"),\n",
    "            current_content=f\"{current.get('bridge_in', '')}\\n{current.get('main_content', '')}\\n{current.get('bridge_out', '')}\",\n",
    "            issues_text=issues_text,\n",
    "            new_evidence=evidence_text[:3000],\n",
    "            prev_bridge_out=prev_bridge if prev_bridge else \"(First)\",\n",
    "            next_bridge_in=next_bridge if next_bridge else \"(Last)\"\n",
    "        )\n",
    "        \n",
    "        structured_llm = llm.with_structured_output(ProseGenerationOutput)\n",
    "        result = await structured_llm.ainvoke([HumanMessage(content=prompt)])\n",
    "        \n",
    "        prose_store[node_id] = {\n",
    "            \"node_id\": node_id,\n",
    "            \"main_content\": result.main_content,\n",
    "            \"bridge_in\": result.bridge_in,\n",
    "            \"bridge_out\": result.bridge_out,\n",
    "            \"summary\": result.summary,\n",
    "            \"revision_count\": current.get(\"revision_count\", 0) + 1\n",
    "        }\n",
    "        \n",
    "        print(f\"    Revised: {len(result.main_content)} chars\")\n",
    "    \n",
    "    return {\n",
    "        \"prose_store\": prose_store,\n",
    "        \"current_refinement_iteration\": iteration + 1\n",
    "    }\n",
    "\n",
    "\n",
    "def should_continue_refining(state: CombinedTier1State) -> Literal[\"targeted_retrieval\", \"assemble\"]:\n",
    "    \"\"\"Decide whether to continue refinement.\"\"\"\n",
    "    iteration = state.get(\"current_refinement_iteration\", 0)\n",
    "    quality_scores = state.get(\"quality_scores\", [])\n",
    "    nodes_to_patch = state.get(\"nodes_to_patch\", [])\n",
    "    \n",
    "    latest_score = quality_scores[-1] if quality_scores else 0\n",
    "    \n",
    "    print(f\"\\n--- Quality Gate 2 ---\")\n",
    "    print(f\"  Iteration: {iteration}/{MAX_REFINEMENT_ITERATIONS}\")\n",
    "    print(f\"  Score: {latest_score}/{QUALITY_THRESHOLD}\")\n",
    "    \n",
    "    if iteration >= MAX_REFINEMENT_ITERATIONS:\n",
    "        print(\"  Max iterations. Finalizing.\")\n",
    "        return \"assemble\"\n",
    "    if latest_score >= QUALITY_THRESHOLD:\n",
    "        print(\"  Quality threshold met. Finalizing.\")\n",
    "        return \"assemble\"\n",
    "    if not nodes_to_patch:\n",
    "        print(\"  No issues. Finalizing.\")\n",
    "        return \"assemble\"\n",
    "    \n",
    "    print(\"  Continuing refinement.\")\n",
    "    return \"targeted_retrieval\"\n",
    "\n",
    "\n",
    "print(\"Phase 4 retrieval and patching defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hrpkztl86ol",
   "metadata": {},
   "source": [
    "## 8. Phase 5: Final Assembly\n",
    "\n",
    "Assembles the final markdown report from all prose entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13xtzj6ns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Phase 5: Final Assembly =====\n",
    "\n",
    "async def assemble_document(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Assemble the final document from prose entries.\"\"\"\n",
    "    skeleton = state[\"skeleton\"]\n",
    "    prose_store = state[\"prose_store\"]\n",
    "    quality_scores = state.get(\"quality_scores\", [])\n",
    "    source_urls = state.get(\"research_source_urls\", [])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Phase 5: Final Assembly\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    leaf_nodes = get_leaf_nodes(skeleton)\n",
    "    \n",
    "    # Build document\n",
    "    parts = []\n",
    "    parts.append(f\"# Research Report\\n\")\n",
    "    parts.append(f\"**Thesis:** {skeleton.get('thesis', '')}\\n\\n\")\n",
    "    \n",
    "    for node_id in leaf_nodes:\n",
    "        if node_id not in prose_store:\n",
    "            continue\n",
    "        node = skeleton[\"nodes\"].get(node_id, {})\n",
    "        prose = prose_store[node_id]\n",
    "        \n",
    "        parts.append(f\"## {node.get('title', node_id)}\\n\\n\")\n",
    "        if prose.get(\"bridge_in\"):\n",
    "            parts.append(f\"{prose['bridge_in']} \")\n",
    "        parts.append(f\"{prose.get('main_content', '')}\")\n",
    "        if prose.get(\"bridge_out\"):\n",
    "            parts.append(f\" {prose['bridge_out']}\")\n",
    "        parts.append(\"\\n\\n\")\n",
    "    \n",
    "    # References\n",
    "    unique_urls = list(set(source_urls))\n",
    "    if unique_urls:\n",
    "        parts.append(\"## References\\n\\n\")\n",
    "        for i, url in enumerate(unique_urls[:25], 1):\n",
    "            parts.append(f\"{i}. {url}\\n\")\n",
    "    \n",
    "    final_report = \"\".join(parts)\n",
    "    word_count = len(final_report.split())\n",
    "    \n",
    "    print(f\"  Document: {len(final_report)} chars ({word_count} words)\")\n",
    "    print(f\"  Sections: {len(leaf_nodes)}\")\n",
    "    print(f\"  Quality: {' -> '.join([f'{s:.1f}' for s in quality_scores])}\")\n",
    "    print(f\"  Sources: {len(unique_urls)}\")\n",
    "    print(f\"  Cache: {knowledge_base.get_stats_summary()}\")\n",
    "    \n",
    "    return {\"final_report\": final_report}\n",
    "\n",
    "\n",
    "print(\"Phase 5 assembly defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h2odwkgb4lf",
   "metadata": {},
   "source": [
    "## 9. Graph Construction\n",
    "\n",
    "Build the complete LangGraph combining all phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v8576b11i7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Graph Construction =====\n",
    "\n",
    "builder = StateGraph(CombinedTier1State)\n",
    "\n",
    "# Add all nodes\n",
    "builder.add_node(\"decompose_question\", decompose_question)\n",
    "builder.add_node(\"execute_research_sprint\", execute_research_sprint)\n",
    "builder.add_node(\"research_retrospective\", research_retrospective)\n",
    "builder.add_node(\"quality_gate_1\", quality_gate_1)\n",
    "builder.add_node(\"compress_findings\", compress_findings)\n",
    "builder.add_node(\"generate_skeleton\", generate_skeleton)\n",
    "builder.add_node(\"expand_all_nodes\", expand_all_nodes)\n",
    "builder.add_node(\"critique_document\", critique_document)\n",
    "builder.add_node(\"targeted_retrieval\", targeted_retrieval)\n",
    "builder.add_node(\"apply_patches\", apply_patches)\n",
    "builder.add_node(\"assemble_document\", assemble_document)\n",
    "\n",
    "# Phase 1: Research Sprints\n",
    "builder.add_edge(START, \"decompose_question\")\n",
    "builder.add_edge(\"decompose_question\", \"execute_research_sprint\")\n",
    "builder.add_edge(\"execute_research_sprint\", \"research_retrospective\")\n",
    "\n",
    "# Research loop\n",
    "builder.add_conditional_edges(\n",
    "    \"research_retrospective\",\n",
    "    should_continue_research,\n",
    "    {\n",
    "        \"execute_sprint\": \"execute_research_sprint\",\n",
    "        \"quality_gate_1\": \"quality_gate_1\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Phase 1 -> Phase 2\n",
    "builder.add_edge(\"quality_gate_1\", \"compress_findings\")\n",
    "builder.add_edge(\"compress_findings\", \"generate_skeleton\")\n",
    "\n",
    "# Phase 2 -> Phase 3\n",
    "builder.add_edge(\"generate_skeleton\", \"expand_all_nodes\")\n",
    "\n",
    "# Phase 3 -> Phase 4\n",
    "builder.add_edge(\"expand_all_nodes\", \"critique_document\")\n",
    "\n",
    "# Phase 4: Refinement loop\n",
    "builder.add_conditional_edges(\n",
    "    \"critique_document\",\n",
    "    should_continue_refining,\n",
    "    {\n",
    "        \"targeted_retrieval\": \"targeted_retrieval\",\n",
    "        \"assemble\": \"assemble_document\"\n",
    "    }\n",
    ")\n",
    "\n",
    "builder.add_edge(\"targeted_retrieval\", \"apply_patches\")\n",
    "builder.add_edge(\"apply_patches\", \"critique_document\")\n",
    "\n",
    "# Phase 5\n",
    "builder.add_edge(\"assemble_document\", END)\n",
    "\n",
    "# Compile\n",
    "combined_tier1_graph = builder.compile()\n",
    "\n",
    "print(\"Combined Tier 1 Agent compiled successfully!\")\n",
    "print(\"\\nArchitecture:\")\n",
    "print(\"  Phase 1: decompose → [sprint → retrospective] → gate1 → compress\")\n",
    "print(\"  Phase 2: generate_skeleton\")\n",
    "print(\"  Phase 3: expand_all_nodes\")\n",
    "print(\"  Phase 4: [critique → retrieval → patch] loop\")\n",
    "print(\"  Phase 5: assemble_document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cysou8b9rc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(combined_tier1_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thcu30qhwx",
   "metadata": {},
   "source": [
    "## 10. Agent Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fn44x7nej9u",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Agent Wrapper =====\n",
    "\n",
    "async def combined_tier1_agent_async(inputs: dict) -> dict:\n",
    "    \"\"\"Async version of the Combined Tier 1 research agent.\"\"\"\n",
    "    global knowledge_base\n",
    "    knowledge_base = KnowledgeBase()  # Reset for fresh session\n",
    "    \n",
    "    question = inputs.get(\"question\", \"\")\n",
    "    \n",
    "    result = await combined_tier1_graph.ainvoke(\n",
    "        {\"question\": question},\n",
    "        config={\"recursion_limit\": 100}\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"output\": result.get(\"final_report\", \"\"),\n",
    "        \"source_urls\": list(set(result.get(\"research_source_urls\", []))),\n",
    "        \"quality_scores\": result.get(\"quality_scores\", []),\n",
    "        \"cache_stats\": knowledge_base.stats.copy(),\n",
    "        \"skeleton\": result.get(\"skeleton\", {}),\n",
    "        \"research_sprints\": result.get(\"current_research_sprint\", 1) - 1,\n",
    "        \"refinement_iterations\": result.get(\"current_refinement_iteration\", 0)\n",
    "    }\n",
    "\n",
    "\n",
    "def combined_tier1_agent(inputs: dict) -> dict:\n",
    "    \"\"\"Sync wrapper for Combined Tier 1 research agent. Compatible with evaluation harness.\"\"\"\n",
    "    question = inputs.get(\"question\", \"\")\n",
    "    \n",
    "    async def _execute():\n",
    "        global knowledge_base\n",
    "        knowledge_base = KnowledgeBase()\n",
    "        return await combined_tier1_graph.ainvoke(\n",
    "            {\"question\": question},\n",
    "            config={\"recursion_limit\": 100}\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        import concurrent.futures\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future = executor.submit(asyncio.run, _execute())\n",
    "            result = future.result()\n",
    "    except RuntimeError:\n",
    "        result = asyncio.run(_execute())\n",
    "    \n",
    "    return {\n",
    "        \"output\": result.get(\"final_report\", \"\"),\n",
    "        \"source_urls\": list(set(result.get(\"research_source_urls\", []))),\n",
    "        \"quality_scores\": result.get(\"quality_scores\", []),\n",
    "        \"cache_stats\": knowledge_base.stats.copy()\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Agent wrappers defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9umohyn6mv",
   "metadata": {},
   "source": [
    "## 11. Manual Test\n",
    "\n",
    "Run this cell to verify the Combined Tier 1 agent works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ftb2wr0s",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Test\n",
    "test_question = \"What are the key benefits and challenges of using large language models in enterprise applications?\"\n",
    "\n",
    "print(f\"Testing Combined Tier 1 Agent\")\n",
    "print(f\"Question: {test_question}\")\n",
    "print(\"\\nRunning combined architecture (this will take several minutes)...\\n\")\n",
    "\n",
    "try:\n",
    "    result = await combined_tier1_agent_async({\"question\": test_question})\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FINAL REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    print(result[\"output\"][:4000] + \"...\" if len(result[\"output\"]) > 4000 else result[\"output\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Report length: {len(result['output'])} characters\")\n",
    "    print(f\"Research sprints: {result.get('research_sprints', 'N/A')}\")\n",
    "    print(f\"Refinement iterations: {result.get('refinement_iterations', 'N/A')}\")\n",
    "    print(f\"Quality scores: {result.get('quality_scores', [])}\")\n",
    "    print(f\"Unique sources: {len(result.get('source_urls', []))}\")\n",
    "    \n",
    "    stats = result.get(\"cache_stats\", {})\n",
    "    total = stats.get(\"total_queries\", 0)\n",
    "    avoided = stats.get(\"web_searches_avoided\", 0)\n",
    "    hit_rate = avoided / total * 100 if total > 0 else 0\n",
    "    \n",
    "    print(f\"\\nCache Performance:\")\n",
    "    print(f\"  Total queries: {total}\")\n",
    "    print(f\"  Cache hits: {avoided} ({hit_rate:.1f}%)\")\n",
    "    print(f\"  L1 hits: {stats.get('l1_hits', 0)}\")\n",
    "    print(f\"  L2 HIGH: {stats.get('l2_high', 0)}\")\n",
    "    \n",
    "    print(\"\\nAgent test PASSED\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Agent test FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t9poimyaxh8",
   "metadata": {},
   "source": [
    "## 12. Evaluation Harness Integration\n",
    "\n",
    "Once the manual test passes, run full evaluation on all 20 questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pxf3um3t2r",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation harness\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from evaluation import (\n",
    "    ExperimentHarness,\n",
    "    fact_recall,\n",
    "    citation_precision,\n",
    "    coherence_judge,\n",
    "    depth_judge,\n",
    "    relevance_judge,\n",
    "    minimum_sources_check\n",
    ")\n",
    "\n",
    "harness = ExperimentHarness(\n",
    "    dataset_path=\"../data/deep_research_agent_test_dataset.yaml\",\n",
    "    langsmith_dataset_name=\"deep-research-golden-v2\"\n",
    ")\n",
    "\n",
    "print(\"Evaluation harness initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45h611vhxet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Evaluation - UNCOMMENT TO RUN\n",
    "# WARNING: This is expensive and takes 2-3 hours\n",
    "\n",
    "# evaluators = [\n",
    "#     fact_recall,\n",
    "#     citation_precision,\n",
    "#     minimum_sources_check,\n",
    "#     coherence_judge,\n",
    "#     depth_judge,\n",
    "#     relevance_judge,\n",
    "# ]\n",
    "# \n",
    "# print(\"Starting FULL evaluation on all 20 questions...\")\n",
    "# print(\"Combined Tier 1 Agent - estimated 2-3 hours\")\n",
    "# print(\"=\" * 80 + \"\\n\")\n",
    "# \n",
    "# results = harness.run_evaluation(\n",
    "#     agent_fn=combined_tier1_agent,\n",
    "#     evaluators=evaluators,\n",
    "#     experiment_name=\"combined_tier1_v1\",\n",
    "#     monte_carlo_runs=1,\n",
    "#     max_concurrency=2,\n",
    "#     description=\"Combined Tier 1 paradigms evaluation\"\n",
    "# )\n",
    "# \n",
    "# print(\"\\n\" + \"=\" * 80)\n",
    "# print(\"FULL EVALUATION RESULTS\")\n",
    "# print(\"=\" * 80)\n",
    "# print(f\"Experiment: {results.experiment_name}\")\n",
    "# print(f\"Questions: {results.num_questions}\")\n",
    "# \n",
    "# print(f\"\\n{'Metric':<30} {'Mean':<10}\")\n",
    "# print(\"-\" * 40)\n",
    "# for metric in sorted(results.metrics.keys()):\n",
    "#     if not metric.endswith('_std'):\n",
    "#         print(f\"{metric:<30} {results.metrics[metric]:<10.3f}\")\n",
    "\n",
    "print(\"Full evaluation cell ready. Uncomment to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uw84e6cx0y",
   "metadata": {},
   "source": [
    "## Architecture Summary\n",
    "\n",
    "### Combined Paradigms\n",
    "\n",
    "This notebook unifies four Tier 1 paradigms:\n",
    "\n",
    "1. **Cascading Knowledge Cache** - Wraps ALL search operations\n",
    "   - Layer 1: Exact query match\n",
    "   - Layer 2: Semantic similarity\n",
    "   - Expected 30-50% cache hits by verification phase\n",
    "\n",
    "2. **Agile Sprints** - Research phase (Phase 1)\n",
    "   - Decompose question into backlog\n",
    "   - Sprint-based execution with retrospectives\n",
    "   - Dynamic re-prioritization\n",
    "\n",
    "3. **Iterative Refinement V2** - Document generation (Phases 2-4)\n",
    "   - Skeleton-based structure\n",
    "   - Per-node prose generation\n",
    "   - Patch-based refinement (not full regeneration)\n",
    "\n",
    "4. **Quality Gates** - Strategic checkpoints\n",
    "   - Gate 1: Source sufficiency after research\n",
    "   - Gate 2: Quality threshold for refinement exit\n",
    "\n",
    "### Expected Performance\n",
    "\n",
    "| Phase | Expected Cache Hits |\n",
    "|-------|---------------------|\n",
    "| Research Sprint 1 | 0% (cold cache) |\n",
    "| Research Sprint 2-3 | 10-25% |\n",
    "| Verification Sprint 1 | 30-40% |\n",
    "| Verification Sprint 2 | 40-60% |\n",
    "\n",
    "### Key Metrics to Track\n",
    "\n",
    "- Cache hit rate > 30%\n",
    "- Quality score >= 7.5\n",
    "- Report coherence and depth\n",
    "- Citation presence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
