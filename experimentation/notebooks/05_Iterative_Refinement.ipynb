{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Paradigm 05: Iterative Refinement Research Agent\n",
                "\n",
                "This notebook implements the **Iterative Refinement (Diffusion-Based Reasoning)** paradigm from the Research Paradigms document.\n",
                "\n",
                "## Core Concept\n",
                "\n",
                "Iterative refinement applies a Generate-Critique-Revise loop to improve output quality:\n",
                "- **Generate**: Create initial research and draft\n",
                "- **Critique**: Identify weak claims, gaps, and inconsistencies\n",
                "- **Revise**: Fix identified issues with targeted improvements\n",
                "- **Converge**: Stop when quality is sufficient or max iterations reached\n",
                "\n",
                "## Literature Validation\n",
                "\n",
                "> \"WebThinker, LongDPO, CycleResearcher... iterating between generating and refining outputs to achieve higher quality results. This is the only universally validated component across all paradigms.\" —Feasibility Report\n",
                "\n",
                "## Technology Stack\n",
                "\n",
                "- **LLM**: `gpt-5-mini-2025-08-07`\n",
                "- **Web Search**: Tavily API\n",
                "- **Tracing**: LangSmith\n",
                "- **Framework**: LangGraph"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import operator\n",
                "import asyncio\n",
                "from pathlib import Path\n",
                "from typing import List, Annotated, TypedDict, Literal\n",
                "\n",
                "from dotenv import load_dotenv\n",
                "from pydantic import BaseModel, Field\n",
                "\n",
                "from langchain_openai import ChatOpenAI\n",
                "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
                "from tavily import TavilyClient\n",
                "\n",
                "from langgraph.graph import StateGraph, START, END\n",
                "\n",
                "# Load environment variables\n",
                "env_path = Path(\"../.env\")\n",
                "load_dotenv(env_path)\n",
                "\n",
                "# Configure LangSmith tracing\n",
                "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
                "os.environ[\"LANGSMITH_PROJECT\"] = \"deep_research_new\"\n",
                "\n",
                "print(\"Environment configured successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize LLM and Tavily client\n",
                "MODEL_NAME = \"gpt-5-mini-2025-08-07\"\n",
                "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
                "tavily_client = TavilyClient()\n",
                "\n",
                "# Refinement Configuration\n",
                "MAX_REVISIONS = 3  # Maximum critique-revise iterations\n",
                "MIN_QUALITY_THRESHOLD = 7.0  # Stop if quality exceeds this (1-10 scale)\n",
                "\n",
                "print(f\"Using model: {MODEL_NAME}\")\n",
                "print(f\"Max revisions: {MAX_REVISIONS}\")\n",
                "print(f\"Quality threshold: {MIN_QUALITY_THRESHOLD}/10\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. State Definitions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CritiqueIssue(BaseModel):\n",
                "    \"\"\"An issue identified during critique.\"\"\"\n",
                "    issue_type: str = Field(description=\"Type: weak_claim, missing_evidence, logical_gap, unclear\")\n",
                "    location: str = Field(description=\"Where in the text this issue appears\")\n",
                "    description: str = Field(description=\"Description of the issue\")\n",
                "    suggestion: str = Field(description=\"Suggested fix\")\n",
                "\n",
                "class CritiqueResult(BaseModel):\n",
                "    \"\"\"Result of critique analysis.\"\"\"\n",
                "    issues: List[CritiqueIssue] = Field(default_factory=list)\n",
                "    overall_quality: float = Field(description=\"Quality score 1-10\")\n",
                "    summary: str = Field(description=\"Summary of critique\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class IterativeRefinementState(TypedDict):\n",
                "    \"\"\"State for the Iterative Refinement Research Agent.\"\"\"\n",
                "    # Input\n",
                "    question: str\n",
                "    \n",
                "    # Research phase\n",
                "    search_results: Annotated[List[str], operator.add]\n",
                "    source_urls: Annotated[List[str], operator.add]\n",
                "    \n",
                "    # Draft management\n",
                "    current_draft: str\n",
                "    draft_history: Annotated[List[str], operator.add]  # Track all versions\n",
                "    \n",
                "    # Critique tracking\n",
                "    current_critique: str\n",
                "    critique_history: Annotated[List[str], operator.add]\n",
                "    revision_count: int\n",
                "    quality_scores: Annotated[List[float], operator.add]\n",
                "    \n",
                "    # Output\n",
                "    final_report: str"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def search_web(query: str, max_results: int = 10) -> tuple[List[str], List[str]]:\n",
                "    \"\"\"Execute web search using Tavily. Returns (results, urls).\"\"\"\n",
                "    try:\n",
                "        if len(query) > 400:\n",
                "            query = query[:400]\n",
                "        \n",
                "        response = tavily_client.search(\n",
                "            query=query,\n",
                "            max_results=max_results,\n",
                "            include_answer=True\n",
                "        )\n",
                "        \n",
                "        results = []\n",
                "        urls = []\n",
                "        \n",
                "        if response.get(\"answer\"):\n",
                "            results.append(f\"Summary: {response['answer']}\")\n",
                "        \n",
                "        for r in response.get(\"results\", []):\n",
                "            url = r.get('url', '')\n",
                "            urls.append(url)\n",
                "            results.append(f\"- {r.get('title', 'No title')}: {r.get('content', '')[:500]}... (Source: {url})\")\n",
                "        \n",
                "        return results, urls\n",
                "    except Exception as e:\n",
                "        return [f\"Search error: {str(e)}\"], []"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Node Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prompts\n",
                "RESEARCH_PROMPT = \"\"\"You are a research expert. Conduct comprehensive research on this question.\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "Generate 5 focused search queries to gather information from different angles.\n",
                "Return ONLY the search queries, one per line.\n",
                "\"\"\"\n",
                "\n",
                "DRAFT_PROMPT = \"\"\"You are a research analyst writing a comprehensive report.\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "Research Findings:\n",
                "{research_findings}\n",
                "\n",
                "Write a detailed research report (1000-1500 words) that:\n",
                "1. Directly addresses the research question\n",
                "2. Synthesizes findings from multiple sources\n",
                "3. Provides evidence for all claims\n",
                "4. Includes proper citations\n",
                "5. Acknowledges limitations or uncertainties\n",
                "\n",
                "Structure with clear sections and be comprehensive.\n",
                "\"\"\"\n",
                "\n",
                "CRITIQUE_PROMPT = \"\"\"You are a critical reviewer evaluating a research report.\n",
                "\n",
                "Original Question: {question}\n",
                "\n",
                "Report to Critique:\n",
                "{draft}\n",
                "\n",
                "Analyze this report and identify issues:\n",
                "\n",
                "1. WEAK CLAIMS: Statements without sufficient evidence\n",
                "2. MISSING EVIDENCE: Important points that need citations\n",
                "3. LOGICAL GAPS: Missing connections or reasoning holes\n",
                "4. UNCLEAR SECTIONS: Confusing or ambiguous passages\n",
                "\n",
                "For each issue, provide:\n",
                "- Type (weak_claim/missing_evidence/logical_gap/unclear)\n",
                "- Location (quote the problematic text)\n",
                "- Description of the problem\n",
                "- Specific suggestion for improvement\n",
                "\n",
                "Also provide an OVERALL QUALITY SCORE (1-10) and a summary.\n",
                "\n",
                "Format your response as:\n",
                "QUALITY SCORE: [1-10]\n",
                "\n",
                "ISSUES:\n",
                "1. [Type]: \"[Location]\" - [Description]. Suggestion: [Fix]\n",
                "2. ...\n",
                "\n",
                "SUMMARY: [Overall assessment]\n",
                "\"\"\"\n",
                "\n",
                "REVISE_PROMPT = \"\"\"You are a research analyst revising a report based on critical feedback.\n",
                "\n",
                "Original Question: {question}\n",
                "\n",
                "Current Draft:\n",
                "{current_draft}\n",
                "\n",
                "Critique Feedback:\n",
                "{critique}\n",
                "\n",
                "Revise the report to address ALL identified issues:\n",
                "- Fix weak claims with evidence\n",
                "- Add missing citations and evidence\n",
                "- Fill logical gaps with explanations\n",
                "- Clarify unclear sections\n",
                "\n",
                "Maintain the overall structure but improve quality. Do NOT shorten the report.\n",
                "Output the complete revised report.\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def conduct_research(state: IterativeRefinementState) -> dict:\n",
                "    \"\"\"Conduct initial research phase.\"\"\"\n",
                "    question = state[\"question\"]\n",
                "    \n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Research Phase\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    # Generate search queries\n",
                "    prompt = RESEARCH_PROMPT.format(question=question)\n",
                "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
                "    queries = [q.strip() for q in response.content.split(\"\\n\") if q.strip()][:5]\n",
                "    \n",
                "    # Execute searches\n",
                "    all_results = []\n",
                "    all_urls = []\n",
                "    \n",
                "    for query in queries:\n",
                "        print(f\"  Searching: {query[:50]}...\")\n",
                "        results, urls = search_web(query)\n",
                "        all_results.extend(results)\n",
                "        all_urls.extend(urls)\n",
                "    \n",
                "    print(f\"  Collected {len(all_results)} results from {len(set(all_urls))} sources\")\n",
                "    \n",
                "    return {\n",
                "        \"search_results\": all_results,\n",
                "        \"source_urls\": all_urls,\n",
                "        \"revision_count\": 0\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def generate_draft(state: IterativeRefinementState) -> dict:\n",
                "    \"\"\"Generate initial draft from research.\"\"\"\n",
                "    question = state[\"question\"]\n",
                "    search_results = state.get(\"search_results\", [])\n",
                "    \n",
                "    print(f\"\\n--- Generating Initial Draft ---\")\n",
                "    \n",
                "    prompt = DRAFT_PROMPT.format(\n",
                "        question=question,\n",
                "        research_findings=\"\\n\\n\".join(search_results[:25])\n",
                "    )\n",
                "    \n",
                "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
                "    \n",
                "    print(f\"  Draft generated: {len(response.content)} characters\")\n",
                "    \n",
                "    return {\n",
                "        \"current_draft\": response.content,\n",
                "        \"draft_history\": [f\"V0 (Initial): {len(response.content)} chars\"]\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def critique_draft(state: IterativeRefinementState) -> dict:\n",
                "    \"\"\"Critique the current draft.\"\"\"\n",
                "    question = state[\"question\"]\n",
                "    current_draft = state.get(\"current_draft\", \"\")\n",
                "    revision_count = state.get(\"revision_count\", 0)\n",
                "    \n",
                "    print(f\"\\n--- Critique Phase (Revision {revision_count}) ---\")\n",
                "    \n",
                "    prompt = CRITIQUE_PROMPT.format(\n",
                "        question=question,\n",
                "        draft=current_draft\n",
                "    )\n",
                "    \n",
                "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
                "    critique = response.content\n",
                "    \n",
                "    # Extract quality score\n",
                "    quality_score = 5.0  # Default\n",
                "    try:\n",
                "        import re\n",
                "        match = re.search(r'QUALITY SCORE:\\s*(\\d+\\.?\\d*)', critique)\n",
                "        if match:\n",
                "            quality_score = float(match.group(1))\n",
                "    except:\n",
                "        pass\n",
                "    \n",
                "    print(f\"  Quality score: {quality_score}/10\")\n",
                "    \n",
                "    return {\n",
                "        \"current_critique\": critique,\n",
                "        \"critique_history\": [f\"R{revision_count}: Score {quality_score}/10\"],\n",
                "        \"quality_scores\": [quality_score]\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def should_continue_refining(state: IterativeRefinementState) -> Literal[\"revise\", \"finalize\"]:\n",
                "    \"\"\"Decide whether to continue refining or finalize.\"\"\"\n",
                "    revision_count = state.get(\"revision_count\", 0)\n",
                "    quality_scores = state.get(\"quality_scores\", [])\n",
                "    \n",
                "    latest_score = quality_scores[-1] if quality_scores else 0\n",
                "    \n",
                "    # Stop conditions\n",
                "    if revision_count >= MAX_REVISIONS:\n",
                "        print(f\"  Max revisions ({MAX_REVISIONS}) reached. Finalizing.\")\n",
                "        return \"finalize\"\n",
                "    \n",
                "    if latest_score >= MIN_QUALITY_THRESHOLD:\n",
                "        print(f\"  Quality threshold ({MIN_QUALITY_THRESHOLD}) met. Finalizing.\")\n",
                "        return \"finalize\"\n",
                "    \n",
                "    print(f\"  Continuing refinement (score {latest_score} < threshold {MIN_QUALITY_THRESHOLD})\")\n",
                "    return \"revise\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def revise_draft(state: IterativeRefinementState) -> dict:\n",
                "    \"\"\"Revise the draft based on critique.\"\"\"\n",
                "    question = state[\"question\"]\n",
                "    current_draft = state.get(\"current_draft\", \"\")\n",
                "    critique = state.get(\"current_critique\", \"\")\n",
                "    revision_count = state.get(\"revision_count\", 0)\n",
                "    \n",
                "    print(f\"\\n--- Revision Phase ({revision_count + 1}/{MAX_REVISIONS}) ---\")\n",
                "    \n",
                "    prompt = REVISE_PROMPT.format(\n",
                "        question=question,\n",
                "        current_draft=current_draft,\n",
                "        critique=critique\n",
                "    )\n",
                "    \n",
                "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
                "    revised_draft = response.content\n",
                "    \n",
                "    # Calculate improvement\n",
                "    len_change = len(revised_draft) - len(current_draft)\n",
                "    print(f\"  Revised draft: {len(revised_draft)} chars ({'+' if len_change >= 0 else ''}{len_change})\")\n",
                "    \n",
                "    return {\n",
                "        \"current_draft\": revised_draft,\n",
                "        \"draft_history\": [f\"V{revision_count + 1}: {len(revised_draft)} chars\"],\n",
                "        \"revision_count\": revision_count + 1\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def finalize_report(state: IterativeRefinementState) -> dict:\n",
                "    \"\"\"Finalize the report.\"\"\"\n",
                "    current_draft = state.get(\"current_draft\", \"\")\n",
                "    revision_count = state.get(\"revision_count\", 0)\n",
                "    quality_scores = state.get(\"quality_scores\", [])\n",
                "    \n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Report Finalized\")\n",
                "    print(f\"{'='*60}\")\n",
                "    print(f\"  Total revisions: {revision_count}\")\n",
                "    print(f\"  Quality progression: {' -> '.join([f'{s:.1f}' for s in quality_scores])}\")\n",
                "    print(f\"  Final length: {len(current_draft)} characters\")\n",
                "    \n",
                "    return {\n",
                "        \"final_report\": current_draft\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Graph Construction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build the Iterative Refinement Research Agent graph\n",
                "ir_builder = StateGraph(IterativeRefinementState)\n",
                "\n",
                "# Add nodes\n",
                "ir_builder.add_node(\"conduct_research\", conduct_research)\n",
                "ir_builder.add_node(\"generate_draft\", generate_draft)\n",
                "ir_builder.add_node(\"critique_draft\", critique_draft)\n",
                "ir_builder.add_node(\"revise_draft\", revise_draft)\n",
                "ir_builder.add_node(\"finalize_report\", finalize_report)\n",
                "\n",
                "# Add edges\n",
                "ir_builder.add_edge(START, \"conduct_research\")\n",
                "ir_builder.add_edge(\"conduct_research\", \"generate_draft\")\n",
                "ir_builder.add_edge(\"generate_draft\", \"critique_draft\")\n",
                "\n",
                "# Conditional edge: revise or finalize\n",
                "ir_builder.add_conditional_edges(\n",
                "    \"critique_draft\",\n",
                "    should_continue_refining,\n",
                "    {\n",
                "        \"revise\": \"revise_draft\",\n",
                "        \"finalize\": \"finalize_report\"\n",
                "    }\n",
                ")\n",
                "\n",
                "# Loop back to critique after revision\n",
                "ir_builder.add_edge(\"revise_draft\", \"critique_draft\")\n",
                "ir_builder.add_edge(\"finalize_report\", END)\n",
                "\n",
                "# Compile\n",
                "iterative_refinement_graph = ir_builder.compile()\n",
                "\n",
                "print(\"Iterative Refinement Research Agent compiled successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the graph\n",
                "from IPython.display import Image, display\n",
                "\n",
                "try:\n",
                "    display(Image(iterative_refinement_graph.get_graph().draw_mermaid_png()))\n",
                "except Exception as e:\n",
                "    print(f\"Could not display graph: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Agent Wrapper for Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def iterative_refinement_agent(inputs: dict) -> dict:\n",
                "    \"\"\"\n",
                "    Wrapper function for Iterative Refinement research agent.\n",
                "    \n",
                "    Compatible with evaluation harness.\n",
                "    \n",
                "    Args:\n",
                "        inputs: Dictionary with 'question' key\n",
                "        \n",
                "    Returns:\n",
                "        Dictionary with 'output' key containing final report\n",
                "    \"\"\"\n",
                "    question = inputs.get(\"question\", \"\")\n",
                "    \n",
                "    # Run with recursion limit\n",
                "    result = asyncio.run(\n",
                "        iterative_refinement_graph.ainvoke(\n",
                "            {\"question\": question},\n",
                "            config={\"recursion_limit\": 50}\n",
                "        )\n",
                "    )\n",
                "    \n",
                "    return {\n",
                "        \"output\": result.get(\"final_report\", \"\"),\n",
                "        \"revision_count\": result.get(\"revision_count\", 0),\n",
                "        \"quality_scores\": result.get(\"quality_scores\", []),\n",
                "        \"source_urls\": result.get(\"source_urls\", [])\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Manual Test\n",
                "\n",
                "Run this cell to verify the agent works correctly with a simple test question."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple test\n",
                "test_question = \"What are the key benefits and challenges of using large language models in enterprise applications?\"\n",
                "\n",
                "print(f\"Testing Iterative Refinement Agent with question:\\n{test_question}\\n\")\n",
                "print(\"Running iterative research (this may take several minutes)...\\n\")\n",
                "\n",
                "try:\n",
                "    result = iterative_refinement_agent({\"question\": test_question})\n",
                "    \n",
                "    print(\"\\n\" + \"=\" * 80)\n",
                "    print(\"FINAL REPORT\")\n",
                "    print(\"=\" * 80)\n",
                "    print(result[\"output\"][:3000] + \"...\" if len(result[\"output\"]) > 3000 else result[\"output\"])\n",
                "    print(\"\\n\" + \"=\" * 80)\n",
                "    print(f\"Report length: {len(result['output'])} characters\")\n",
                "    print(f\"Total revisions: {result.get('revision_count', 0)}\")\n",
                "    print(f\"Quality progression: {result.get('quality_scores', [])}\")\n",
                "    print(f\"Unique sources: {len(set(result.get('source_urls', [])))}\")\n",
                "    print(\"Agent test PASSED ✓\")\n",
                "except Exception as e:\n",
                "    print(f\"Agent test FAILED: {e}\")\n",
                "    import traceback\n",
                "    traceback.print_exc()\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Evaluation Harness Integration\n",
                "\n",
                "Once the manual test passes, uncomment and run the cells below for full evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import evaluation harness and metrics\n",
                "import sys\n",
                "sys.path.insert(0, \"..\")\n",
                "from evaluation import (\n",
                "    ExperimentHarness, \n",
                "    fact_recall, \n",
                "    citation_precision,\n",
                "    coherence_judge, \n",
                "    depth_judge, \n",
                "    relevance_judge,\n",
                "    minimum_sources_check\n",
                ")\n",
                "\n",
                "# Initialize harness with the golden test dataset\n",
                "harness = ExperimentHarness(\n",
                "    dataset_path=\"../data/deep_research_agent_test_dataset.yaml\",\n",
                "    langsmith_dataset_name=\"deep-research-golden-v2\"\n",
                ")\n",
                "\n",
                "print(\"Evaluation harness initialized successfully!\")\n",
                "print(f\"Dataset: {harness.dataset_path}\")\n",
                "print(f\"LangSmith dataset name: {harness.langsmith_dataset_name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Full Evaluation on All 20 Questions\n",
                "# ⚠️ EXPENSIVE - Only uncomment when ready for full evaluation\n",
                "# Uncomment to run:\n",
                "\n",
                "# # Define comprehensive evaluator suite\n",
                "# evaluators = [\n",
                "#     fact_recall,              # Required facts coverage\n",
                "#     citation_precision,       # Citation URL validity\n",
                "#     minimum_sources_check,    # Minimum source count\n",
                "#     coherence_judge,          # Logical structure\n",
                "#     depth_judge,              # Analysis depth\n",
                "#     relevance_judge,          # Addresses question\n",
                "# ]\n",
                "# \n",
                "# # Run full evaluation\n",
                "# print(\"Starting FULL evaluation on all 20 questions...\")\n",
                "# print(\"Iterative Refinement Agent - this will take 1-2 hours.\")\n",
                "# print(\"=\" * 80 + \"\\n\")\n",
                "# \n",
                "# results = harness.run_evaluation(\n",
                "#     agent_fn=iterative_refinement_agent,\n",
                "#     evaluators=evaluators,\n",
                "#     experiment_name=\"iterative_refinement_v1\",\n",
                "#     monte_carlo_runs=1,  # Single run to reduce cost\n",
                "#     max_concurrency=2,   # Lower concurrency for stability\n",
                "#     description=\"Iterative Refinement paradigm evaluation on all difficulty tiers\"\n",
                "# )\n",
                "# \n",
                "# # Display comprehensive results\n",
                "# print(\"\\n\" + \"=\" * 80)\n",
                "# print(\"FULL EVALUATION RESULTS\")\n",
                "# print(\"=\" * 80)\n",
                "# print(f\"Experiment: {results.experiment_name}\")\n",
                "# print(f\"Questions evaluated: {results.num_questions}\")\n",
                "# print(f\"Runs per question: {results.num_runs}\")\n",
                "# \n",
                "# print(f\"\\n{'Metric':<30} {'Mean':<10}\")\n",
                "# print(\"-\" * 40)\n",
                "# for metric_name in sorted(results.metrics.keys()):\n",
                "#     if not metric_name.endswith('_std'):\n",
                "#         value = results.metrics.get(metric_name, 0)\n",
                "#         print(f\"{metric_name:<30} {value:<10.3f}\")\n",
                "# \n",
                "# # Save results to file\n",
                "# import json\n",
                "# from datetime import datetime\n",
                "# \n",
                "# results_file = Path(\"../results\") / f\"iterative_refinement_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
                "# results_file.parent.mkdir(exist_ok=True)\n",
                "# \n",
                "# with open(results_file, 'w') as f:\n",
                "#     json.dump({\n",
                "#         \"experiment_name\": results.experiment_name,\n",
                "#         \"num_questions\": results.num_questions,\n",
                "#         \"num_runs\": results.num_runs,\n",
                "#         \"metrics\": results.metrics,\n",
                "#         \"per_question\": results.per_question_results\n",
                "#     }, f, indent=2)\n",
                "# \n",
                "# print(f\"\\nResults saved to: {results_file}\")\n",
                "\n",
                "print(\"Full evaluation cell ready. Uncomment to run when ready.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}