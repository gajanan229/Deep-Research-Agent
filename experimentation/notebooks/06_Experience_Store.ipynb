{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Paradigm 06: Experience Store Research Agent\n",
                "\n",
                "This notebook implements the **Experience Store** paradigm from the Research Paradigms document.\n",
                "\n",
                "## Core Concept\n",
                "\n",
                "The Experience Store enables non-parametric learning through case-based reasoning:\n",
                "- **Case Storage**: Store successful research trajectories and outcomes\n",
                "- **Case Retrieval**: Find similar past cases for new queries\n",
                "- **Strategy Adaptation**: Apply learned strategies to new problems\n",
                "\n",
                "## Literature Validation\n",
                "\n",
                "> \"DS-Agent, Agent K, AgentRxiv... demonstrate non-parametric learning through experience storage, achieving 21 gold medals in Kaggle competitions through stored strategies.\" —Feasibility Report\n",
                "\n",
                "## Technology Stack\n",
                "\n",
                "- **LLM**: `gpt-5-mini-2025-08-07`\n",
                "- **Web Search**: Tavily API\n",
                "- **Tracing**: LangSmith\n",
                "- **Framework**: LangGraph\n",
                "- **Case Storage**: In-memory dictionary (MVP)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import operator\n",
                "import asyncio\n",
                "import hashlib\n",
                "import json\n",
                "from pathlib import Path\n",
                "from typing import List, Annotated, TypedDict, Literal, Optional, Dict, Any\n",
                "from datetime import datetime\n",
                "\n",
                "from dotenv import load_dotenv\n",
                "from pydantic import BaseModel, Field\n",
                "\n",
                "from langchain_openai import ChatOpenAI\n",
                "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
                "from tavily import TavilyClient\n",
                "\n",
                "from langgraph.graph import StateGraph, START, END\n",
                "\n",
                "# Load environment variables\n",
                "env_path = Path(\"../.env\")\n",
                "load_dotenv(env_path)\n",
                "\n",
                "# Configure LangSmith tracing\n",
                "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
                "os.environ[\"LANGSMITH_PROJECT\"] = \"deep_research_new\"\n",
                "\n",
                "print(\"Environment configured successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize LLM and Tavily client\n",
                "MODEL_NAME = \"gpt-5-mini-2025-08-07\"\n",
                "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
                "tavily_client = TavilyClient()\n",
                "\n",
                "# Experience Store Configuration\n",
                "EXPERIENCE_STORE_PATH = Path(\"../data/experience_store.json\")\n",
                "MAX_RETRIEVED_CASES = 3  # Number of similar cases to retrieve\n",
                "SIMILARITY_THRESHOLD = 0.3  # Minimum similarity for case retrieval\n",
                "\n",
                "print(f\"Using model: {MODEL_NAME}\")\n",
                "print(f\"Experience store: {EXPERIENCE_STORE_PATH}\")\n",
                "print(f\"Max retrieved cases: {MAX_RETRIEVED_CASES}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Experience Store Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ExperienceCase(BaseModel):\n",
                "    \"\"\"A stored experience case.\"\"\"\n",
                "    case_id: str = Field(description=\"Unique case identifier\")\n",
                "    question: str = Field(description=\"The research question\")\n",
                "    category: str = Field(default=\"general\", description=\"Question category\")\n",
                "    keywords: List[str] = Field(default_factory=list, description=\"Key terms\")\n",
                "    successful_strategies: List[str] = Field(default_factory=list, description=\"What worked\")\n",
                "    search_queries: List[str] = Field(default_factory=list, description=\"Effective queries\")\n",
                "    report_structure: List[str] = Field(default_factory=list, description=\"Section structure\")\n",
                "    quality_score: float = Field(default=0.0, description=\"Final quality score\")\n",
                "    timestamp: str = Field(default=\"\", description=\"When this case was created\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ExperienceStore:\n",
                "    \"\"\"Simple in-memory experience store with persistence.\"\"\"\n",
                "    \n",
                "    def __init__(self, store_path: Path):\n",
                "        self.store_path = store_path\n",
                "        self.cases: Dict[str, ExperienceCase] = {}\n",
                "        self._load()\n",
                "    \n",
                "    def _load(self):\n",
                "        \"\"\"Load cases from disk.\"\"\"\n",
                "        if self.store_path.exists():\n",
                "            try:\n",
                "                with open(self.store_path, 'r') as f:\n",
                "                    data = json.load(f)\n",
                "                    for case_id, case_data in data.items():\n",
                "                        self.cases[case_id] = ExperienceCase(**case_data)\n",
                "                print(f\"Loaded {len(self.cases)} cases from experience store\")\n",
                "            except Exception as e:\n",
                "                print(f\"Error loading experience store: {e}\")\n",
                "                self.cases = {}\n",
                "        else:\n",
                "            print(\"No existing experience store found. Starting fresh.\")\n",
                "    \n",
                "    def _save(self):\n",
                "        \"\"\"Save cases to disk.\"\"\"\n",
                "        try:\n",
                "            self.store_path.parent.mkdir(parents=True, exist_ok=True)\n",
                "            with open(self.store_path, 'w') as f:\n",
                "                data = {k: v.model_dump() for k, v in self.cases.items()}\n",
                "                json.dump(data, f, indent=2)\n",
                "        except Exception as e:\n",
                "            print(f\"Error saving experience store: {e}\")\n",
                "    \n",
                "    def _generate_case_id(self, question: str) -> str:\n",
                "        \"\"\"Generate unique case ID from question.\"\"\"\n",
                "        return hashlib.md5(question.encode()).hexdigest()[:12]\n",
                "    \n",
                "    def _calculate_similarity(self, query_keywords: List[str], case: ExperienceCase) -> float:\n",
                "        \"\"\"Calculate keyword-based similarity.\"\"\"\n",
                "        if not query_keywords or not case.keywords:\n",
                "            return 0.0\n",
                "        \n",
                "        query_set = set(kw.lower() for kw in query_keywords)\n",
                "        case_set = set(kw.lower() for kw in case.keywords)\n",
                "        \n",
                "        intersection = len(query_set & case_set)\n",
                "        union = len(query_set | case_set)\n",
                "        \n",
                "        return intersection / union if union > 0 else 0.0\n",
                "    \n",
                "    def store_case(self, case: ExperienceCase) -> str:\n",
                "        \"\"\"Store a new experience case.\"\"\"\n",
                "        if not case.case_id:\n",
                "            case.case_id = self._generate_case_id(case.question)\n",
                "        if not case.timestamp:\n",
                "            case.timestamp = datetime.now().isoformat()\n",
                "        \n",
                "        self.cases[case.case_id] = case\n",
                "        self._save()\n",
                "        \n",
                "        return case.case_id\n",
                "    \n",
                "    def retrieve_similar(self, keywords: List[str], max_cases: int = 3) -> List[ExperienceCase]:\n",
                "        \"\"\"Retrieve similar cases based on keywords.\"\"\"\n",
                "        if not self.cases:\n",
                "            return []\n",
                "        \n",
                "        scored_cases = []\n",
                "        for case in self.cases.values():\n",
                "            similarity = self._calculate_similarity(keywords, case)\n",
                "            if similarity >= SIMILARITY_THRESHOLD:\n",
                "                scored_cases.append((similarity, case))\n",
                "        \n",
                "        # Sort by similarity and quality score\n",
                "        scored_cases.sort(key=lambda x: (x[0], x[1].quality_score), reverse=True)\n",
                "        \n",
                "        return [case for _, case in scored_cases[:max_cases]]\n",
                "    \n",
                "    def get_stats(self) -> dict:\n",
                "        \"\"\"Get store statistics.\"\"\"\n",
                "        if not self.cases:\n",
                "            return {\"total_cases\": 0}\n",
                "        \n",
                "        scores = [c.quality_score for c in self.cases.values()]\n",
                "        return {\n",
                "            \"total_cases\": len(self.cases),\n",
                "            \"avg_quality\": sum(scores) / len(scores) if scores else 0,\n",
                "            \"categories\": list(set(c.category for c in self.cases.values()))\n",
                "        }\n",
                "\n",
                "# Initialize global experience store\n",
                "experience_store = ExperienceStore(EXPERIENCE_STORE_PATH)\n",
                "print(f\"Store stats: {experience_store.get_stats()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. State Definitions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ExperienceStoreState(TypedDict):\n",
                "    \"\"\"State for the Experience Store Research Agent.\"\"\"\n",
                "    # Input\n",
                "    question: str\n",
                "    \n",
                "    # Experience retrieval\n",
                "    query_keywords: List[str]\n",
                "    retrieved_cases: List[dict]  # Serialized ExperienceCase objects\n",
                "    adapted_strategy: str\n",
                "    \n",
                "    # Research execution\n",
                "    search_queries: List[str]\n",
                "    search_results: Annotated[List[str], operator.add]\n",
                "    source_urls: Annotated[List[str], operator.add]\n",
                "    \n",
                "    # Report\n",
                "    report_structure: List[str]\n",
                "    final_report: str\n",
                "    quality_score: float\n",
                "    \n",
                "    # Case creation\n",
                "    successful_strategies: List[str]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def search_web(query: str, max_results: int = 10) -> tuple[List[str], List[str]]:\n",
                "    \"\"\"Execute web search using Tavily. Returns (results, urls).\"\"\"\n",
                "    try:\n",
                "        if len(query) > 400:\n",
                "            query = query[:400]\n",
                "        \n",
                "        response = tavily_client.search(\n",
                "            query=query,\n",
                "            max_results=max_results,\n",
                "            include_answer=True\n",
                "        )\n",
                "        \n",
                "        results = []\n",
                "        urls = []\n",
                "        \n",
                "        if response.get(\"answer\"):\n",
                "            results.append(f\"Summary: {response['answer']}\")\n",
                "        \n",
                "        for r in response.get(\"results\", []):\n",
                "            url = r.get('url', '')\n",
                "            urls.append(url)\n",
                "            results.append(f\"- {r.get('title', 'No title')}: {r.get('content', '')[:500]}... (Source: {url})\")\n",
                "        \n",
                "        return results, urls\n",
                "    except Exception as e:\n",
                "        return [f\"Search error: {str(e)}\"], []"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Node Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prompts\n",
                "EXTRACT_KEYWORDS_PROMPT = \"\"\"Extract 5-10 key terms from this research question that would help find similar past research:\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "Return ONLY the keywords, one per line. Include:\n",
                "- Main topic terms\n",
                "- Domain/field terms\n",
                "- Key concepts\n",
                "\"\"\"\n",
                "\n",
                "ADAPT_STRATEGY_PROMPT = \"\"\"You are adapting successful research strategies from similar past cases.\n",
                "\n",
                "Current Question: {question}\n",
                "\n",
                "Similar Past Cases:\n",
                "{past_cases}\n",
                "\n",
                "Based on these past successes, recommend:\n",
                "1. Search query patterns that worked well\n",
                "2. Report structure that was effective\n",
                "3. Key strategies to apply\n",
                "\n",
                "Be specific and actionable.\n",
                "\"\"\"\n",
                "\n",
                "GENERATE_QUERIES_PROMPT = \"\"\"Generate search queries for this research question.\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "Strategy guidance from past experience:\n",
                "{strategy}\n",
                "\n",
                "Generate 5-7 effective search queries. Apply the patterns that worked before.\n",
                "Return ONLY the search queries, one per line.\n",
                "\"\"\"\n",
                "\n",
                "WRITE_REPORT_PROMPT = \"\"\"Write a comprehensive research report.\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "Strategy guidance:\n",
                "{strategy}\n",
                "\n",
                "Suggested structure:\n",
                "{structure}\n",
                "\n",
                "Research Findings:\n",
                "{findings}\n",
                "\n",
                "Write a detailed report (1000-1500 words) following the suggested structure.\n",
                "Include citations and evidence from the research.\n",
                "\"\"\"\n",
                "\n",
                "EVALUATE_REPORT_PROMPT = \"\"\"Evaluate this research report on a scale of 1-10.\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "Report:\n",
                "{report}\n",
                "\n",
                "Rate on:\n",
                "1. Relevance to question (1-10)\n",
                "2. Evidence quality (1-10)\n",
                "3. Coherence (1-10)\n",
                "4. Completeness (1-10)\n",
                "\n",
                "Respond with ONLY four numbers separated by commas (e.g., \"8, 7, 9, 8\").\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def extract_keywords(state: ExperienceStoreState) -> dict:\n",
                "    \"\"\"Extract keywords from the question for case retrieval.\"\"\"\n",
                "    question = state[\"question\"]\n",
                "    \n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Experience Store: Extracting Keywords\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    prompt = EXTRACT_KEYWORDS_PROMPT.format(question=question)\n",
                "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
                "    \n",
                "    keywords = [kw.strip() for kw in response.content.split(\"\\n\") if kw.strip()]\n",
                "    print(f\"  Keywords: {keywords[:5]}...\")\n",
                "    \n",
                "    return {\n",
                "        \"query_keywords\": keywords\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def retrieve_cases(state: ExperienceStoreState) -> dict:\n",
                "    \"\"\"Retrieve similar cases from experience store.\"\"\"\n",
                "    keywords = state.get(\"query_keywords\", [])\n",
                "    \n",
                "    print(f\"\\n--- Retrieving Similar Cases ---\")\n",
                "    \n",
                "    similar_cases = experience_store.retrieve_similar(keywords, MAX_RETRIEVED_CASES)\n",
                "    \n",
                "    if similar_cases:\n",
                "        print(f\"  Found {len(similar_cases)} similar cases:\")\n",
                "        for case in similar_cases:\n",
                "            print(f\"    - {case.question[:50]}... (score: {case.quality_score:.1f})\")\n",
                "    else:\n",
                "        print(\"  No similar cases found. Starting fresh.\")\n",
                "    \n",
                "    # Serialize cases for state\n",
                "    serialized = [case.model_dump() for case in similar_cases]\n",
                "    \n",
                "    return {\n",
                "        \"retrieved_cases\": serialized\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def adapt_strategy(state: ExperienceStoreState) -> dict:\n",
                "    \"\"\"Adapt strategies from retrieved cases.\"\"\"\n",
                "    question = state[\"question\"]\n",
                "    retrieved_cases = state.get(\"retrieved_cases\", [])\n",
                "    \n",
                "    print(f\"\\n--- Adapting Strategy ---\")\n",
                "    \n",
                "    if not retrieved_cases:\n",
                "        # Default strategy if no cases found\n",
                "        strategy = \"No prior experience available. Use standard research approach.\"\n",
                "        structure = [\"Introduction\", \"Background\", \"Analysis\", \"Findings\", \"Conclusion\"]\n",
                "    else:\n",
                "        # Format past cases for LLM\n",
                "        cases_text = \"\"\n",
                "        for i, case in enumerate(retrieved_cases, 1):\n",
                "            cases_text += f\"\\nCase {i}: {case['question'][:100]}...\\n\"\n",
                "            cases_text += f\"  Strategies: {', '.join(case.get('successful_strategies', [])[:3])}\\n\"\n",
                "            cases_text += f\"  Structure: {', '.join(case.get('report_structure', [])[:5])}\\n\"\n",
                "            cases_text += f\"  Quality: {case.get('quality_score', 0):.1f}/10\\n\"\n",
                "        \n",
                "        prompt = ADAPT_STRATEGY_PROMPT.format(\n",
                "            question=question,\n",
                "            past_cases=cases_text\n",
                "        )\n",
                "        \n",
                "        response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
                "        strategy = response.content\n",
                "        \n",
                "        # Extract structure from best case\n",
                "        best_case = max(retrieved_cases, key=lambda c: c.get('quality_score', 0))\n",
                "        structure = best_case.get('report_structure', [\"Introduction\", \"Analysis\", \"Conclusion\"])\n",
                "    \n",
                "    print(f\"  Strategy adapted (length: {len(strategy)} chars)\")\n",
                "    \n",
                "    return {\n",
                "        \"adapted_strategy\": strategy,\n",
                "        \"report_structure\": structure\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def execute_research(state: ExperienceStoreState) -> dict:\n",
                "    \"\"\"Execute research with adapted strategy.\"\"\"\n",
                "    question = state[\"question\"]\n",
                "    strategy = state.get(\"adapted_strategy\", \"\")\n",
                "    \n",
                "    print(f\"\\n--- Executing Research ---\")\n",
                "    \n",
                "    # Generate queries using strategy\n",
                "    prompt = GENERATE_QUERIES_PROMPT.format(\n",
                "        question=question,\n",
                "        strategy=strategy[:500]\n",
                "    )\n",
                "    \n",
                "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
                "    queries = [q.strip() for q in response.content.split(\"\\n\") if q.strip()][:7]\n",
                "    \n",
                "    # Execute searches\n",
                "    all_results = []\n",
                "    all_urls = []\n",
                "    \n",
                "    for query in queries:\n",
                "        print(f\"  Searching: {query[:50]}...\")\n",
                "        results, urls = search_web(query)\n",
                "        all_results.extend(results)\n",
                "        all_urls.extend(urls)\n",
                "    \n",
                "    print(f\"  Collected {len(all_results)} results from {len(set(all_urls))} sources\")\n",
                "    \n",
                "    return {\n",
                "        \"search_queries\": queries,\n",
                "        \"search_results\": all_results,\n",
                "        \"source_urls\": all_urls\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def write_report(state: ExperienceStoreState) -> dict:\n",
                "    \"\"\"Write report using adapted structure.\"\"\"\n",
                "    question = state[\"question\"]\n",
                "    strategy = state.get(\"adapted_strategy\", \"\")\n",
                "    structure = state.get(\"report_structure\", [])\n",
                "    search_results = state.get(\"search_results\", [])\n",
                "    \n",
                "    print(f\"\\n--- Writing Report ---\")\n",
                "    \n",
                "    prompt = WRITE_REPORT_PROMPT.format(\n",
                "        question=question,\n",
                "        strategy=strategy[:500],\n",
                "        structure=\"\\n\".join(f\"- {s}\" for s in structure),\n",
                "        findings=\"\\n\\n\".join(search_results[:25])\n",
                "    )\n",
                "    \n",
                "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
                "    \n",
                "    print(f\"  Report generated: {len(response.content)} chars\")\n",
                "    \n",
                "    return {\n",
                "        \"final_report\": response.content\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def evaluate_and_store(state: ExperienceStoreState) -> dict:\n",
                "    \"\"\"Evaluate report and store experience.\"\"\"\n",
                "    question = state[\"question\"]\n",
                "    report = state.get(\"final_report\", \"\")\n",
                "    keywords = state.get(\"query_keywords\", [])\n",
                "    queries = state.get(\"search_queries\", [])\n",
                "    structure = state.get(\"report_structure\", [])\n",
                "    \n",
                "    print(f\"\\n--- Evaluating and Storing Experience ---\")\n",
                "    \n",
                "    # Evaluate report\n",
                "    eval_prompt = EVALUATE_REPORT_PROMPT.format(\n",
                "        question=question,\n",
                "        report=report[:3000]\n",
                "    )\n",
                "    \n",
                "    response = await llm.ainvoke([HumanMessage(content=eval_prompt)])\n",
                "    \n",
                "    # Parse scores\n",
                "    try:\n",
                "        scores = [float(s.strip()) for s in response.content.split(\",\")][:4]\n",
                "        quality_score = sum(scores) / len(scores)\n",
                "    except:\n",
                "        quality_score = 6.0\n",
                "        scores = [6, 6, 6, 6]\n",
                "    \n",
                "    print(f\"  Quality scores: {scores}\")\n",
                "    print(f\"  Average: {quality_score:.1f}/10\")\n",
                "    \n",
                "    # Identify successful strategies\n",
                "    strategies = []\n",
                "    if quality_score >= 7:\n",
                "        strategies.append(\"Used comprehensive search queries\")\n",
                "        strategies.append(f\"Applied {len(queries)} targeted searches\")\n",
                "        if state.get(\"retrieved_cases\"):\n",
                "            strategies.append(\"Adapted strategies from similar past cases\")\n",
                "    \n",
                "    # Store the case\n",
                "    case = ExperienceCase(\n",
                "        case_id=\"\",\n",
                "        question=question,\n",
                "        category=\"research\",\n",
                "        keywords=keywords,\n",
                "        successful_strategies=strategies,\n",
                "        search_queries=queries,\n",
                "        report_structure=structure,\n",
                "        quality_score=quality_score\n",
                "    )\n",
                "    \n",
                "    case_id = experience_store.store_case(case)\n",
                "    print(f\"  Stored as case: {case_id}\")\n",
                "    print(f\"  Total cases in store: {len(experience_store.cases)}\")\n",
                "    \n",
                "    return {\n",
                "        \"quality_score\": quality_score,\n",
                "        \"successful_strategies\": strategies\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Graph Construction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build the Experience Store Research Agent graph\n",
                "es_builder = StateGraph(ExperienceStoreState)\n",
                "\n",
                "# Add nodes\n",
                "es_builder.add_node(\"extract_keywords\", extract_keywords)\n",
                "es_builder.add_node(\"retrieve_cases\", retrieve_cases)\n",
                "es_builder.add_node(\"adapt_strategy\", adapt_strategy)\n",
                "es_builder.add_node(\"execute_research\", execute_research)\n",
                "es_builder.add_node(\"write_report\", write_report)\n",
                "es_builder.add_node(\"evaluate_and_store\", evaluate_and_store)\n",
                "\n",
                "# Add edges - linear flow\n",
                "es_builder.add_edge(START, \"extract_keywords\")\n",
                "es_builder.add_edge(\"extract_keywords\", \"retrieve_cases\")\n",
                "es_builder.add_edge(\"retrieve_cases\", \"adapt_strategy\")\n",
                "es_builder.add_edge(\"adapt_strategy\", \"execute_research\")\n",
                "es_builder.add_edge(\"execute_research\", \"write_report\")\n",
                "es_builder.add_edge(\"write_report\", \"evaluate_and_store\")\n",
                "es_builder.add_edge(\"evaluate_and_store\", END)\n",
                "\n",
                "# Compile\n",
                "experience_store_graph = es_builder.compile()\n",
                "\n",
                "print(\"Experience Store Research Agent compiled successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the graph\n",
                "from IPython.display import Image, display\n",
                "\n",
                "try:\n",
                "    display(Image(experience_store_graph.get_graph().draw_mermaid_png()))\n",
                "except Exception as e:\n",
                "    print(f\"Could not display graph: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Agent Wrapper for Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def experience_store_agent(inputs: dict) -> dict:\n",
                "    \"\"\"\n",
                "    Wrapper function for Experience Store research agent.\n",
                "    \n",
                "    Compatible with evaluation harness.\n",
                "    \n",
                "    Args:\n",
                "        inputs: Dictionary with 'question' key\n",
                "        \n",
                "    Returns:\n",
                "        Dictionary with 'output' key containing final report\n",
                "    \"\"\"\n",
                "    question = inputs.get(\"question\", \"\")\n",
                "    \n",
                "    # Run with recursion limit\n",
                "    result = asyncio.run(\n",
                "        experience_store_graph.ainvoke(\n",
                "            {\"question\": question},\n",
                "            config={\"recursion_limit\": 50}\n",
                "        )\n",
                "    )\n",
                "    \n",
                "    return {\n",
                "        \"output\": result.get(\"final_report\", \"\"),\n",
                "        \"quality_score\": result.get(\"quality_score\", 0),\n",
                "        \"retrieved_cases\": len(result.get(\"retrieved_cases\", [])),\n",
                "        \"source_urls\": result.get(\"source_urls\", [])\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Manual Test\n",
                "\n",
                "Run this cell to verify the agent works correctly with a simple test question."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple test\n",
                "test_question = \"What are the key benefits and challenges of using large language models in enterprise applications?\"\n",
                "\n",
                "print(f\"Testing Experience Store Agent with question:\\n{test_question}\\n\")\n",
                "print(\"Running experience-guided research (this may take several minutes)...\\n\")\n",
                "\n",
                "try:\n",
                "    result = experience_store_agent({\"question\": test_question})\n",
                "    \n",
                "    print(\"\\n\" + \"=\" * 80)\n",
                "    print(\"FINAL REPORT\")\n",
                "    print(\"=\" * 80)\n",
                "    print(result[\"output\"][:3000] + \"...\" if len(result[\"output\"]) > 3000 else result[\"output\"])\n",
                "    print(\"\\n\" + \"=\" * 80)\n",
                "    print(f\"Report length: {len(result['output'])} characters\")\n",
                "    print(f\"Quality score: {result.get('quality_score', 0):.1f}/10\")\n",
                "    print(f\"Similar cases used: {result.get('retrieved_cases', 0)}\")\n",
                "    print(f\"Unique sources: {len(set(result.get('source_urls', [])))}\")\n",
                "    print(f\"\\nExperience Store Stats: {experience_store.get_stats()}\")\n",
                "    print(\"Agent test PASSED ✓\")\n",
                "except Exception as e:\n",
                "    print(f\"Agent test FAILED: {e}\")\n",
                "    import traceback\n",
                "    traceback.print_exc()\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Evaluation Harness Integration\n",
                "\n",
                "Once the manual test passes, uncomment and run the cells below for full evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import evaluation harness and metrics\n",
                "import sys\n",
                "sys.path.insert(0, \"..\")\n",
                "from evaluation import (\n",
                "    ExperimentHarness, \n",
                "    fact_recall, \n",
                "    citation_precision,\n",
                "    coherence_judge, \n",
                "    depth_judge, \n",
                "    relevance_judge,\n",
                "    minimum_sources_check\n",
                ")\n",
                "\n",
                "# Initialize harness with the golden test dataset\n",
                "harness = ExperimentHarness(\n",
                "    dataset_path=\"../data/deep_research_agent_test_dataset.yaml\",\n",
                "    langsmith_dataset_name=\"deep-research-golden-v2\"\n",
                ")\n",
                "\n",
                "print(\"Evaluation harness initialized successfully!\")\n",
                "print(f\"Dataset: {harness.dataset_path}\")\n",
                "print(f\"LangSmith dataset name: {harness.langsmith_dataset_name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Full Evaluation on All 20 Questions\n",
                "# ⚠️ EXPENSIVE - Only uncomment when ready for full evaluation\n",
                "# NOTE: The Experience Store improves over time as it accumulates cases.\n",
                "# Running the full evaluation will populate the store with experiences.\n",
                "# Uncomment to run:\n",
                "\n",
                "# # Define comprehensive evaluator suite\n",
                "# evaluators = [\n",
                "#     fact_recall,              # Required facts coverage\n",
                "#     citation_precision,       # Citation URL validity\n",
                "#     minimum_sources_check,    # Minimum source count\n",
                "#     coherence_judge,          # Logical structure\n",
                "#     depth_judge,              # Analysis depth\n",
                "#     relevance_judge,          # Addresses question\n",
                "# ]\n",
                "# \n",
                "# # Run full evaluation\n",
                "# print(\"Starting FULL evaluation on all 20 questions...\")\n",
                "# print(\"Experience Store Agent - this will take 1-2 hours.\")\n",
                "# print(f\"Starting with {len(experience_store.cases)} existing cases.\")\n",
                "# print(\"=\" * 80 + \"\\n\")\n",
                "# \n",
                "# results = harness.run_evaluation(\n",
                "#     agent_fn=experience_store_agent,\n",
                "#     evaluators=evaluators,\n",
                "#     experiment_name=\"experience_store_v1\",\n",
                "#     monte_carlo_runs=1,  # Single run to reduce cost\n",
                "#     max_concurrency=2,   # Lower concurrency for stability\n",
                "#     description=\"Experience Store paradigm evaluation on all difficulty tiers\"\n",
                "# )\n",
                "# \n",
                "# # Display comprehensive results\n",
                "# print(\"\\n\" + \"=\" * 80)\n",
                "# print(\"FULL EVALUATION RESULTS\")\n",
                "# print(\"=\" * 80)\n",
                "# print(f\"Experiment: {results.experiment_name}\")\n",
                "# print(f\"Questions evaluated: {results.num_questions}\")\n",
                "# print(f\"Runs per question: {results.num_runs}\")\n",
                "# print(f\"\\nExperience Store now contains: {len(experience_store.cases)} cases\")\n",
                "# \n",
                "# print(f\"\\n{'Metric':<30} {'Mean':<10}\")\n",
                "# print(\"-\" * 40)\n",
                "# for metric_name in sorted(results.metrics.keys()):\n",
                "#     if not metric_name.endswith('_std'):\n",
                "#         value = results.metrics.get(metric_name, 0)\n",
                "#         print(f\"{metric_name:<30} {value:<10.3f}\")\n",
                "# \n",
                "# # Save results to file\n",
                "# import json\n",
                "# from datetime import datetime\n",
                "# \n",
                "# results_file = Path(\"../results\") / f\"experience_store_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
                "# results_file.parent.mkdir(exist_ok=True)\n",
                "# \n",
                "# with open(results_file, 'w') as f:\n",
                "#     json.dump({\n",
                "#         \"experiment_name\": results.experiment_name,\n",
                "#         \"num_questions\": results.num_questions,\n",
                "#         \"num_runs\": results.num_runs,\n",
                "#         \"metrics\": results.metrics,\n",
                "#         \"per_question\": results.per_question_results,\n",
                "#         \"final_store_size\": len(experience_store.cases)\n",
                "#     }, f, indent=2)\n",
                "# \n",
                "# print(f\"\\nResults saved to: {results_file}\")\n",
                "\n",
                "print(\"Full evaluation cell ready. Uncomment to run when ready.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}