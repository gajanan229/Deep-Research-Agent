{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline B: Multi-Agent Supervisor Research System\n",
    "\n",
    "This notebook implements a sophisticated multi-agent research system with:\n",
    "1. **User Clarification** - Interactive scoping to understand research needs\n",
    "2. **Research Brief Generation** - Structured research question creation\n",
    "3. **Multi-Agent Supervisor** - Coordinates parallel research agents\n",
    "4. **Research Agents** - Specialized sub-agents for focused topics\n",
    "5. **Final Report Synthesis** - Comprehensive report generation\n",
    "\n",
    "**Technology Stack:**\n",
    "- LLM: `gpt-5-mini-2025-08-07`\n",
    "- Web Search: Tavily API\n",
    "- Tracing: LangSmith\n",
    "- Framework: LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nest_asyncio applied for Jupyter compatibility\n",
      "Environment configured successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import operator\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Annotated, Sequence, Literal\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import (\n",
    "    AIMessage, \n",
    "    HumanMessage, \n",
    "    SystemMessage, \n",
    "    BaseMessage,\n",
    "    ToolMessage,\n",
    "    get_buffer_string,\n",
    "    filter_messages\n",
    ")\n",
    "from langchain_core.tools import tool, InjectedToolArg\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.types import Command\n",
    "from tavily import TavilyClient\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Ensure async compatibility for Jupyter environments\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    print(\"nest_asyncio applied for Jupyter compatibility\")\n",
    "except ImportError:\n",
    "    print(\"nest_asyncio not available - install with: pip install nest_asyncio\")\n",
    "\n",
    "# Load environment variables\n",
    "env_path = Path(\"../.env\")\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Verify API keys\n",
    "assert os.environ.get(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY not set\"\n",
    "assert os.environ.get(\"TAVILY_API_KEY\"), \"TAVILY_API_KEY not set\"\n",
    "assert os.environ.get(\"LANGSMITH_API_KEY\"), \"LANGSMITH_API_KEY not set\"\n",
    "\n",
    "# Configure LangSmith tracing\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"deep_research_new\"\n",
    "\n",
    "print(\"Environment configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-5-mini-2025-08-07\n"
     ]
    }
   ],
   "source": [
    "# Model Configuration\n",
    "MODEL_NAME = \"gpt-5-mini-2025-08-07\"\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "summarization_llm = ChatOpenAI(model=MODEL_NAME, temperature=0, max_tokens=4000)\n",
    "compress_llm = ChatOpenAI(model=MODEL_NAME, temperature=0, max_tokens=32000)\n",
    "writer_llm = ChatOpenAI(model=MODEL_NAME, temperature=0, max_tokens=32000)\n",
    "\n",
    "# Tavily client\n",
    "tavily_client = TavilyClient()\n",
    "\n",
    "# System constants\n",
    "max_researcher_iterations = 6\n",
    "max_concurrent_researchers = 3\n",
    "\n",
    "print(f\"Using model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prompt templates for the deep research system.\n",
    "\n",
    "This module contains all prompt templates used across the research workflow components,\n",
    "including user clarification, research brief generation, and report synthesis.\n",
    "\"\"\"\n",
    "\n",
    "clarify_with_user_instructions=\"\"\"\n",
    "These are the messages that have been exchanged so far from the user asking for the report:\n",
    "<Messages>\n",
    "{messages}\n",
    "</Messages>\n",
    "\n",
    "Today's date is {date}.\n",
    "\n",
    "Assess whether you need to ask a clarifying question, or if the user has already provided enough information for you to start research.\n",
    "IMPORTANT: If you can see in the messages history that you have already asked a clarifying question, you almost always do not need to ask another one. Only ask another question if ABSOLUTELY NECESSARY.\n",
    "\n",
    "If there are acronyms, abbreviations, or unknown terms, ask the user to clarify.\n",
    "If you need to ask a question, follow these guidelines:\n",
    "- Be concise while gathering all necessary information\n",
    "- Make sure to gather all the information needed to carry out the research task in a concise, well-structured manner.\n",
    "- Use bullet points or numbered lists if appropriate for clarity. Make sure that this uses markdown formatting and will be rendered correctly if the string output is passed to a markdown renderer.\n",
    "- Don't ask for unnecessary information, or information that the user has already provided. If you can see that the user has already provided the information, do not ask for it again.\n",
    "\n",
    "Respond in valid JSON format with these exact keys:\n",
    "\"need_clarification\": boolean,\n",
    "\"question\": \"<question to ask the user to clarify the report scope>\",\n",
    "\"verification\": \"<verification message that we will start research>\"\n",
    "\n",
    "If you need to ask a clarifying question, return:\n",
    "\"need_clarification\": true,\n",
    "\"question\": \"<your clarifying question>\",\n",
    "\"verification\": \"\"\n",
    "\n",
    "If you do not need to ask a clarifying question, return:\n",
    "\"need_clarification\": false,\n",
    "\"question\": \"\",\n",
    "\"verification\": \"<acknowledgement message that you will now start research based on the provided information>\"\n",
    "\n",
    "For the verification message when no clarification is needed:\n",
    "- Acknowledge that you have sufficient information to proceed\n",
    "- Briefly summarize the key aspects of what you understand from their request\n",
    "- Confirm that you will now begin the research process\n",
    "- Keep the message concise and professional\n",
    "\"\"\"\n",
    "\n",
    "transform_messages_into_research_topic_prompt = \"\"\"You will be given a set of messages that have been exchanged so far between yourself and the user. \n",
    "Your job is to translate these messages into a more detailed and concrete research question that will be used to guide the research.\n",
    "\n",
    "The messages that have been exchanged so far between yourself and the user are:\n",
    "<Messages>\n",
    "{messages}\n",
    "</Messages>\n",
    "\n",
    "Today's date is {date}.\n",
    "\n",
    "You will return a single research question that will be used to guide the research.\n",
    "\n",
    "Guidelines:\n",
    "1. Maximize Specificity and Detail\n",
    "- Include all known user preferences and explicitly list key attributes or dimensions to consider.\n",
    "- It is important that all details from the user are included in the instructions.\n",
    "\n",
    "2. Handle Unstated Dimensions Carefully\n",
    "- When research quality requires considering additional dimensions that the user hasn't specified, acknowledge them as open considerations rather than assumed preferences.\n",
    "- Example: Instead of assuming \"budget-friendly options,\" say \"consider all price ranges unless cost constraints are specified.\"\n",
    "- Only mention dimensions that are genuinely necessary for comprehensive research in that domain.\n",
    "\n",
    "3. Avoid Unwarranted Assumptions\n",
    "- Never invent specific user preferences, constraints, or requirements that weren't stated.\n",
    "- If the user hasn't provided a particular detail, explicitly note this lack of specification.\n",
    "- Guide the researcher to treat unspecified aspects as flexible rather than making assumptions.\n",
    "\n",
    "4. Distinguish Between Research Scope and User Preferences\n",
    "- Research scope: What topics/dimensions should be investigated (can be broader than user's explicit mentions)\n",
    "- User preferences: Specific constraints, requirements, or preferences (must only include what user stated)\n",
    "- Example: \"Research coffee quality factors (including bean sourcing, roasting methods, brewing techniques) for San Francisco coffee shops, with primary focus on taste as specified by the user.\"\n",
    "\n",
    "5. Use the First Person\n",
    "- Phrase the request from the perspective of the user.\n",
    "\n",
    "6. Sources\n",
    "- If specific sources should be prioritized, specify them in the research question.\n",
    "- For product and travel research, prefer linking directly to official or primary websites (e.g., official brand sites, manufacturer pages, or reputable e-commerce platforms like Amazon for user reviews) rather than aggregator sites or SEO-heavy blogs.\n",
    "- For academic or scientific queries, prefer linking directly to the original paper or official journal publication rather than survey papers or secondary summaries.\n",
    "- For people, try linking directly to their LinkedIn profile, or their personal website if they have one.\n",
    "- If the query is in a specific language, prioritize sources published in that language.\n",
    "\"\"\"\n",
    "\n",
    "research_agent_prompt =  \"\"\"You are a research assistant conducting research on the user's input topic. For context, today's date is {date}.\n",
    "\n",
    "<Task>\n",
    "Your job is to use tools to gather information about the user's input topic.\n",
    "You can use any of the tools provided to you to find resources that can help answer the research question. You can call these tools in series or in parallel, your research is conducted in a tool-calling loop.\n",
    "</Task>\n",
    "\n",
    "<Available Tools>\n",
    "You have access to two main tools:\n",
    "1. **tavily_search**: For conducting web searches to gather information\n",
    "2. **think_tool**: For reflection and strategic planning during research\n",
    "\n",
    "**CRITICAL: Use think_tool after each search to reflect on results and plan next steps**\n",
    "</Available Tools>\n",
    "\n",
    "<Instructions>\n",
    "Think like a human researcher with limited time. Follow these steps:\n",
    "\n",
    "1. **Read the question carefully** - What specific information does the user need?\n",
    "2. **Start with broader searches** - Use broad, comprehensive queries first\n",
    "3. **After each search, pause and assess** - Do I have enough to answer? What's still missing?\n",
    "4. **Execute narrower searches as you gather information** - Fill in the gaps\n",
    "5. **Stop when you can answer confidently** - Don't keep searching for perfection\n",
    "</Instructions>\n",
    "\n",
    "<Hard Limits>\n",
    "**Tool Call Budgets** (Prevent excessive searching):\n",
    "- **Simple queries**: Use 2-3 search tool calls maximum\n",
    "- **Complex queries**: Use up to 5 search tool calls maximum\n",
    "- **Always stop**: After 5 search tool calls if you cannot find the right sources\n",
    "\n",
    "**Stop Immediately When**:\n",
    "- You can answer the user's question comprehensively\n",
    "- You have 3+ relevant examples/sources for the question\n",
    "- Your last 2 searches returned similar information\n",
    "</Hard Limits>\n",
    "\n",
    "<Show Your Thinking>\n",
    "After each search tool call, use think_tool to analyze the results:\n",
    "- What key information did I find?\n",
    "- What's missing?\n",
    "- Do I have enough to answer the question comprehensively?\n",
    "- Should I search more or provide my answer?\n",
    "</Show Your Thinking>\n",
    "\"\"\"\n",
    "\n",
    "summarize_webpage_prompt = \"\"\"You are tasked with summarizing the raw content of a webpage retrieved from a web search. Your goal is to create a summary that preserves the most important information from the original web page. This summary will be used by a downstream research agent, so it's crucial to maintain the key details without losing essential information.\n",
    "\n",
    "Here is the raw content of the webpage:\n",
    "\n",
    "<webpage_content>\n",
    "{webpage_content}\n",
    "</webpage_content>\n",
    "\n",
    "Please follow these guidelines to create your summary:\n",
    "\n",
    "1. Identify and preserve the main topic or purpose of the webpage.\n",
    "2. Retain key facts, statistics, and data points that are central to the content's message.\n",
    "3. Keep important quotes from credible sources or experts.\n",
    "4. Maintain the chronological order of events if the content is time-sensitive or historical.\n",
    "5. Preserve any lists or step-by-step instructions if present.\n",
    "6. Include relevant dates, names, and locations that are crucial to understanding the content.\n",
    "7. Summarize lengthy explanations while keeping the core message intact.\n",
    "\n",
    "When handling different types of content:\n",
    "\n",
    "- For news articles: Focus on the who, what, when, where, why, and how.\n",
    "- For scientific content: Preserve methodology, results, and conclusions.\n",
    "- For opinion pieces: Maintain the main arguments and supporting points.\n",
    "- For product pages: Keep key features, specifications, and unique selling points.\n",
    "\n",
    "Your summary should be significantly shorter than the original content but comprehensive enough to stand alone as a source of information. Aim for about 25-30 percent of the original length, unless the content is already concise.\n",
    "\n",
    "Present your summary in the following format:\n",
    "\n",
    "```\n",
    "{{\n",
    "   \"summary\": \"Your summary here, structured with appropriate paragraphs or bullet points as needed\",\n",
    "   \"key_excerpts\": \"First important quote or excerpt, Second important quote or excerpt, Third important quote or excerpt, ...Add more excerpts as needed, up to a maximum of 5\"\n",
    "}}\n",
    "```\n",
    "\n",
    "Here are two examples of good summaries:\n",
    "\n",
    "Example 1 (for a news article):\n",
    "```json\n",
    "{{\n",
    "   \"summary\": \"On July 15, 2023, NASA successfully launched the Artemis II mission from Kennedy Space Center. This marks the first crewed mission to the Moon since Apollo 17 in 1972. The four-person crew, led by Commander Jane Smith, will orbit the Moon for 10 days before returning to Earth. This mission is a crucial step in NASA's plans to establish a permanent human presence on the Moon by 2030.\",\n",
    "   \"key_excerpts\": \"Artemis II represents a new era in space exploration, said NASA Administrator John Doe. The mission will test critical systems for future long-duration stays on the Moon, explained Lead Engineer Sarah Johnson. We're not just going back to the Moon, we're going forward to the Moon, Commander Jane Smith stated during the pre-launch press conference.\"\n",
    "}}\n",
    "```\n",
    "\n",
    "Example 2 (for a scientific article):\n",
    "```json\n",
    "{{\n",
    "   \"summary\": \"A new study published in Nature Climate Change reveals that global sea levels are rising faster than previously thought. Researchers analyzed satellite data from 1993 to 2022 and found that the rate of sea-level rise has accelerated by 0.08 mm/year² over the past three decades. This acceleration is primarily attributed to melting ice sheets in Greenland and Antarctica. The study projects that if current trends continue, global sea levels could rise by up to 2 meters by 2100, posing significant risks to coastal communities worldwide.\",\n",
    "   \"key_excerpts\": \"Our findings indicate a clear acceleration in sea-level rise, which has significant implications for coastal planning and adaptation strategies, lead author Dr. Emily Brown stated. The rate of ice sheet melt in Greenland and Antarctica has tripled since the 1990s, the study reports. Without immediate and substantial reductions in greenhouse gas emissions, we are looking at potentially catastrophic sea-level rise by the end of this century, warned co-author Professor Michael Green.\"  \n",
    "}}\n",
    "```\n",
    "\n",
    "Remember, your goal is to create a summary that can be easily understood and utilized by a downstream research agent while preserving the most critical information from the original webpage.\n",
    "\n",
    "Today's date is {date}.\n",
    "\"\"\"\n",
    "\n",
    "# Research agent prompt for MCP (Model Context Protocol) file access\n",
    "research_agent_prompt_with_mcp = \"\"\"You are a research assistant conducting research on the user's input topic using local files. For context, today's date is {date}.\n",
    "\n",
    "<Task>\n",
    "Your job is to use file system tools to gather information from local research files.\n",
    "You can use any of the tools provided to you to find and read files that help answer the research question. You can call these tools in series or in parallel, your research is conducted in a tool-calling loop.\n",
    "</Task>\n",
    "\n",
    "<Available Tools>\n",
    "You have access to file system tools and thinking tools:\n",
    "- **list_allowed_directories**: See what directories you can access\n",
    "- **list_directory**: List files in directories\n",
    "- **read_file**: Read individual files\n",
    "- **read_multiple_files**: Read multiple files at once\n",
    "- **search_files**: Find files containing specific content\n",
    "- **think_tool**: For reflection and strategic planning during research\n",
    "\n",
    "**CRITICAL: Use think_tool after reading files to reflect on findings and plan next steps**\n",
    "</Available Tools>\n",
    "\n",
    "<Instructions>\n",
    "Think like a human researcher with access to a document library. Follow these steps:\n",
    "\n",
    "1. **Read the question carefully** - What specific information does the user need?\n",
    "2. **Explore available files** - Use list_allowed_directories and list_directory to understand what's available\n",
    "3. **Identify relevant files** - Use search_files if needed to find documents matching the topic\n",
    "4. **Read strategically** - Start with most relevant files, use read_multiple_files for efficiency\n",
    "5. **After reading, pause and assess** - Do I have enough to answer? What's still missing?\n",
    "6. **Stop when you can answer confidently** - Don't keep reading for perfection\n",
    "</Instructions>\n",
    "\n",
    "<Hard Limits>\n",
    "**File Operation Budgets** (Prevent excessive file reading):\n",
    "- **Simple queries**: Use 3-4 file operations maximum\n",
    "- **Complex queries**: Use up to 6 file operations maximum\n",
    "- **Always stop**: After 6 file operations if you cannot find the right information\n",
    "\n",
    "**Stop Immediately When**:\n",
    "- You can answer the user's question comprehensively from the files\n",
    "- You have comprehensive information from 3+ relevant files\n",
    "- Your last 2 file reads contained similar information\n",
    "</Hard Limits>\n",
    "\n",
    "<Show Your Thinking>\n",
    "After reading files, use think_tool to analyze what you found:\n",
    "- What key information did I find?\n",
    "- What's missing?\n",
    "- Do I have enough to answer the question comprehensively?\n",
    "- Should I read more files or provide my answer?\n",
    "- Always cite which files you used for your information\n",
    "</Show Your Thinking>\"\"\"\n",
    "\n",
    "lead_researcher_prompt = \"\"\"You are a research supervisor. Your job is to conduct research by calling the \"ConductResearch\" tool. For context, today's date is {date}.\n",
    "\n",
    "<Task>\n",
    "Your focus is to call the \"ConductResearch\" tool to conduct research against the overall research question passed in by the user. \n",
    "When you are completely satisfied with the research findings returned from the tool calls, then you should call the \"ResearchComplete\" tool to indicate that you are done with your research.\n",
    "</Task>\n",
    "\n",
    "<Available Tools>\n",
    "You have access to three main tools:\n",
    "1. **ConductResearch**: Delegate research tasks to specialized sub-agents\n",
    "2. **ResearchComplete**: Indicate that research is complete\n",
    "3. **think_tool**: For reflection and strategic planning during research\n",
    "\n",
    "**CRITICAL: Use think_tool before calling ConductResearch to plan your approach, and after each ConductResearch to assess progress**\n",
    "**PARALLEL RESEARCH**: When you identify multiple independent sub-topics that can be explored simultaneously, make multiple ConductResearch tool calls in a single response to enable parallel research execution. This is more efficient than sequential research for comparative or multi-faceted questions. Use at most {max_concurrent_research_units} parallel agents per iteration.\n",
    "</Available Tools>\n",
    "\n",
    "<Instructions>\n",
    "Think like a research manager with limited time and resources. Follow these steps:\n",
    "\n",
    "1. **Read the question carefully** - What specific information does the user need?\n",
    "2. **Decide how to delegate the research** - Carefully consider the question and decide how to delegate the research. Are there multiple independent directions that can be explored simultaneously?\n",
    "3. **After each call to ConductResearch, pause and assess** - Do I have enough to answer? What's still missing?\n",
    "</Instructions>\n",
    "\n",
    "<Hard Limits>\n",
    "**Task Delegation Budgets** (Prevent excessive delegation):\n",
    "- **Bias towards single agent** - Use single agent for simplicity unless the user request has clear opportunity for parallelization\n",
    "- **Stop when you can answer confidently** - Don't keep delegating research for perfection\n",
    "- **Limit tool calls** - Always stop after {max_researcher_iterations} tool calls to think_tool and ConductResearch if you cannot find the right sources\n",
    "</Hard Limits>\n",
    "\n",
    "<Show Your Thinking>\n",
    "Before you call ConductResearch tool call, use think_tool to plan your approach:\n",
    "- Can the task be broken down into smaller sub-tasks?\n",
    "\n",
    "After each ConductResearch tool call, use think_tool to analyze the results:\n",
    "- What key information did I find?\n",
    "- What's missing?\n",
    "- Do I have enough to answer the question comprehensively?\n",
    "- Should I delegate more research or call ResearchComplete?\n",
    "</Show Your Thinking>\n",
    "\n",
    "<Scaling Rules>\n",
    "**Simple fact-finding, lists, and rankings** can use a single sub-agent:\n",
    "- *Example*: List the top 10 coffee shops in San Francisco → Use 1 sub-agent\n",
    "\n",
    "**Comparisons presented in the user request** can use a sub-agent for each element of the comparison:\n",
    "- *Example*: Compare OpenAI vs. Anthropic vs. DeepMind approaches to AI safety → Use 3 sub-agents\n",
    "- Delegate clear, distinct, non-overlapping subtopics\n",
    "\n",
    "**Important Reminders:**\n",
    "- Each ConductResearch call spawns a dedicated research agent for that specific topic\n",
    "- A separate agent will write the final report - you just need to gather information\n",
    "- When calling ConductResearch, provide complete standalone instructions - sub-agents can't see other agents' work\n",
    "- Do NOT use acronyms or abbreviations in your research questions, be very clear and specific\n",
    "</Scaling Rules>\"\"\"\n",
    "\n",
    "compress_research_system_prompt = \"\"\"You are a research assistant that has conducted research on a topic by calling several tools and web searches. Your job is now to clean up the findings, but preserve all of the relevant statements and information that the researcher has gathered. For context, today's date is {date}.\n",
    "\n",
    "<Task>\n",
    "You need to clean up information gathered from tool calls and web searches in the existing messages.\n",
    "All relevant information should be repeated and rewritten verbatim, but in a cleaner format.\n",
    "The purpose of this step is just to remove any obviously irrelevant or duplicate information.\n",
    "For example, if three sources all say \"X\", you could say \"These three sources all stated X\".\n",
    "Only these fully comprehensive cleaned findings are going to be returned to the user, so it's crucial that you don't lose any information from the raw messages.\n",
    "</Task>\n",
    "\n",
    "<Tool Call Filtering>\n",
    "**IMPORTANT**: When processing the research messages, focus only on substantive research content:\n",
    "- **Include**: All tavily_search results and findings from web searches\n",
    "- **Exclude**: think_tool calls and responses - these are internal agent reflections for decision-making and should not be included in the final research report\n",
    "- **Focus on**: Actual information gathered from external sources, not the agent's internal reasoning process\n",
    "\n",
    "The think_tool calls contain strategic reflections and decision-making notes that are internal to the research process but do not contain factual information that should be preserved in the final report.\n",
    "</Tool Call Filtering>\n",
    "\n",
    "<Guidelines>\n",
    "1. Your output findings should be fully comprehensive and include ALL of the information and sources that the researcher has gathered from tool calls and web searches. It is expected that you repeat key information verbatim.\n",
    "2. This report can be as long as necessary to return ALL of the information that the researcher has gathered.\n",
    "3. In your report, you should return inline citations for each source that the researcher found.\n",
    "4. You should include a \"Sources\" section at the end of the report that lists all of the sources the researcher found with corresponding citations, cited against statements in the report.\n",
    "5. Make sure to include ALL of the sources that the researcher gathered in the report, and how they were used to answer the question!\n",
    "6. It's really important not to lose any sources. A later LLM will be used to merge this report with others, so having all of the sources is critical.\n",
    "</Guidelines>\n",
    "\n",
    "<Output Format>\n",
    "The report should be structured like this:\n",
    "**List of Queries and Tool Calls Made**\n",
    "**Fully Comprehensive Findings**\n",
    "**List of All Relevant Sources (with citations in the report)**\n",
    "</Output Format>\n",
    "\n",
    "<Citation Rules>\n",
    "- Assign each unique URL a single citation number in your text\n",
    "- End with ### Sources that lists each source with corresponding numbers\n",
    "- IMPORTANT: Number sources sequentially without gaps (1,2,3,4...) in the final list regardless of which sources you choose\n",
    "- Example format:\n",
    "  [1] Source Title: URL\n",
    "  [2] Source Title: URL\n",
    "</Citation Rules>\n",
    "\n",
    "Critical Reminder: It is extremely important that any information that is even remotely relevant to the user's research topic is preserved verbatim (e.g. don't rewrite it, don't summarize it, don't paraphrase it).\n",
    "\"\"\"\n",
    "\n",
    "compress_research_human_message = \"\"\"All above messages are about research conducted by an AI Researcher for the following research topic:\n",
    "\n",
    "RESEARCH TOPIC: {research_topic}\n",
    "\n",
    "Your task is to clean up these research findings while preserving ALL information that is relevant to answering this specific research question. \n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- DO NOT summarize or paraphrase the information - preserve it verbatim\n",
    "- DO NOT lose any details, facts, names, numbers, or specific findings\n",
    "- DO NOT filter out information that seems relevant to the research topic\n",
    "- Organize the information in a cleaner format but keep all the substance\n",
    "- Include ALL sources and citations found during research\n",
    "- Remember this research was conducted to answer the specific question above\n",
    "\n",
    "The cleaned findings will be used for final report generation, so comprehensiveness is critical.\"\"\"\n",
    "\n",
    "final_report_generation_prompt = \"\"\"Based on all the research conducted, create a comprehensive, well-structured answer to the overall research brief:\n",
    "<Research Brief>\n",
    "{research_brief}\n",
    "</Research Brief>\n",
    "\n",
    "CRITICAL: Make sure the answer is written in the same language as the human messages!\n",
    "For example, if the user's messages are in English, then MAKE SURE you write your response in English. If the user's messages are in Chinese, then MAKE SURE you write your entire response in Chinese.\n",
    "This is critical. The user will only understand the answer if it is written in the same language as their input message.\n",
    "\n",
    "Today's date is {date}.\n",
    "\n",
    "Here are the findings from the research that you conducted:\n",
    "<Findings>\n",
    "{findings}\n",
    "</Findings>\n",
    "\n",
    "Please create a detailed answer to the overall research brief that:\n",
    "1. Is well-organized with proper headings (# for title, ## for sections, ### for subsections)\n",
    "2. Includes specific facts and insights from the research\n",
    "3. References relevant sources using [Title](URL) format\n",
    "4. Provides a balanced, thorough analysis. Be as comprehensive as possible, and include all information that is relevant to the overall research question. People are using you for deep research and will expect detailed, comprehensive answers.\n",
    "5. Includes a \"Sources\" section at the end with all referenced links\n",
    "\n",
    "You can structure your report in a number of different ways. Here are some examples:\n",
    "\n",
    "To answer a question that asks you to compare two things, you might structure your report like this:\n",
    "1/ intro\n",
    "2/ overview of topic A\n",
    "3/ overview of topic B\n",
    "4/ comparison between A and B\n",
    "5/ conclusion\n",
    "\n",
    "To answer a question that asks you to return a list of things, you might only need a single section which is the entire list.\n",
    "1/ list of things or table of things\n",
    "Or, you could choose to make each item in the list a separate section in the report. When asked for lists, you don't need an introduction or conclusion.\n",
    "1/ item 1\n",
    "2/ item 2\n",
    "3/ item 3\n",
    "\n",
    "To answer a question that asks you to summarize a topic, give a report, or give an overview, you might structure your report like this:\n",
    "1/ overview of topic\n",
    "2/ concept 1\n",
    "3/ concept 2\n",
    "4/ concept 3\n",
    "5/ conclusion\n",
    "\n",
    "If you think you can answer the question with a single section, you can do that too!\n",
    "1/ answer\n",
    "\n",
    "REMEMBER: Section is a VERY fluid and loose concept. You can structure your report however you think is best, including in ways that are not listed above!\n",
    "Make sure that your sections are cohesive, and make sense for the reader.\n",
    "\n",
    "For each section of the report, do the following:\n",
    "- Use simple, clear language\n",
    "- Use ## for section title (Markdown format) for each section of the report\n",
    "- Do NOT ever refer to yourself as the writer of the report. This should be a professional report without any self-referential language. \n",
    "- Do not say what you are doing in the report. Just write the report without any commentary from yourself.\n",
    "- Each section should be as long as necessary to deeply answer the question with the information you have gathered. It is expected that sections will be fairly long and verbose. You are writing a deep research report, and users will expect a thorough answer.\n",
    "- Use bullet points to list out information when appropriate, but by default, write in paragraph form.\n",
    "\n",
    "REMEMBER:\n",
    "The brief and research may be in English, but you need to translate this information to the right language when writing the final answer.\n",
    "Make sure the final answer report is in the SAME language as the human messages in the message history.\n",
    "\n",
    "Format the report in clear markdown with proper structure and include source references where appropriate.\n",
    "\n",
    "<Citation Rules>\n",
    "- Assign each unique URL a single citation number in your text\n",
    "- End with ### Sources that lists each source with corresponding numbers\n",
    "- IMPORTANT: Number sources sequentially without gaps (1,2,3,4...) in the final list regardless of which sources you choose\n",
    "- Each source should be a separate line item in a list, so that in markdown it is rendered as a list.\n",
    "- Example format:\n",
    "  [1] Source Title: URL\n",
    "  [2] Source Title: URL\n",
    "- Citations are extremely important. Make sure to include these, and pay a lot of attention to getting these right. Users will often use these citations to look into more information.\n",
    "</Citation Rules>\n",
    "\"\"\"\n",
    "\n",
    "BRIEF_CRITERIA_PROMPT = \"\"\"\n",
    "<role>\n",
    "You are an expert research brief evaluator specializing in assessing whether generated research briefs accurately capture user-specified criteria without loss of important details.\n",
    "</role>\n",
    "\n",
    "<task>\n",
    "Determine if the research brief adequately captures the specific success criterion provided. Return a binary assessment with detailed reasoning.\n",
    "</task>\n",
    "\n",
    "<evaluation_context>\n",
    "Research briefs are critical for guiding downstream research agents. Missing or inadequately captured criteria can lead to incomplete research that fails to address user needs. Accurate evaluation ensures research quality and user satisfaction.\n",
    "</evaluation_context>\n",
    "\n",
    "<criterion_to_evaluate>\n",
    "{criterion}\n",
    "</criterion_to_evaluate>\n",
    "\n",
    "<research_brief>\n",
    "{research_brief}\n",
    "</research_brief>\n",
    "\n",
    "<evaluation_guidelines>\n",
    "CAPTURED (criterion is adequately represented) if:\n",
    "- The research brief explicitly mentions or directly addresses the criterion\n",
    "- The brief contains equivalent language or concepts that clearly cover the criterion\n",
    "- The criterion's intent is preserved even if worded differently\n",
    "- All key aspects of the criterion are represented in the brief\n",
    "\n",
    "NOT CAPTURED (criterion is missing or inadequately addressed) if:\n",
    "- The criterion is completely absent from the research brief\n",
    "- The brief only partially addresses the criterion, missing important aspects\n",
    "- The criterion is implied but not clearly stated or actionable for researchers\n",
    "- The brief contradicts or conflicts with the criterion\n",
    "\n",
    "<evaluation_examples>\n",
    "Example 1 - CAPTURED:\n",
    "Criterion: \"Current age is 25\"\n",
    "Brief: \"...investment advice for a 25-year-old investor...\"\n",
    "Judgment: CAPTURED - age is explicitly mentioned\n",
    "\n",
    "Example 2 - NOT CAPTURED:\n",
    "Criterion: \"Monthly rent below 7k\"\n",
    "Brief: \"...find apartments in Manhattan with good amenities...\"\n",
    "Judgment: NOT CAPTURED - budget constraint is completely missing\n",
    "\n",
    "Example 3 - CAPTURED:\n",
    "Criterion: \"High risk tolerance\"\n",
    "Brief: \"...willing to accept significant market volatility for higher returns...\"\n",
    "Judgment: CAPTURED - equivalent concept expressed differently\n",
    "\n",
    "Example 4 - NOT CAPTURED:\n",
    "Criterion: \"Doorman building required\"\n",
    "Brief: \"...find apartments with modern amenities...\"\n",
    "Judgment: NOT CAPTURED - specific doorman requirement not mentioned\n",
    "</evaluation_examples>\n",
    "</evaluation_guidelines>\n",
    "\n",
    "<output_instructions>\n",
    "1. Carefully examine the research brief for evidence of the specific criterion\n",
    "2. Look for both explicit mentions and equivalent concepts\n",
    "3. Provide specific quotes or references from the brief as evidence\n",
    "4. Be systematic - when in doubt about partial coverage, lean toward NOT CAPTURED for quality assurance\n",
    "5. Focus on whether a researcher could act on this criterion based on the brief alone\n",
    "</output_instructions>\"\"\"\n",
    "\n",
    "BRIEF_HALLUCINATION_PROMPT = \"\"\"\n",
    "## Brief Hallucination Evaluator\n",
    "\n",
    "<role>\n",
    "You are a meticulous research brief auditor specializing in identifying unwarranted assumptions that could mislead research efforts.\n",
    "</role>\n",
    "\n",
    "<task>  \n",
    "Determine if the research brief makes assumptions beyond what the user explicitly provided. Return a binary pass/fail judgment.\n",
    "</task>\n",
    "\n",
    "<evaluation_context>\n",
    "Research briefs should only include requirements, preferences, and constraints that users explicitly stated or clearly implied. Adding assumptions can lead to research that misses the user's actual needs.\n",
    "</evaluation_context>\n",
    "\n",
    "<research_brief>\n",
    "{research_brief}\n",
    "</research_brief>\n",
    "\n",
    "<success_criteria>\n",
    "{success_criteria}\n",
    "</success_criteria>\n",
    "\n",
    "<evaluation_guidelines>\n",
    "PASS (no unwarranted assumptions) if:\n",
    "- Brief only includes explicitly stated user requirements\n",
    "- Any inferences are clearly marked as such or logically necessary\n",
    "- Source suggestions are general recommendations, not specific assumptions\n",
    "- Brief stays within the scope of what the user actually requested\n",
    "\n",
    "FAIL (contains unwarranted assumptions) if:\n",
    "- Brief adds specific preferences user never mentioned\n",
    "- Brief assumes demographic, geographic, or contextual details not provided\n",
    "- Brief narrows scope beyond user's stated constraints\n",
    "- Brief introduces requirements user didn't specify\n",
    "\n",
    "<evaluation_examples>\n",
    "Example 1 - PASS:\n",
    "User criteria: [\"Looking for coffee shops\", \"In San Francisco\"] \n",
    "Brief: \"...research coffee shops in San Francisco area...\"\n",
    "Judgment: PASS - stays within stated scope\n",
    "\n",
    "Example 2 - FAIL:\n",
    "User criteria: [\"Looking for coffee shops\", \"In San Francisco\"]\n",
    "Brief: \"...research trendy coffee shops for young professionals in San Francisco...\"\n",
    "Judgment: FAIL - assumes \"trendy\" and \"young professionals\" demographics\n",
    "\n",
    "Example 3 - PASS:\n",
    "User criteria: [\"Budget under $3000\", \"2 bedroom apartment\"]\n",
    "Brief: \"...find 2-bedroom apartments within $3000 budget, consulting rental sites and local listings...\"\n",
    "Judgment: PASS - source suggestions are appropriate, no preference assumptions\n",
    "\n",
    "Example 4 - FAIL:\n",
    "User criteria: [\"Budget under $3000\", \"2 bedroom apartment\"] \n",
    "Brief: \"...find modern 2-bedroom apartments under $3000 in safe neighborhoods with good schools...\"\n",
    "Judgment: FAIL - assumes \"modern\", \"safe\", and \"good schools\" preferences\n",
    "</evaluation_examples>\n",
    "</evaluation_guidelines>\n",
    "\n",
    "<output_instructions>\n",
    "Carefully scan the brief for any details not explicitly provided by the user. Be strict - when in doubt about whether something was user-specified, lean toward FAIL.\n",
    "</output_instructions>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_today_str() -> str:\n",
    "    \"\"\"Get current date in human-readable format.\"\"\"\n",
    "    return datetime.now().strftime(\"%a %b %#d, %Y\")\n",
    "\n",
    "def get_current_dir() -> Path:\n",
    "    \"\"\"Get the current directory of the module.\n",
    "\n",
    "    This function is compatible with Jupyter notebooks and regular Python scripts.\n",
    "\n",
    "    Returns:\n",
    "        Path object representing the current directory\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return Path(__file__).resolve().parent\n",
    "    except NameError:  # __file__ is not defined\n",
    "        return Path.cwd()\n",
    "\n",
    "def tavily_search_multiple(\n",
    "    search_queries: List[str], \n",
    "    max_results: int = 3, \n",
    "    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\", \n",
    "    include_raw_content: bool = True, \n",
    ") -> List[dict]:\n",
    "    \"\"\"Perform search using Tavily API for multiple queries.\n",
    "\n",
    "    Args:\n",
    "        search_queries: List of search queries to execute\n",
    "        max_results: Maximum number of results per query\n",
    "        topic: Topic filter for search results\n",
    "        include_raw_content: Whether to include raw webpage content\n",
    "\n",
    "    Returns:\n",
    "        List of search result dictionaries\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute searches sequentially. Note: yon can use AsyncTavilyClient to parallelize this step.\n",
    "    search_docs = []\n",
    "    for query in search_queries:\n",
    "        result = tavily_client.search(\n",
    "            query,\n",
    "            max_results=max_results,\n",
    "            include_raw_content=include_raw_content,\n",
    "            topic=topic\n",
    "        )\n",
    "        search_docs.append(result)\n",
    "\n",
    "    return search_docs\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    \"\"\"Schema for webpage content summarization.\"\"\"\n",
    "    summary: str = Field(description=\"Concise summary of the webpage content\")\n",
    "    key_excerpts: str = Field(description=\"Important quotes and excerpts from the content\")\n",
    "\n",
    "def summarize_webpage_content(webpage_content: str) -> str:\n",
    "    \"\"\"Summarize webpage content using the configured summarization model.\n",
    "\n",
    "    Args:\n",
    "        webpage_content: Raw webpage content to summarize\n",
    "\n",
    "    Returns:\n",
    "        Formatted summary with key excerpts\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set up structured output model for summarization\n",
    "        structured_model = summarization_llm.with_structured_output(Summary)\n",
    "\n",
    "        # Generate summary\n",
    "        summary = structured_model.invoke([\n",
    "            HumanMessage(content=summarize_webpage_prompt.format(\n",
    "                webpage_content=webpage_content, \n",
    "                date=get_today_str()\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        # Format summary with clear structure\n",
    "        formatted_summary = (\n",
    "            f\"<summary>\\n{summary.summary}\\n</summary>\\n\\n\"\n",
    "            f\"<key_excerpts>\\n{summary.key_excerpts}\\n</key_excerpts>\"\n",
    "        )\n",
    "\n",
    "        return formatted_summary\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to summarize webpage: {str(e)}\")\n",
    "        return webpage_content[:1000] + \"...\" if len(webpage_content) > 1000 else webpage_content\n",
    "\n",
    "def deduplicate_search_results(search_results: List[dict]) -> dict:\n",
    "    \"\"\"Deduplicate search results by URL to avoid processing duplicate content.\n",
    "\n",
    "    Args:\n",
    "        search_results: List of search result dictionaries\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping URLs to unique results\n",
    "    \"\"\"\n",
    "    unique_results = {}\n",
    "    for response in search_results:\n",
    "        for result in response.get('results', []):\n",
    "            url = result['url']\n",
    "            if url not in unique_results:\n",
    "                unique_results[url] = result\n",
    "    return unique_results\n",
    "\n",
    "def process_search_results(unique_results: dict) -> dict:\n",
    "    \"\"\"Process search results by summarizing content where available.\n",
    "\n",
    "    Args:\n",
    "        unique_results: Dictionary of unique search results\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of processed results with summaries\n",
    "    \"\"\"\n",
    "    summarized_results = {}\n",
    "    for url, result in unique_results.items():\n",
    "        # Use existing content if no raw content for summarization\n",
    "        if not result.get(\"raw_content\"):\n",
    "            content = result['content']\n",
    "        else:\n",
    "            # Summarize raw content for better processing\n",
    "            content = summarize_webpage_content(result['raw_content'])\n",
    "        \n",
    "        summarized_results[url] = {\n",
    "            'title': result['title'],\n",
    "            'content': content\n",
    "        }\n",
    "    return summarized_results\n",
    "\n",
    "def format_search_output(summarized_results: dict) -> str:\n",
    "    \"\"\"Format search results into a well-structured string output.\n",
    "\n",
    "    Args:\n",
    "        summarized_results: Dictionary of processed search results\n",
    "\n",
    "    Returns:\n",
    "        Formatted string of search results with clear source separation\n",
    "    \"\"\"\n",
    "    if not summarized_results:\n",
    "        return \"No valid search results found. Please try different search queries or use a different search API.\"\n",
    "    \n",
    "    formatted_output = \"Search results: \\n\\n\"\n",
    "    \n",
    "    for i, (url, result) in enumerate(summarized_results.items(), 1):\n",
    "        formatted_output += f\"\\n\\n--- SOURCE {i}: {result['title']} ---\\n\"\n",
    "        formatted_output += f\"URL: {url}\\n\\n\"\n",
    "        formatted_output += f\"SUMMARY:\\n{result['content']}\\n\\n\"\n",
    "        formatted_output += \"-\" * 80 + \"\\n\"\n",
    "    \n",
    "    return formatted_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Research Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(parse_docstring=True)\n",
    "def tavily_search(\n",
    "    query: str,\n",
    "    max_results: Annotated[int, InjectedToolArg] = 3,\n",
    "    topic: Annotated[Literal[\"general\", \"news\", \"finance\"], InjectedToolArg] = \"general\",\n",
    ") -> str:\n",
    "    \"\"\"Fetch results from Tavily search API with content summarization.\n",
    "\n",
    "    Args:\n",
    "        query: A single search query to execute\n",
    "        max_results: Maximum number of results to return\n",
    "        topic: Topic to filter results by ('general', 'news', 'finance')\n",
    "\n",
    "    Returns:\n",
    "        Formatted string of search results with summaries\n",
    "    \"\"\"\n",
    "    # Execute search for single query\n",
    "    search_results = tavily_search_multiple(\n",
    "        [query],  # Convert single query to list for the internal function\n",
    "        max_results=max_results,\n",
    "        topic=topic,\n",
    "        include_raw_content=True,\n",
    "    )\n",
    "\n",
    "    # Deduplicate results by URL to avoid processing duplicate content\n",
    "    unique_results = deduplicate_search_results(search_results)\n",
    "\n",
    "    # Process results with summarization\n",
    "    summarized_results = process_search_results(unique_results)\n",
    "\n",
    "    # Format output for consumption\n",
    "    return format_search_output(summarized_results)\n",
    "\n",
    "@tool(parse_docstring=True)\n",
    "def think_tool(reflection: str) -> str:\n",
    "    \"\"\"Tool for strategic reflection on research progress and decision-making.\n",
    "\n",
    "    Use this tool after each search to analyze results and plan next steps systematically.\n",
    "    This creates a deliberate pause in the research workflow for quality decision-making.\n",
    "\n",
    "    When to use:\n",
    "    - After receiving search results: What key information did I find?\n",
    "    - Before deciding next steps: Do I have enough to answer comprehensively?\n",
    "    - When assessing research gaps: What specific information am I still missing?\n",
    "    - Before concluding research: Can I provide a complete answer now?\n",
    "\n",
    "    Reflection should address:\n",
    "    1. Analysis of current findings - What concrete information have I gathered?\n",
    "    2. Gap assessment - What crucial information is still missing?\n",
    "    3. Quality evaluation - Do I have sufficient evidence/examples for a good answer?\n",
    "    4. Strategic decision - Should I continue searching or provide my answer?\n",
    "\n",
    "    Args:\n",
    "        reflection: Your detailed reflection on research progress, findings, gaps, and next steps\n",
    "\n",
    "    Returns:\n",
    "        Confirmation that reflection was recorded for decision-making\n",
    "    \"\"\"\n",
    "    return f\"Reflection recorded: {reflection}\"\n",
    "\n",
    "@tool\n",
    "class ConductResearch(BaseModel):\n",
    "    \"\"\"Tool for delegating a research task to a specialized sub-agent.\"\"\"\n",
    "    research_topic: str = Field(\n",
    "        description=\"The topic to research. Should be a single topic, and should be described in high detail (at least a paragraph).\",\n",
    "    )\n",
    "\n",
    "@tool\n",
    "class ResearchComplete(BaseModel):\n",
    "    \"\"\"Tool for indicating that the research process is complete.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. State Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SCOPING STATE DEFINITIONS =====\n",
    "\n",
    "class AgentInputState(MessagesState):\n",
    "    \"\"\"Input state for the full agent - only contains messages from user input.\"\"\"\n",
    "    pass\n",
    "\n",
    "class AgentState(MessagesState):\n",
    "    \"\"\"\n",
    "    Main state for the full multi-agent research system.\n",
    "\n",
    "    Extends MessagesState with additional fields for research coordination.\n",
    "    Note: Some fields are duplicated across different state classes for proper\n",
    "    state management between subgraphs and the main workflow.\n",
    "    \"\"\"\n",
    "\n",
    "    # Research brief generated from user conversation history\n",
    "    research_brief: Optional[str]\n",
    "    # Messages exchanged with the supervisor agent for coordination\n",
    "    supervisor_messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    # Raw unprocessed research notes collected during the research phase\n",
    "    raw_notes: Annotated[list[str], operator.add] = []\n",
    "    # Processed and structured notes ready for report generation\n",
    "    notes: Annotated[list[str], operator.add] = []\n",
    "    # Final formatted research report\n",
    "    final_report: str\n",
    "\n",
    "# ===== RESEARCH AGENT STATE DEFINITIONS =====\n",
    "\n",
    "class ResearcherState(TypedDict):\n",
    "    \"\"\"\n",
    "    State for the research agent containing message history and research metadata.\n",
    "\n",
    "    This state tracks the researcher's conversation, iteration count for limiting\n",
    "    tool calls, the research topic being investigated, compressed findings,\n",
    "    and raw research notes for detailed analysis.\n",
    "    \"\"\"\n",
    "    researcher_messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    tool_call_iterations: int\n",
    "    research_topic: str\n",
    "    compressed_research: str\n",
    "    raw_notes: Annotated[List[str], operator.add]\n",
    "\n",
    "class ResearcherOutputState(TypedDict):\n",
    "    \"\"\"\n",
    "    Output state for the research agent containing final research results.\n",
    "\n",
    "    This represents the final output of the research process with compressed\n",
    "    research findings and all raw notes from the research process.\n",
    "    \"\"\"\n",
    "    compressed_research: str\n",
    "    raw_notes: Annotated[List[str], operator.add]\n",
    "    researcher_messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "# ===== SUPERVISOR STATE DEFINITIONS =====\n",
    "\n",
    "class SupervisorState(TypedDict):\n",
    "    \"\"\"\n",
    "    State for the multi-agent research supervisor.\n",
    "\n",
    "    Manages coordination between supervisor and research agents, tracking\n",
    "    research progress and accumulating findings from multiple sub-agents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Messages exchanged with supervisor for coordination and decision-making\n",
    "    supervisor_messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    # Detailed research brief that guides the overall research direction\n",
    "    research_brief: str\n",
    "    # Processed and structured notes ready for final report generation\n",
    "    notes: Annotated[list[str], operator.add] = []\n",
    "    # Counter tracking the number of research iterations performed\n",
    "    research_iterations: int = 0\n",
    "    # Raw unprocessed research notes collected from sub-agent research\n",
    "    raw_notes: Annotated[list[str], operator.add] = []\n",
    "\n",
    "# ===== STRUCTURED OUTPUT SCHEMAS =====\n",
    "\n",
    "class ClarifyWithUser(BaseModel):\n",
    "    \"\"\"Schema for user clarification decision and questions.\"\"\"\n",
    "\n",
    "    need_clarification: bool = Field(\n",
    "        description=\"Whether the user needs to be asked a clarifying question.\",\n",
    "    )\n",
    "    question: str = Field(\n",
    "        description=\"A question to ask the user to clarify the report scope\",\n",
    "    )\n",
    "    verification: str = Field(\n",
    "        description=\"Verify message that we will start research after the user has provided the necessary information.\",\n",
    "    )\n",
    "\n",
    "class ResearchQuestion(BaseModel):\n",
    "    \"\"\"Schema for structured research brief generation.\"\"\"\n",
    "\n",
    "    research_brief: str = Field(\n",
    "        description=\"A research question that will be used to guide the research.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Research Scoping Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clarify_with_user(state: AgentState) -> Command[Literal[\"write_research_brief\", \"__end__\"]]:\n",
    "    \"\"\"\n",
    "    Determine if the user's request contains sufficient information to proceed with research.\n",
    "\n",
    "    Uses structured output to make deterministic decisions and avoid hallucination.\n",
    "    Routes to either research brief generation or ends with a clarification question.\n",
    "    \"\"\"\n",
    "    # Set up structured output model\n",
    "    structured_output_model = llm.with_structured_output(ClarifyWithUser)\n",
    "\n",
    "    # Invoke the model with clarification instructions\n",
    "    response = structured_output_model.invoke([\n",
    "        HumanMessage(content=clarify_with_user_instructions.format(\n",
    "            messages=get_buffer_string(messages=state[\"messages\"]), \n",
    "            date=get_today_str()\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Route based on clarification need\n",
    "    if response.need_clarification:\n",
    "        return Command(\n",
    "            goto=END, \n",
    "            update={\"messages\": [AIMessage(content=response.question)]}\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            goto=\"write_research_brief\", \n",
    "            update={\"messages\": [AIMessage(content=response.verification)]}\n",
    "        )\n",
    "\n",
    "def write_research_brief(state: AgentState):\n",
    "    \"\"\"\n",
    "    Transform the conversation history into a comprehensive research brief.\n",
    "\n",
    "    Uses structured output to ensure the brief follows the required format\n",
    "    and contains all necessary details for effective research.\n",
    "    \"\"\"\n",
    "    # Set up structured output model\n",
    "    structured_output_model = llm.with_structured_output(ResearchQuestion)\n",
    "\n",
    "    # Generate research brief from conversation history\n",
    "    response = structured_output_model.invoke([\n",
    "        HumanMessage(content=transform_messages_into_research_topic_prompt.format(\n",
    "            messages=get_buffer_string(state.get(\"messages\", [])),\n",
    "            date=get_today_str()\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Update state with generated research brief and pass it to the supervisor\n",
    "    return {\n",
    "        \"research_brief\": response.research_brief,\n",
    "        \"supervisor_messages\": [HumanMessage(content=f\"{response.research_brief}.\")]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Research Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research agent compiled successfully\n"
     ]
    }
   ],
   "source": [
    "# ===== RESEARCH AGENT IMPLEMENTATION =====\n",
    "\n",
    "# Set up tools and model binding\n",
    "research_tools = [tavily_search, think_tool]\n",
    "research_model_with_tools = llm.bind_tools(research_tools)\n",
    "tools_by_name = {tool.name: tool for tool in research_tools}\n",
    "\n",
    "def llm_call(state: ResearcherState):\n",
    "    \"\"\"Analyze current state and decide on next actions.\n",
    "\n",
    "    The model analyzes the current conversation state and decides whether to:\n",
    "    1. Call search tools to gather more information\n",
    "    2. Provide a final answer based on gathered information\n",
    "\n",
    "    Returns updated state with the model's response.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"researcher_messages\": [\n",
    "            research_model_with_tools.invoke(\n",
    "                [SystemMessage(content=research_agent_prompt.format(date=get_today_str()))] + state[\"researcher_messages\"]\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def tool_node(state: ResearcherState):\n",
    "    \"\"\"Execute all tool calls from the previous LLM response.\n",
    "\n",
    "    Executes all tool calls from the previous LLM responses.\n",
    "    Returns updated state with tool execution results.\n",
    "    \"\"\"\n",
    "    tool_calls = state[\"researcher_messages\"][-1].tool_calls\n",
    "\n",
    "    # Execute all tool calls\n",
    "    observations = []\n",
    "    for tool_call in tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observations.append(tool.invoke(tool_call[\"args\"]))\n",
    "\n",
    "    # Create tool message outputs\n",
    "    tool_outputs = [\n",
    "        ToolMessage(\n",
    "            content=observation,\n",
    "            name=tool_call[\"name\"],\n",
    "            tool_call_id=tool_call[\"id\"]\n",
    "        ) for observation, tool_call in zip(observations, tool_calls)\n",
    "    ]\n",
    "\n",
    "    return {\"researcher_messages\": tool_outputs}\n",
    "\n",
    "def compress_research(state: ResearcherState) -> dict:\n",
    "    \"\"\"Compress research findings into a concise summary.\n",
    "\n",
    "    Takes all the research messages and tool outputs and creates\n",
    "    a compressed summary suitable for the supervisor's decision-making.\n",
    "    \"\"\"\n",
    "\n",
    "    system_message = compress_research_system_prompt.format(date=get_today_str())\n",
    "    messages = [SystemMessage(content=system_message)] + state.get(\"researcher_messages\", []) + [HumanMessage(content=compress_research_human_message.format(research_topic=state.get(\"research_topic\", \"\")))]\n",
    "    response = compress_llm.invoke(messages)\n",
    "\n",
    "    # Extract raw notes from tool and AI messages\n",
    "    raw_notes = [\n",
    "        str(m.content) for m in filter_messages(\n",
    "            state[\"researcher_messages\"], \n",
    "            include_types=[\"tool\", \"ai\"]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"compressed_research\": str(response.content),\n",
    "        \"raw_notes\": [\"\\n\".join(raw_notes)]\n",
    "    }\n",
    "\n",
    "def should_continue(state: ResearcherState) -> Literal[\"tool_node\", \"compress_research\"]:\n",
    "    \"\"\"Determine whether to continue research or provide final answer.\n",
    "\n",
    "    Determines whether the agent should continue the research loop or provide\n",
    "    a final answer based on whether the LLM made tool calls.\n",
    "\n",
    "    Returns:\n",
    "        \"tool_node\": Continue to tool execution\n",
    "        \"compress_research\": Stop and compress research\n",
    "    \"\"\"\n",
    "    messages = state[\"researcher_messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # If the LLM makes a tool call, continue to tool execution\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_node\"\n",
    "    # Otherwise, we have a final answer\n",
    "    return \"compress_research\"\n",
    "\n",
    "# ===== GRAPH CONSTRUCTION =====\n",
    "\n",
    "# Build the agent workflow\n",
    "research_agent_builder = StateGraph(ResearcherState, output_schema=ResearcherOutputState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "research_agent_builder.add_node(\"llm_call\", llm_call)\n",
    "research_agent_builder.add_node(\"tool_node\", tool_node)\n",
    "research_agent_builder.add_node(\"compress_research\", compress_research)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "research_agent_builder.add_edge(START, \"llm_call\")\n",
    "research_agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tool_node\": \"tool_node\", # Continue research loop\n",
    "        \"compress_research\": \"compress_research\", # Provide final answer\n",
    "    },\n",
    ")\n",
    "research_agent_builder.add_edge(\"tool_node\", \"llm_call\") # Loop back for more research\n",
    "research_agent_builder.add_edge(\"compress_research\", END)\n",
    "\n",
    "# Compile the agent\n",
    "researcher_agent = research_agent_builder.compile()\n",
    "print(\"Research agent compiled successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-Agent Supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervisor agent compiled successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\IPython\\core\\inputtransformer2.py:538: RuntimeWarning: coroutine 'Pregel.ainvoke' was never awaited\n",
      "  tokens_by_line.append([])\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# ===== MULTI-AGENT SUPERVISOR =====\n",
    "\n",
    "def get_notes_from_tool_calls(messages: list[BaseMessage]) -> list[str]:\n",
    "    \"\"\"Extract research notes from ToolMessage objects in supervisor message history.\n",
    "\n",
    "    This function retrieves the compressed research findings that sub-agents\n",
    "    return as ToolMessage content. When the supervisor delegates research to\n",
    "    sub-agents via ConductResearch tool calls, each sub-agent returns its\n",
    "    compressed findings as the content of a ToolMessage. This function\n",
    "    extracts all such ToolMessage content to compile the final research notes.\n",
    "\n",
    "    Args:\n",
    "        messages: List of messages from supervisor's conversation history\n",
    "\n",
    "    Returns:\n",
    "        List of research note strings extracted from ToolMessage objects\n",
    "    \"\"\"\n",
    "    return [tool_msg.content for tool_msg in filter_messages(messages, include_types=\"tool\")]\n",
    "\n",
    "# Supervisor tools and model configuration\n",
    "supervisor_tools = [ConductResearch, ResearchComplete, think_tool]\n",
    "supervisor_model_with_tools = llm.bind_tools(supervisor_tools)\n",
    "\n",
    "async def supervisor(state: SupervisorState) -> Command[Literal[\"supervisor_tools\"]]:\n",
    "    \"\"\"Coordinate research activities.\n",
    "\n",
    "    Analyzes the research brief and current progress to decide:\n",
    "    - What research topics need investigation\n",
    "    - Whether to conduct parallel research\n",
    "    - When research is complete\n",
    "\n",
    "    Args:\n",
    "        state: Current supervisor state with messages and research progress\n",
    "\n",
    "    Returns:\n",
    "        Command to proceed to supervisor_tools node with updated state\n",
    "    \"\"\"\n",
    "    supervisor_messages = state.get(\"supervisor_messages\", [])\n",
    "\n",
    "    # Prepare system message with current date and constraints\n",
    "    system_message = lead_researcher_prompt.format(\n",
    "        date=get_today_str(), \n",
    "        max_concurrent_research_units=max_concurrent_researchers,\n",
    "        max_researcher_iterations=max_researcher_iterations\n",
    "    )\n",
    "    messages = [SystemMessage(content=system_message)] + supervisor_messages\n",
    "\n",
    "    # Make decision about next research steps\n",
    "    response = await supervisor_model_with_tools.ainvoke(messages)\n",
    "\n",
    "    return Command(\n",
    "        goto=\"supervisor_tools\",\n",
    "        update={\n",
    "            \"supervisor_messages\": [response],\n",
    "            \"research_iterations\": state.get(\"research_iterations\", 0) + 1\n",
    "        }\n",
    "    )\n",
    "\n",
    "async def supervisor_tools(state: SupervisorState) -> Command[Literal[\"supervisor\", \"__end__\"]]:\n",
    "    \"\"\"Execute supervisor decisions - either conduct research or end the process.\n",
    "\n",
    "    Handles:\n",
    "    - Executing think_tool calls for strategic reflection\n",
    "    - Launching parallel research agents for different topics\n",
    "    - Aggregating research results\n",
    "    - Determining when research is complete\n",
    "\n",
    "    Args:\n",
    "        state: Current supervisor state with messages and iteration count\n",
    "\n",
    "    Returns:\n",
    "        Command to continue supervision, end process, or handle errors\n",
    "    \"\"\"\n",
    "    supervisor_messages = state.get(\"supervisor_messages\", [])\n",
    "    research_iterations = state.get(\"research_iterations\", 0)\n",
    "    most_recent_message = supervisor_messages[-1]\n",
    "\n",
    "    # Initialize variables for single return pattern\n",
    "    tool_messages = []\n",
    "    all_raw_notes = []\n",
    "    next_step = \"supervisor\"  # Default next step\n",
    "    should_end = False\n",
    "\n",
    "    # Check exit criteria first\n",
    "    exceeded_iterations = research_iterations >= max_researcher_iterations\n",
    "    no_tool_calls = not most_recent_message.tool_calls\n",
    "    research_complete = any(\n",
    "        tool_call[\"name\"] == \"ResearchComplete\" \n",
    "        for tool_call in most_recent_message.tool_calls\n",
    "    )\n",
    "\n",
    "    if exceeded_iterations or no_tool_calls or research_complete:\n",
    "        should_end = True\n",
    "        next_step = END\n",
    "\n",
    "    else:\n",
    "        # Execute ALL tool calls before deciding next step\n",
    "        try:\n",
    "            # Separate think_tool calls from ConductResearch calls\n",
    "            think_tool_calls = [\n",
    "                tool_call for tool_call in most_recent_message.tool_calls \n",
    "                if tool_call[\"name\"] == \"think_tool\"\n",
    "            ]\n",
    "\n",
    "            conduct_research_calls = [\n",
    "                tool_call for tool_call in most_recent_message.tool_calls \n",
    "                if tool_call[\"name\"] == \"ConductResearch\"\n",
    "            ]\n",
    "\n",
    "            # Handle think_tool calls (synchronous)\n",
    "            for tool_call in think_tool_calls:\n",
    "                observation = think_tool.invoke(tool_call[\"args\"])\n",
    "                tool_messages.append(\n",
    "                    ToolMessage(\n",
    "                        content=observation,\n",
    "                        name=tool_call[\"name\"],\n",
    "                        tool_call_id=tool_call[\"id\"]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # Handle ConductResearch calls (asynchronous)\n",
    "            if conduct_research_calls:\n",
    "                # Launch parallel research agents\n",
    "                coros = [\n",
    "                    researcher_agent.ainvoke({\n",
    "                        \"researcher_messages\": [\n",
    "                            HumanMessage(content=tool_call[\"args\"][\"research_topic\"])\n",
    "                        ],\n",
    "                        \"research_topic\": tool_call[\"args\"][\"research_topic\"]\n",
    "                    }) \n",
    "                    for tool_call in conduct_research_calls\n",
    "                ]\n",
    "\n",
    "                # Wait for all research to complete\n",
    "                tool_results = await asyncio.gather(*coros)\n",
    "\n",
    "                # Format research results as tool messages\n",
    "                # Each sub-agent returns compressed research findings in result[\"compressed_research\"]\n",
    "                # We write this compressed research as the content of a ToolMessage, which allows\n",
    "                # the supervisor to later retrieve these findings via get_notes_from_tool_calls()\n",
    "                research_tool_messages = [\n",
    "                    ToolMessage(\n",
    "                        content=result.get(\"compressed_research\", \"Error synthesizing research report\"),\n",
    "                        name=tool_call[\"name\"],\n",
    "                        tool_call_id=tool_call[\"id\"]\n",
    "                    ) for result, tool_call in zip(tool_results, conduct_research_calls)\n",
    "                ]\n",
    "\n",
    "                tool_messages.extend(research_tool_messages)\n",
    "\n",
    "                # Aggregate raw notes from all research\n",
    "                all_raw_notes = [\n",
    "                    \"\\n\".join(result.get(\"raw_notes\", [])) \n",
    "                    for result in tool_results\n",
    "                ]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in supervisor tools: {e}\")\n",
    "            should_end = True\n",
    "            next_step = END\n",
    "\n",
    "    # Single return point with appropriate state updates\n",
    "    if should_end:\n",
    "        return Command(\n",
    "            goto=next_step,\n",
    "            update={\n",
    "                \"notes\": get_notes_from_tool_calls(supervisor_messages),\n",
    "                \"research_brief\": state.get(\"research_brief\", \"\")\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            goto=next_step,\n",
    "            update={\n",
    "                \"supervisor_messages\": tool_messages,\n",
    "                \"raw_notes\": all_raw_notes\n",
    "            }\n",
    "        )\n",
    "\n",
    "# ===== GRAPH CONSTRUCTION =====\n",
    "\n",
    "# Build supervisor graph\n",
    "supervisor_builder = StateGraph(SupervisorState)\n",
    "supervisor_builder.add_node(\"supervisor\", supervisor)\n",
    "supervisor_builder.add_node(\"supervisor_tools\", supervisor_tools)\n",
    "supervisor_builder.add_edge(START, \"supervisor\")\n",
    "supervisor_agent = supervisor_builder.compile()\n",
    "print(\"Supervisor agent compiled successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def final_report_generation(state: AgentState):\n",
    "    \"\"\"\n",
    "    Final report generation node.\n",
    "\n",
    "    Synthesizes all research findings into a comprehensive final report\n",
    "    \"\"\"\n",
    "    notes = state.get(\"notes\", [])\n",
    "    findings = \"\\n\\n\".join(notes)\n",
    "    \n",
    "    prompt = final_report_generation_prompt.format(\n",
    "        research_brief=state.get(\"research_brief\", \"\"),\n",
    "        findings=findings,\n",
    "        date=get_today_str()\n",
    "    )\n",
    "    \n",
    "    final_report = await writer_llm.ainvoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    return {\n",
    "        \"final_report\": final_report.content,\n",
    "        \"messages\": [\"Here is the final report: \" + final_report.content],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Complete System Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete research system compiled successfully\n"
     ]
    }
   ],
   "source": [
    "# Build complete research system\n",
    "full_system_builder = StateGraph(AgentState, input_schema=AgentInputState)\n",
    "\n",
    "# Add all nodes\n",
    "full_system_builder.add_node(\"clarify_with_user\", clarify_with_user)\n",
    "full_system_builder.add_node(\"write_research_brief\", write_research_brief)\n",
    "full_system_builder.add_node(\"supervisor_subgraph\", supervisor_agent)\n",
    "full_system_builder.add_node(\"final_report_generation\", final_report_generation)\n",
    "\n",
    "# Add edges\n",
    "full_system_builder.add_edge(START, \"clarify_with_user\")\n",
    "full_system_builder.add_edge(\"write_research_brief\", \"supervisor_subgraph\")\n",
    "full_system_builder.add_edge(\"supervisor_subgraph\", \"final_report_generation\")\n",
    "full_system_builder.add_edge(\"final_report_generation\", END)\n",
    "\n",
    "# Compile with high recursion limit\n",
    "research_graph = full_system_builder.compile()\n",
    "\n",
    "# Visualize\n",
    "display(Image(research_graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "print(\"Complete research system compiled successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Agent Wrapper for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_b_agent(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Wrapper function for multi-agent supervisor research system.\n",
    "    \n",
    "    Compatible with evaluation harness.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Dictionary with 'question' key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with 'output' key containing final report\n",
    "    \"\"\"\n",
    "    question = inputs.get(\"question\", \"\")\n",
    "    \n",
    "    # Run with high recursion limit\n",
    "    result = asyncio.run(\n",
    "        research_graph.ainvoke(\n",
    "            {\"messages\": [HumanMessage(content=question)]},\n",
    "            config={\"recursion_limit\": 50}\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"output\": result.get(\"final_report\", \"\"),\n",
    "        \"research_brief\": result.get(\"research_brief\", \"\"),\n",
    "        \"notes\": result.get(\"notes\", [])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Manual Test (Before Harness Evaluation)\n",
    "\n",
    "Run this cell to verify the agent works correctly with a simple test question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with question: Compare the key features and use cases of LangGraph versus LangChain for building AI agents.\n",
      "\n",
      "Running multi-agent research system (this may take several minutes)...\n",
      "\n",
      "NOTE: System will use supervisor to coordinate multiple research agents in parallel.\n",
      "\n",
      "Error in supervisor tools: Connection error.\n",
      "Agent test FAILED: Connection error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 394, in handle_async_request\n",
      "    resp = await self._pool.handle_async_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 256, in handle_async_request\n",
      "    raise exc from None\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 236, in handle_async_request\n",
      "    response = await connection.handle_async_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 103, in handle_async_request\n",
      "    return await self._connection.handle_async_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 136, in handle_async_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 106, in handle_async_request\n",
      "    ) = await self._receive_response_headers(**kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 177, in _receive_response_headers\n",
      "    event = await self._receive_event(timeout=timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 231, in _receive_event\n",
      "    raise RemoteProtocolError(msg)\n",
      "httpcore.RemoteProtocolError: Server disconnected without sending a response.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1532, in request\n",
      "    response = await self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 1629, in send\n",
      "    response = await self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 1657, in _send_handling_auth\n",
      "    response = await self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 1694, in _send_handling_redirects\n",
      "    response = await self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 1730, in _send_single_request\n",
      "    response = await transport.handle_async_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 393, in handle_async_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"C:\\Python312\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.RemoteProtocolError: Server disconnected without sending a response.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Gajanan Vigneswaran\\AppData\\Local\\Temp\\ipykernel_29988\\331723145.py\", line 9, in <module>\n",
      "    result = baseline_b_agent({\"question\": test_question})\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Gajanan Vigneswaran\\AppData\\Local\\Temp\\ipykernel_29988\\462475301.py\", line 16, in baseline_b_agent\n",
      "    result = asyncio.run(\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langgraph\\pregel\\main.py\", line 3158, in ainvoke\n",
      "    async for chunk in self.astream(\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langgraph\\pregel\\main.py\", line 2971, in astream\n",
      "    async for _ in runner.atick(\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py\", line 304, in atick\n",
      "    await arun_with_retry(\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py\", line 137, in arun_with_retry\n",
      "    return await task.proc.ainvoke(task.input, config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py\", line 705, in ainvoke\n",
      "    input = await asyncio.create_task(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\asyncio\\futures.py\", line 287, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\asyncio\\tasks.py\", line 385, in __wakeup\n",
      "    future.result()\n",
      "  File \"C:\\Python312\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py\", line 473, in ainvoke\n",
      "    ret = await self.afunc(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Gajanan Vigneswaran\\AppData\\Local\\Temp\\ipykernel_29988\\1706208780.py\", line 16, in final_report_generation\n",
      "    final_report = await writer_llm.ainvoke([HumanMessage(content=prompt)])\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 425, in ainvoke\n",
      "    llm_result = await self.agenerate_prompt(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1132, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1090, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"C:\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1343, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 1628, in _agenerate\n",
      "    raise e\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 1621, in _agenerate\n",
      "    raw_response = await self.async_client.with_raw_response.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\openai\\_legacy_response.py\", line 381, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2678, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1797, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1564, in request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "openai.APIConnectionError: Connection error.\n",
      "During task with name 'final_report_generation' and id 'bc89c75b-6e13-dcae-a34f-aacb7b3ccaf7'\n"
     ]
    },
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRemoteProtocolError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:103\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpcore\\_async\\http11.py:136\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpcore\\_async\\http11.py:106\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_response_headers(**kwargs)\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpcore\\_async\\http11.py:177\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpcore\\_async\\http11.py:231\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    230\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mServer disconnected without sending a response.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteProtocolError(msg)\n\u001b[32m    233\u001b[39m \u001b[38;5;28mself\u001b[39m._h11_state.receive_data(data)\n",
      "\u001b[31mRemoteProtocolError\u001b[39m: Server disconnected without sending a response.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRemoteProtocolError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\openai\\_base_client.py:1532\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.send(\n\u001b[32m   1533\u001b[39m         request,\n\u001b[32m   1534\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream_response_body(request=request),\n\u001b[32m   1535\u001b[39m         **kwargs,\n\u001b[32m   1536\u001b[39m     )\n\u001b[32m   1537\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpx\\_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpx\\_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpx\\_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpx\\_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_async_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mRemoteProtocolError\u001b[39m: Server disconnected without sending a response.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNOTE: System will use supervisor to coordinate multiple research agents in parallel.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     result = \u001b[43mbaseline_b_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_question\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRESEARCH REPORT\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mbaseline_b_agent\u001b[39m\u001b[34m(inputs)\u001b[39m\n\u001b[32m     13\u001b[39m question = inputs.get(\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Run with high recursion limit\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m result = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresearch_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mainvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrecursion_limit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m: result.get(\u001b[33m\"\u001b[39m\u001b[33mfinal_report\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mresearch_brief\u001b[39m\u001b[33m\"\u001b[39m: result.get(\u001b[33m\"\u001b[39m\u001b[33mresearch_brief\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnotes\u001b[39m\u001b[33m\"\u001b[39m: result.get(\u001b[33m\"\u001b[39m\u001b[33mnotes\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m     27\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\asyncio\\futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\asyncio\\tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:3158\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3155\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3156\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3158\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   3159\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3160\u001b[39m     config,\n\u001b[32m   3161\u001b[39m     context=context,\n\u001b[32m   3162\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3163\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3164\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3165\u001b[39m     print_mode=print_mode,\n\u001b[32m   3166\u001b[39m     output_keys=output_keys,\n\u001b[32m   3167\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3168\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3169\u001b[39m     durability=durability,\n\u001b[32m   3170\u001b[39m     **kwargs,\n\u001b[32m   3171\u001b[39m ):\n\u001b[32m   3172\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3173\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2971\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2969\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2970\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2972\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2973\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2974\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2975\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2976\u001b[39m ):\n\u001b[32m   2977\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2978\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2979\u001b[39m         stream_mode,\n\u001b[32m   2980\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2983\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2984\u001b[39m     ):\n\u001b[32m   2985\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:304\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    302\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    305\u001b[39m         t,\n\u001b[32m    306\u001b[39m         retry_policy,\n\u001b[32m    307\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    308\u001b[39m         configurable={\n\u001b[32m    309\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    310\u001b[39m                 _acall,\n\u001b[32m    311\u001b[39m                 weakref.ref(t),\n\u001b[32m    312\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    313\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    314\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    315\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    316\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    317\u001b[39m                 loop=loop,\n\u001b[32m    318\u001b[39m             ),\n\u001b[32m    319\u001b[39m         },\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:705\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    706\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    707\u001b[39m         )\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\asyncio\\futures.py:287\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    286\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\asyncio\\tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\asyncio\\futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\asyncio\\tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:473\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mfinal_report_generation\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      8\u001b[39m findings = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(notes)\n\u001b[32m     10\u001b[39m prompt = final_report_generation_prompt.format(\n\u001b[32m     11\u001b[39m     research_brief=state.get(\u001b[33m\"\u001b[39m\u001b[33mresearch_brief\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     12\u001b[39m     findings=findings,\n\u001b[32m     13\u001b[39m     date=get_today_str()\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m final_report = \u001b[38;5;28;01mawait\u001b[39;00m writer_llm.ainvoke([HumanMessage(content=prompt)])\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfinal_report\u001b[39m\u001b[33m\"\u001b[39m: final_report.content,\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mHere is the final report: \u001b[39m\u001b[33m\"\u001b[39m + final_report.content],\n\u001b[32m     21\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:425\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    415\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    417\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    422\u001b[39m     **kwargs: Any,\n\u001b[32m    423\u001b[39m ) -> AIMessage:\n\u001b[32m    424\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    426\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    427\u001b[39m         stop=stop,\n\u001b[32m    428\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    429\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    430\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    431\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    432\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    433\u001b[39m         **kwargs,\n\u001b[32m    434\u001b[39m     )\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    436\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m, cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n\u001b[32m    437\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1132\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1123\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1124\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m   1125\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1129\u001b[39m     **kwargs: Any,\n\u001b[32m   1130\u001b[39m ) -> LLMResult:\n\u001b[32m   1131\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m   1133\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m   1134\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1090\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m   1077\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m   1078\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1079\u001b[39m             *[\n\u001b[32m   1080\u001b[39m                 run_manager.on_llm_end(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1088\u001b[39m             ]\n\u001b[32m   1089\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1090\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m   1091\u001b[39m flattened_outputs = [\n\u001b[32m   1092\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1093\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m   1094\u001b[39m ]\n\u001b[32m   1095\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\asyncio\\tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1343\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1341\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1342\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1344\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1345\u001b[39m     )\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1347\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1628\u001b[39m, in \u001b[36mBaseChatOpenAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1626\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1627\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1628\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1629\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1630\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1631\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1632\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1633\u001b[39m ):\n\u001b[32m   1634\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1621\u001b[39m, in \u001b[36mBaseChatOpenAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1614\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1615\u001b[39m             response,\n\u001b[32m   1616\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1617\u001b[39m             metadata=generation_info,\n\u001b[32m   1618\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1619\u001b[39m         )\n\u001b[32m   1620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1621\u001b[39m         raw_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client.with_raw_response.create(\n\u001b[32m   1622\u001b[39m             **payload\n\u001b[32m   1623\u001b[39m         )\n\u001b[32m   1624\u001b[39m         response = raw_response.parse()\n\u001b[32m   1625\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\openai\\_legacy_response.py:381\u001b[39m, in \u001b[36masync_to_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    377\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    379\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2678\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2631\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2632\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2633\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2675\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   2676\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2677\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2678\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2679\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2680\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2681\u001b[39m             {\n\u001b[32m   2682\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2683\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2684\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2685\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2686\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2687\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2688\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2689\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2690\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2691\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2692\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2693\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2694\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2695\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2696\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2697\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2698\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2699\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_retention\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_retention,\n\u001b[32m   2700\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2701\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2702\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2703\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2704\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2705\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2706\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2707\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2708\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2709\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2710\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2711\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2712\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2713\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2714\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2715\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2716\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2717\u001b[39m             },\n\u001b[32m   2718\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2719\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2720\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2721\u001b[39m         ),\n\u001b[32m   2722\u001b[39m         options=make_request_options(\n\u001b[32m   2723\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2724\u001b[39m         ),\n\u001b[32m   2725\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2726\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2727\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2728\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\openai\\_base_client.py:1797\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1784\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1785\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1792\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1793\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1794\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1795\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1796\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1797\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gajanan Vigneswaran\\OneDrive - Ryerson University\\Desktop\\Deep-Research-Agent\\venv\\Lib\\site-packages\\openai\\_base_client.py:1564\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1561\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1563\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1564\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1566\u001b[39m log.debug(\n\u001b[32m   1567\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   1568\u001b[39m     request.method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1572\u001b[39m     response.headers,\n\u001b[32m   1573\u001b[39m )\n\u001b[32m   1574\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mrequest_id: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, response.headers.get(\u001b[33m\"\u001b[39m\u001b[33mx-request-id\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mAPIConnectionError\u001b[39m: Connection error.",
      "During task with name 'final_report_generation' and id 'bc89c75b-6e13-dcae-a34f-aacb7b3ccaf7'"
     ]
    }
   ],
   "source": [
    "# Simple test\n",
    "test_question = \"What are the key benefits and challenges of using large language models in enterprise applications?\"\n",
    "\n",
    "print(f\"Testing with question: {test_question}\\n\")\n",
    "print(\"Running multi-agent research system (this may take several minutes)...\\n\")\n",
    "print(\"NOTE: System will use supervisor to coordinate multiple research agents in parallel.\\n\")\n",
    "\n",
    "try:\n",
    "    result = baseline_b_agent({\"question\": test_question})\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"RESEARCH REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    print(result[\"output\"][:3000] + \"...\" if len(result[\"output\"]) > 3000 else result[\"output\"])\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Report length: {len(result['output'])} characters\")\n",
    "    print(f\"Research brief: {result.get('research_brief', 'N/A')[:200]}...\")\n",
    "    print(f\"Number of research notes: {len(result.get('notes', []))}\")\n",
    "    print(\"Agent test PASSED ✓\")\n",
    "except Exception as e:\n",
    "    print(f\"Agent test FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluation Harness Integration\n",
    "\n",
    "Once the manual test passes, uncomment and run the cells below for full evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import evaluation harness\n",
    "# import sys\n",
    "# sys.path.insert(0, \"..\")\n",
    "# from evaluation import ExperimentHarness, fact_recall, coherence_judge, depth_judge, relevance_judge\n",
    "\n",
    "# # Initialize harness\n",
    "# harness = ExperimentHarness(\"../data/deep_research_agent_test_dataset.yaml\")\n",
    "\n",
    "# # Run evaluation\n",
    "# results = harness.run_evaluation(\n",
    "#     agent_fn=baseline_b_agent,\n",
    "#     evaluators=[fact_recall, coherence_judge, depth_judge, relevance_judge],\n",
    "#     experiment_name=\"baseline_b_supervisor\",\n",
    "#     monte_carlo_runs=3\n",
    "# )\n",
    "\n",
    "# print(f\"Evaluation complete!\")\n",
    "# print(f\"Metrics: {results.metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
