{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Combined Tier 1: Skeleton-First Evidence Orchestration (SFEO) Research Agent\n",
        "\n",
        "This notebook implements the **Skeleton-First Evidence Orchestration (SFEO)** architecture, combining the validated Tier 1 paradigms:\n",
        "\n",
        "- **Iterative Refinement V2** (Patch-Based): Skeleton-driven document structure with semantic addressing\n",
        "- **Agile Sprints**: Sprint-based evidence gathering with retrospectives\n",
        "- **Knowledge Cache**: Cascading 3-layer cache for search optimization\n",
        "- **Quality Gates**: Multi-stage quality checkpoints preventing error propagation\n",
        "\n",
        "## Core Innovation\n",
        "\n",
        "Traditional deep research: `Question → Research → Synthesize → Write`\n",
        "\n",
        "SFEO inverts this: `Question → Hypothesize Structure → Research to Validate → Construct with Evidence → Refine Gaps`\n",
        "\n",
        "## The Three Laws of SFEO\n",
        "\n",
        "1. **Structure Precedes Content**: The document skeleton drives research, creating focused queries\n",
        "2. **Quality Gates Guard Transitions**: No phase proceeds until quality criteria are met\n",
        "3. **The Cache is Omnipresent**: Every search passes through the knowledge cache\n",
        "\n",
        "## Architecture Phases\n",
        "\n",
        "- **Phase A**: Strategic Planning (Query → Skeleton → Claims → Backlog) + Gate 1\n",
        "- **Phase B**: Evidence Gathering (Sprint Loop with Cache) + Gate 2\n",
        "- **Phase C**: Document Construction (Prose Patches → Bridges → Assembly) + Gate 3\n",
        "- **Phase D**: Refinement & Polish (Critique → Patch → Cascade → Final)\n",
        "\n",
        "## Technology Stack\n",
        "\n",
        "- **LLM**: `gpt-5-mini-2025-08-07`\n",
        "- **Web Search**: Tavily API\n",
        "- **Embeddings**: OpenAI text-embedding-3-small\n",
        "- **Tracing**: LangSmith\n",
        "- **Framework**: LangGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import operator\n",
        "import asyncio\n",
        "import hashlib\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Annotated, TypedDict, Literal, Optional, Any\n",
        "from urllib.parse import urlparse\n",
        "from datetime import datetime\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from tavily import TavilyClient\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Load environment variables\n",
        "env_path = Path(\"../.env\")\n",
        "load_dotenv(env_path)\n",
        "\n",
        "# Configure LangSmith tracing\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"deep_research_sfeo\"\n",
        "\n",
        "print(\"Environment configured successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize LLM, Tavily, and Embeddings\n",
        "MODEL_NAME = \"gpt-5-mini-2025-08-07\"\n",
        "llm = ChatOpenAI(model=MODEL_NAME, temperature=0, max_retries=10)\n",
        "tavily_client = TavilyClient()\n",
        "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# PHASE A: STRATEGIC PLANNING CONFIGURATION\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "SKELETON_CONFIG = {\n",
        "    \"min_sections\": 4,\n",
        "    \"max_sections\": 7,\n",
        "    \"target_words_per_section\": 300,\n",
        "    \"max_skeleton_refinement_attempts\": 2\n",
        "}\n",
        "\n",
        "GATE_1_CONFIG = {\n",
        "    \"thesis_clarity_threshold\": 7.0,\n",
        "    \"coverage_threshold\": 0.8,\n",
        "    \"structure_threshold\": 7.0,\n",
        "    \"max_attempts\": 2\n",
        "}\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# PHASE B: EVIDENCE GATHERING CONFIGURATION  \n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "SPRINT_CONFIG = {\n",
        "    \"max_sprints\": 3,\n",
        "    \"claims_per_sprint\": 4,\n",
        "    \"searches_per_claim\": 2,\n",
        "    \"min_sources_per_section\": 2\n",
        "}\n",
        "\n",
        "CACHE_CONFIG = {\n",
        "    \"high_confidence_threshold\": 0.75,\n",
        "    \"low_confidence_threshold\": 0.40,\n",
        "    \"specificity_adjustment\": 0.15,\n",
        "    \"chunk_size\": 500,\n",
        "    \"chunk_overlap\": 100,\n",
        "    \"top_k_retrieval\": 5\n",
        "}\n",
        "\n",
        "GATE_2_CONFIG = {\n",
        "    \"section_coverage_threshold\": 0.7,\n",
        "    \"min_domain_diversity\": 4,\n",
        "    \"verification_rate_threshold\": 0.6,\n",
        "    \"max_emergency_attempts\": 1\n",
        "}\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# PHASE C: DOCUMENT CONSTRUCTION CONFIGURATION\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "PROSE_CONFIG = {\n",
        "    \"min_words_per_section\": 200,\n",
        "    \"max_words_per_section\": 500,\n",
        "    \"require_bridge_sentences\": True\n",
        "}\n",
        "\n",
        "GATE_3_CONFIG = {\n",
        "    \"coherence_threshold\": 6.5,\n",
        "    \"depth_threshold\": 6.5,\n",
        "    \"min_citations_per_section\": 1\n",
        "}\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# PHASE D: REFINEMENT CONFIGURATION\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "REFINEMENT_CONFIG = {\n",
        "    \"max_iterations\": 2,\n",
        "    \"quality_threshold\": 7.5,\n",
        "    \"min_improvement_threshold\": 0.3,\n",
        "    \"max_cascades_per_iteration\": 3\n",
        "}\n",
        "\n",
        "print(f\"Using model: {MODEL_NAME}\")\n",
        "print(f\"Sprint config: max {SPRINT_CONFIG['max_sprints']} sprints, {SPRINT_CONFIG['claims_per_sprint']} claims/sprint\")\n",
        "print(f\"Cache thresholds: HIGH >= {CACHE_CONFIG['high_confidence_threshold']}, LOW < {CACHE_CONFIG['low_confidence_threshold']}\")\n",
        "print(f\"Refinement: max {REFINEMENT_CONFIG['max_iterations']} iterations, quality target {REFINEMENT_CONFIG['quality_threshold']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Models\n",
        "\n",
        "Combined data models from all Tier 1 paradigms:\n",
        "- **Skeleton & Claims** (from Iterative Refinement)\n",
        "- **Cache Structures** (from Knowledge Cache)\n",
        "- **Gate Results** (from Quality Gates)\n",
        "- **Sprint Tracking** (from Agile Sprints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SKELETON STRUCTURE (from Iterative Refinement)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "class SkeletonNode(BaseModel):\n",
        "    \"\"\"A node in the document skeleton hierarchy.\"\"\"\n",
        "    node_id: str = Field(description=\"Unique identifier like 'sec:intro' or 'sec:methods'\")\n",
        "    title: str = Field(description=\"Section title for the final document\")\n",
        "    intent: str = Field(description=\"1-3 sentence description of what this section should accomplish\")\n",
        "    target_word_count: int = Field(default=300, description=\"Approximate target length\")\n",
        "    dependencies: List[str] = Field(default_factory=list, description=\"Node IDs this section depends on\")\n",
        "    children: List[str] = Field(default_factory=list, description=\"Child node IDs (empty for leaf nodes)\")\n",
        "    claim_placeholders: List[str] = Field(default_factory=list, description=\"Claims this section must support\")\n",
        "    is_expanded: bool = Field(default=False, description=\"Whether prose has been generated\")\n",
        "\n",
        "\n",
        "class DocumentSkeleton(BaseModel):\n",
        "    \"\"\"The complete document skeleton structure.\"\"\"\n",
        "    thesis: str = Field(description=\"One-sentence statement of the document's central purpose\")\n",
        "    root_nodes: List[str] = Field(description=\"Top-level section node IDs in document order\")\n",
        "    nodes: Dict[str, SkeletonNode] = Field(default_factory=dict, description=\"All nodes by ID\")\n",
        "    style_constraints: str = Field(default=\"\", description=\"Global style guidelines\")\n",
        "\n",
        "\n",
        "class SkeletonGenerationOutput(BaseModel):\n",
        "    \"\"\"Output schema for skeleton generation.\"\"\"\n",
        "    thesis: str = Field(description=\"One-sentence thesis statement\")\n",
        "    sections: List[SkeletonNode] = Field(description=\"All sections in document order\")\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# CLAIMS & EVIDENCE (from Iterative Refinement)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "class Claim(BaseModel):\n",
        "    \"\"\"A verifiable assertion in the document.\"\"\"\n",
        "    claim_id: str = Field(description=\"Unique identifier for this claim\")\n",
        "    claim_text: str = Field(description=\"The assertion itself, stated precisely\")\n",
        "    source_node: str = Field(description=\"Skeleton node ID where this claim appears\")\n",
        "    verification_status: Literal[\"unverified\", \"verified\", \"contested\", \"retracted\"] = Field(\n",
        "        default=\"unverified\", description=\"Current verification state\"\n",
        "    )\n",
        "    supporting_evidence: List[str] = Field(default_factory=list, description=\"Sources supporting this claim\")\n",
        "    claim_dependencies: List[str] = Field(default_factory=list, description=\"Other claim IDs this depends on\")\n",
        "\n",
        "\n",
        "class ClaimExtractionOutput(BaseModel):\n",
        "    \"\"\"Output schema for claim extraction.\"\"\"\n",
        "    claims: List[Claim] = Field(description=\"All factual claims extracted\")\n",
        "\n",
        "\n",
        "class ResearchTask(BaseModel):\n",
        "    \"\"\"A research task in the backlog.\"\"\"\n",
        "    task_id: str = Field(description=\"Unique identifier\")\n",
        "    claim_id: str = Field(description=\"Associated claim ID\")\n",
        "    query: str = Field(description=\"Search query to execute\")\n",
        "    priority: int = Field(default=1, description=\"Priority (1=highest)\")\n",
        "    status: Literal[\"pending\", \"in_progress\", \"completed\", \"failed\"] = Field(default=\"pending\")\n",
        "    evidence_found: List[str] = Field(default_factory=list)\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# PROSE ENTRIES (from Iterative Refinement)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "class ProseEntry(BaseModel):\n",
        "    \"\"\"Content stored for each expanded node.\"\"\"\n",
        "    node_id: str = Field(description=\"The skeleton node this prose belongs to\")\n",
        "    main_content: str = Field(description=\"The substantive prose for this section\")\n",
        "    bridge_in: str = Field(default=\"\", description=\"Transitional sentences from previous section\")\n",
        "    bridge_out: str = Field(default=\"\", description=\"Transitional sentences to next section\")\n",
        "    summary: str = Field(default=\"\", description=\"1-2 sentence compression of content\")\n",
        "    revision_count: int = Field(default=0)\n",
        "    citations_used: List[str] = Field(default_factory=list)\n",
        "\n",
        "\n",
        "class ProseGenerationOutput(BaseModel):\n",
        "    \"\"\"Output schema for prose generation.\"\"\"\n",
        "    bridge_in: str = Field(description=\"Transitional sentences from previous section\")\n",
        "    main_content: str = Field(description=\"The main prose content for this section\")\n",
        "    bridge_out: str = Field(description=\"Transitional sentences to next section\")\n",
        "    summary: str = Field(description=\"1-2 sentence summary\")\n",
        "    citations_used: List[str] = Field(default_factory=list)\n",
        "\n",
        "\n",
        "print(\"Skeleton and Claims models defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# CRITIQUE & NOISE MAP (from Iterative Refinement)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "class CritiqueIssue(BaseModel):\n",
        "    \"\"\"An issue identified during critique.\"\"\"\n",
        "    issue_id: str = Field(description=\"Unique identifier\")\n",
        "    scope: Literal[\"global\", \"section\", \"transition\"] = Field(description=\"Level of the issue\")\n",
        "    target_nodes: List[str] = Field(description=\"Affected skeleton node IDs\")\n",
        "    issue_type: Literal[\"weak_claim\", \"missing_evidence\", \"logical_gap\", \"unclear\", \n",
        "                        \"coherence\", \"depth\", \"transition\", \"contradiction\"] = Field(\n",
        "        description=\"Category of issue\"\n",
        "    )\n",
        "    severity: Literal[\"critical\", \"major\", \"minor\"] = Field(description=\"How serious\")\n",
        "    affected_claims: List[str] = Field(default_factory=list, description=\"Claim IDs affected\")\n",
        "    description: str = Field(description=\"What the problem is\")\n",
        "    suggestion: str = Field(description=\"How to fix it\")\n",
        "    search_query: str = Field(default=\"\", description=\"Query to find evidence if needed\")\n",
        "\n",
        "\n",
        "class CritiqueResult(BaseModel):\n",
        "    \"\"\"Complete critique output.\"\"\"\n",
        "    overall_quality: float = Field(description=\"Quality score 1-10\")\n",
        "    issues: List[CritiqueIssue] = Field(default_factory=list)\n",
        "    strengths: str = Field(default=\"\", description=\"What the document does well\")\n",
        "    summary: str = Field(description=\"Overall assessment\")\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# CACHE STRUCTURES (from Knowledge Cache)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "class CachedDocument(BaseModel):\n",
        "    \"\"\"A cached web document.\"\"\"\n",
        "    url: str = Field(description=\"Original URL\")\n",
        "    normalized_url: str = Field(description=\"Normalized URL for lookup\")\n",
        "    content: str = Field(description=\"Full text content\")\n",
        "    content_hash: str = Field(description=\"SHA-256 hash of content\")\n",
        "    title: str = Field(default=\"\", description=\"Page title\")\n",
        "    retrieval_timestamp: str = Field(description=\"When this was retrieved\")\n",
        "    source_query: str = Field(default=\"\", description=\"Query that led to this content\")\n",
        "\n",
        "\n",
        "class CachedChunk(BaseModel):\n",
        "    \"\"\"A chunk of content with embedding.\"\"\"\n",
        "    chunk_id: str = Field(description=\"Unique identifier\")\n",
        "    text: str = Field(description=\"Chunk text content\")\n",
        "    embedding: List[float] = Field(description=\"Vector embedding\")\n",
        "    source_url: str = Field(description=\"Source document URL\")\n",
        "    position: int = Field(description=\"Position within source document\")\n",
        "\n",
        "\n",
        "class CacheDecision(BaseModel):\n",
        "    \"\"\"Record of a cache decision for observability.\"\"\"\n",
        "    query: str\n",
        "    layer_reached: Literal[\"L1\", \"L2\", \"L3\"]\n",
        "    decision: Literal[\"HIT\", \"HIGH_CONF\", \"MEDIUM_CONF\", \"LOW_CONF\",\n",
        "                      \"SUFFICIENT\", \"PARTIAL\", \"INSUFFICIENT\"]\n",
        "    confidence_score: float = 0.0\n",
        "    action_taken: Literal[\"USE_CACHE\", \"SEARCH\", \"TARGETED_SEARCH\"]\n",
        "    reasoning: str = \"\"\n",
        "    timestamp: str = \"\"\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# GATE RESULTS (from Quality Gates)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "class GateResult(BaseModel):\n",
        "    \"\"\"Result of a quality gate check.\"\"\"\n",
        "    gate_name: str = Field(description=\"Which gate (gate_1, gate_2, gate_3)\")\n",
        "    passed: bool = Field(description=\"Whether the gate was passed\")\n",
        "    scores: Dict[str, float] = Field(default_factory=dict, description=\"Individual criterion scores\")\n",
        "    reason: str = Field(description=\"Explanation of the gate result\")\n",
        "    suggestions: List[str] = Field(default_factory=list, description=\"Improvement suggestions\")\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SPRINT TRACKING (from Agile Sprints)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "class SprintFinding(BaseModel):\n",
        "    \"\"\"A finding from a research sprint.\"\"\"\n",
        "    claim_id: str = Field(description=\"The claim this finding addresses\")\n",
        "    finding: str = Field(description=\"The key finding or insight\")\n",
        "    sources: List[str] = Field(default_factory=list, description=\"Source URLs\")\n",
        "    cache_hit: bool = Field(default=False, description=\"Whether this came from cache\")\n",
        "\n",
        "\n",
        "print(\"Critique, Cache, Gate, and Sprint models defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Knowledge Cache Integration\n",
        "\n",
        "The Knowledge Cache provides a 3-layer cascading lookup:\n",
        "- **Layer 1**: Deterministic deduplication (exact query/URL matching)\n",
        "- **Layer 2**: Semantic similarity (vector search with confidence scoring)\n",
        "- **Layer 3**: LLM judgment (gap analysis for medium-confidence hits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KnowledgeBase:\n",
        "    \"\"\"Session-scoped knowledge base with cascading cache capabilities.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.url_registry: Dict[str, CachedDocument] = {}\n",
        "        self.query_cache: Dict[str, Dict] = {}\n",
        "        self.chunks: List[CachedChunk] = []\n",
        "        self.chunk_embeddings: Optional[np.ndarray] = None\n",
        "\n",
        "        # Statistics\n",
        "        self.stats = {\n",
        "            \"total_queries\": 0,\n",
        "            \"l1_hits\": 0,\n",
        "            \"l2_high\": 0,\n",
        "            \"l2_medium\": 0,\n",
        "            \"l2_low\": 0,\n",
        "            \"l3_sufficient\": 0,\n",
        "            \"l3_partial\": 0,\n",
        "            \"l3_insufficient\": 0,\n",
        "            \"web_searches_executed\": 0,\n",
        "            \"web_searches_avoided\": 0\n",
        "        }\n",
        "\n",
        "    # === URL Normalization ===\n",
        "    def normalize_url(self, url: str) -> str:\n",
        "        \"\"\"Normalize URL for consistent lookup.\"\"\"\n",
        "        try:\n",
        "            parsed = urlparse(url)\n",
        "            host = parsed.netloc.lower()\n",
        "            if host.startswith(\"www.\"):\n",
        "                host = host[4:]\n",
        "            path = parsed.path.rstrip(\"/\")\n",
        "            tracking_params = {\"utm_source\", \"utm_medium\", \"utm_campaign\", \"ref\", \"fbclid\"}\n",
        "            query_params = sorted(parsed.query.split(\"&\")) if parsed.query else []\n",
        "            query_params = [p for p in query_params if p.split(\"=\")[0] not in tracking_params]\n",
        "            query = \"&\".join(query_params)\n",
        "            normalized = f\"https://{host}{path}\"\n",
        "            if query:\n",
        "                normalized += f\"?{query}\"\n",
        "            return normalized\n",
        "        except:\n",
        "            return url.lower()\n",
        "\n",
        "    # === Query Normalization ===\n",
        "    def normalize_query_light(self, query: str) -> str:\n",
        "        \"\"\"Light normalization: lowercase, collapse whitespace.\"\"\"\n",
        "        return \" \".join(query.lower().split())\n",
        "\n",
        "    def normalize_query_aggressive(self, query: str) -> str:\n",
        "        \"\"\"Aggressive normalization: remove stop words, sort terms.\"\"\"\n",
        "        stop_words = {\"the\", \"a\", \"an\", \"is\", \"are\", \"of\", \"in\", \"to\", \"for\", \"and\", \"or\", \n",
        "                      \"what\", \"how\", \"why\", \"when\", \"where\"}\n",
        "        light = self.normalize_query_light(query)\n",
        "        terms = [t for t in light.split() if t not in stop_words and len(t) > 1]\n",
        "        return \" \".join(sorted(terms))\n",
        "\n",
        "    # === Content Hashing ===\n",
        "    def compute_content_hash(self, content: str) -> str:\n",
        "        \"\"\"Compute SHA-256 hash of content.\"\"\"\n",
        "        return hashlib.sha256(content.encode()).hexdigest()\n",
        "\n",
        "    # === Document Storage ===\n",
        "    def add_document(self, url: str, content: str, title: str = \"\", source_query: str = \"\"):\n",
        "        \"\"\"Add a document to the knowledge base.\"\"\"\n",
        "        normalized_url = self.normalize_url(url)\n",
        "        doc = CachedDocument(\n",
        "            url=url,\n",
        "            normalized_url=normalized_url,\n",
        "            content=content,\n",
        "            content_hash=self.compute_content_hash(content),\n",
        "            title=title,\n",
        "            retrieval_timestamp=datetime.now().isoformat(),\n",
        "            source_query=source_query\n",
        "        )\n",
        "        self.url_registry[normalized_url] = doc\n",
        "        self._chunk_and_embed(doc)\n",
        "        return doc\n",
        "\n",
        "    def _chunk_and_embed(self, doc: CachedDocument):\n",
        "        \"\"\"Chunk document and compute embeddings.\"\"\"\n",
        "        content = doc.content\n",
        "        chunks_text = []\n",
        "        chunk_size = CACHE_CONFIG[\"chunk_size\"]\n",
        "        chunk_overlap = CACHE_CONFIG[\"chunk_overlap\"]\n",
        "\n",
        "        for i in range(0, len(content), chunk_size - chunk_overlap):\n",
        "            chunk_text = content[i:i + chunk_size]\n",
        "            if len(chunk_text) > 50:\n",
        "                chunks_text.append(chunk_text)\n",
        "\n",
        "        if not chunks_text:\n",
        "            return\n",
        "\n",
        "        embeddings = embeddings_model.embed_documents(chunks_text)\n",
        "\n",
        "        for i, (text, embedding) in enumerate(zip(chunks_text, embeddings)):\n",
        "            chunk = CachedChunk(\n",
        "                chunk_id=f\"{doc.content_hash[:8]}_{i}\",\n",
        "                text=text,\n",
        "                embedding=embedding,\n",
        "                source_url=doc.url,\n",
        "                position=i\n",
        "            )\n",
        "            self.chunks.append(chunk)\n",
        "\n",
        "        self._update_embedding_matrix()\n",
        "\n",
        "    def _update_embedding_matrix(self):\n",
        "        \"\"\"Update the numpy matrix of embeddings for fast search.\"\"\"\n",
        "        if self.chunks:\n",
        "            self.chunk_embeddings = np.array([c.embedding for c in self.chunks])\n",
        "\n",
        "    # === Query Cache ===\n",
        "    def add_query(self, query: str, result_urls: List[str], result_summary: str):\n",
        "        \"\"\"Add a query to the cache.\"\"\"\n",
        "        entry = {\n",
        "            \"original_query\": query,\n",
        "            \"light_normalized\": self.normalize_query_light(query),\n",
        "            \"aggressive_normalized\": self.normalize_query_aggressive(query),\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"result_urls\": result_urls,\n",
        "            \"result_summary\": result_summary\n",
        "        }\n",
        "        self.query_cache[entry[\"light_normalized\"]] = entry\n",
        "        self.query_cache[entry[\"aggressive_normalized\"]] = entry\n",
        "        return entry\n",
        "\n",
        "    # === Lookups ===\n",
        "    def lookup_query_exact(self, query: str) -> Optional[Dict]:\n",
        "        \"\"\"Check for exact query match.\"\"\"\n",
        "        light = self.normalize_query_light(query)\n",
        "        return self.query_cache.get(light)\n",
        "\n",
        "    def lookup_query_aggressive(self, query: str) -> Optional[Dict]:\n",
        "        \"\"\"Check for bag-of-words query match.\"\"\"\n",
        "        aggressive = self.normalize_query_aggressive(query)\n",
        "        return self.query_cache.get(aggressive)\n",
        "\n",
        "    # === Semantic Search ===\n",
        "    def semantic_search(self, query: str, top_k: int = None) -> List[Tuple[CachedChunk, float]]:\n",
        "        \"\"\"Find semantically similar chunks.\"\"\"\n",
        "        if top_k is None:\n",
        "            top_k = CACHE_CONFIG[\"top_k_retrieval\"]\n",
        "            \n",
        "        if not self.chunks or self.chunk_embeddings is None:\n",
        "            return []\n",
        "\n",
        "        query_embedding = np.array(embeddings_model.embed_query(query))\n",
        "        similarities = np.dot(self.chunk_embeddings, query_embedding) / (\n",
        "            np.linalg.norm(self.chunk_embeddings, axis=1) * np.linalg.norm(query_embedding) + 1e-8\n",
        "        )\n",
        "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            results.append((self.chunks[idx], float(similarities[idx])))\n",
        "        return results\n",
        "\n",
        "    def get_stats_summary(self) -> str:\n",
        "        \"\"\"Get human-readable stats summary.\"\"\"\n",
        "        total = self.stats[\"total_queries\"]\n",
        "        if total == 0:\n",
        "            return \"No queries processed yet.\"\n",
        "        avoided = self.stats[\"web_searches_avoided\"]\n",
        "        hit_rate = avoided / total * 100 if total > 0 else 0\n",
        "        return f\"Cache: {avoided}/{total} avoided ({hit_rate:.1f}% hit rate), {len(self.chunks)} chunks indexed\"\n",
        "\n",
        "\n",
        "print(\"KnowledgeBase class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# HELPER FUNCTIONS\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def extract_domain(url: str) -> str:\n",
        "    \"\"\"Extract domain from URL.\"\"\"\n",
        "    try:\n",
        "        parsed = urlparse(url)\n",
        "        domain = parsed.netloc\n",
        "        if domain.startswith(\"www.\"):\n",
        "            domain = domain[4:]\n",
        "        return domain\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "\n",
        "def search_web(query: str, max_results: int = 8) -> Tuple[str, List[str], List[str]]:\n",
        "    \"\"\"Execute web search using Tavily. Returns (summary, results, urls).\"\"\"\n",
        "    try:\n",
        "        if len(query) > 400:\n",
        "            query = query[:400]\n",
        "\n",
        "        response = tavily_client.search(\n",
        "            query=query,\n",
        "            max_results=max_results,\n",
        "            include_answer=True\n",
        "        )\n",
        "\n",
        "        results = []\n",
        "        urls = []\n",
        "        summary = response.get(\"answer\", \"\")\n",
        "\n",
        "        for r in response.get(\"results\", []):\n",
        "            url = r.get('url', '')\n",
        "            urls.append(url)\n",
        "            content = r.get('content', '')[:500]\n",
        "            title = r.get('title', 'No title')\n",
        "            results.append(f\"[{title}] {content}... (Source: {url})\")\n",
        "\n",
        "        return summary, results, urls\n",
        "    except Exception as e:\n",
        "        return f\"Search error: {str(e)}\", [], []\n",
        "\n",
        "\n",
        "def compute_confidence(top_results: List[Tuple[CachedChunk, float]], query: str) -> float:\n",
        "    \"\"\"Compute multi-signal confidence score for cache results.\"\"\"\n",
        "    if not top_results:\n",
        "        return 0.0\n",
        "\n",
        "    top_score = top_results[0][1]\n",
        "    score_gap = top_results[0][1] - top_results[1][1] if len(top_results) > 1 else top_score\n",
        "\n",
        "    query_terms = set(query.lower().split())\n",
        "    top_chunk_terms = set(top_results[0][0].text.lower().split())\n",
        "    term_overlap = len(query_terms & top_chunk_terms) / len(query_terms | top_chunk_terms) if query_terms | top_chunk_terms else 0\n",
        "\n",
        "    # Weighted combination\n",
        "    confidence = 0.5 * top_score + 0.25 * min(score_gap * 2, 1.0) + 0.25 * term_overlap\n",
        "    return max(0.0, min(confidence, 1.0))\n",
        "\n",
        "\n",
        "async def cascaded_search(query: str, kb: KnowledgeBase) -> Tuple[str, List[str], CacheDecision]:\n",
        "    \"\"\"Execute the full cascading cache check and search if needed.\"\"\"\n",
        "    kb.stats[\"total_queries\"] += 1\n",
        "    timestamp = datetime.now().isoformat()\n",
        "\n",
        "    # === Layer 1: Deterministic Deduplication ===\n",
        "    exact_match = kb.lookup_query_exact(query)\n",
        "    if exact_match:\n",
        "        kb.stats[\"l1_hits\"] += 1\n",
        "        kb.stats[\"web_searches_avoided\"] += 1\n",
        "        return exact_match[\"result_summary\"], exact_match[\"result_urls\"], CacheDecision(\n",
        "            query=query, layer_reached=\"L1\", decision=\"HIT\",\n",
        "            confidence_score=1.0, action_taken=\"USE_CACHE\",\n",
        "            reasoning=\"Exact query match in cache\", timestamp=timestamp\n",
        "        )\n",
        "\n",
        "    aggressive_match = kb.lookup_query_aggressive(query)\n",
        "    if aggressive_match:\n",
        "        kb.stats[\"l1_hits\"] += 1\n",
        "        kb.stats[\"web_searches_avoided\"] += 1\n",
        "        return aggressive_match[\"result_summary\"], aggressive_match[\"result_urls\"], CacheDecision(\n",
        "            query=query, layer_reached=\"L1\", decision=\"HIT\",\n",
        "            confidence_score=0.95, action_taken=\"USE_CACHE\",\n",
        "            reasoning=\"Bag-of-words match in cache\", timestamp=timestamp\n",
        "        )\n",
        "\n",
        "    # === Layer 2: Semantic Retrieval ===\n",
        "    results = kb.semantic_search(query)\n",
        "    \n",
        "    if not results:\n",
        "        # No cached content, go to web search\n",
        "        summary, search_results, urls = search_web(query)\n",
        "        kb.stats[\"web_searches_executed\"] += 1\n",
        "        kb.stats[\"l2_low\"] += 1\n",
        "        \n",
        "        # Cache results\n",
        "        query_content = f\"Query: {query}\\n\\nAnswer: {summary}\\n\\nResults:\\n\" + \"\\n\\n\".join(search_results)\n",
        "        synthetic_url = f\"search://{kb.compute_content_hash(query)[:16]}\"\n",
        "        kb.add_document(synthetic_url, query_content, title=f\"Search: {query[:50]}\", source_query=query)\n",
        "        kb.add_query(query, urls, summary)\n",
        "        \n",
        "        return query_content, urls, CacheDecision(\n",
        "            query=query, layer_reached=\"L2\", decision=\"LOW_CONF\",\n",
        "            confidence_score=0.0, action_taken=\"SEARCH\",\n",
        "            reasoning=\"No cached content found\", timestamp=timestamp\n",
        "        )\n",
        "\n",
        "    confidence = compute_confidence(results, query)\n",
        "    high_thresh = CACHE_CONFIG[\"high_confidence_threshold\"]\n",
        "    low_thresh = CACHE_CONFIG[\"low_confidence_threshold\"]\n",
        "\n",
        "    if confidence >= high_thresh:\n",
        "        kb.stats[\"l2_high\"] += 1\n",
        "        kb.stats[\"web_searches_avoided\"] += 1\n",
        "        content = \"\\n\\n\".join([f\"[From: {c.source_url}]\\n{c.text}\" for c, _ in results[:3]])\n",
        "        urls = list(set([c.source_url for c, _ in results]))\n",
        "        return content, urls, CacheDecision(\n",
        "            query=query, layer_reached=\"L2\", decision=\"HIGH_CONF\",\n",
        "            confidence_score=confidence, action_taken=\"USE_CACHE\",\n",
        "            reasoning=f\"High semantic similarity ({confidence:.2f})\", timestamp=timestamp\n",
        "        )\n",
        "\n",
        "    elif confidence < low_thresh:\n",
        "        kb.stats[\"l2_low\"] += 1\n",
        "        summary, search_results, urls = search_web(query)\n",
        "        kb.stats[\"web_searches_executed\"] += 1\n",
        "        \n",
        "        query_content = f\"Query: {query}\\n\\nAnswer: {summary}\\n\\nResults:\\n\" + \"\\n\\n\".join(search_results)\n",
        "        synthetic_url = f\"search://{kb.compute_content_hash(query)[:16]}\"\n",
        "        kb.add_document(synthetic_url, query_content, title=f\"Search: {query[:50]}\", source_query=query)\n",
        "        kb.add_query(query, urls, summary)\n",
        "        \n",
        "        return query_content, urls, CacheDecision(\n",
        "            query=query, layer_reached=\"L2\", decision=\"LOW_CONF\",\n",
        "            confidence_score=confidence, action_taken=\"SEARCH\",\n",
        "            reasoning=f\"Low confidence ({confidence:.2f}), executed web search\", timestamp=timestamp\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        # === Layer 3: Medium confidence - use LLM judgment ===\n",
        "        kb.stats[\"l2_medium\"] += 1\n",
        "        \n",
        "        # For simplicity, we'll use the cached content + targeted search\n",
        "        cached_content = \"\\n\\n\".join([f\"[Cached: {c.source_url}]\\n{c.text}\" for c, _ in results[:2]])\n",
        "        cached_urls = [c.source_url for c, _ in results[:2]]\n",
        "        \n",
        "        # Execute supplementary search\n",
        "        summary, search_results, new_urls = search_web(query, max_results=4)\n",
        "        kb.stats[\"web_searches_executed\"] += 1\n",
        "        \n",
        "        gap_content = f\"Query: {query}\\n\\nAnswer: {summary}\\n\\nResults:\\n\" + \"\\n\\n\".join(search_results)\n",
        "        synthetic_url = f\"search://{kb.compute_content_hash(query)[:16]}\"\n",
        "        kb.add_document(synthetic_url, gap_content, title=f\"Search: {query[:50]}\", source_query=query)\n",
        "        kb.add_query(query, new_urls, summary)\n",
        "        \n",
        "        combined_content = cached_content + f\"\\n\\n[Supplementary search]\\n{gap_content}\"\n",
        "        combined_urls = list(set(cached_urls + new_urls))\n",
        "        \n",
        "        kb.stats[\"l3_partial\"] += 1\n",
        "        \n",
        "        return combined_content, combined_urls, CacheDecision(\n",
        "            query=query, layer_reached=\"L3\", decision=\"PARTIAL\",\n",
        "            confidence_score=confidence, action_taken=\"TARGETED_SEARCH\",\n",
        "            reasoning=f\"Medium confidence ({confidence:.2f}), combined cache + search\", timestamp=timestamp\n",
        "        )\n",
        "\n",
        "\n",
        "def get_leaf_nodes(skeleton: Dict[str, Any]) -> List[str]:\n",
        "    \"\"\"Get all leaf node IDs in document order.\"\"\"\n",
        "    nodes = skeleton.get(\"nodes\", {})\n",
        "    root_nodes = skeleton.get(\"root_nodes\", [])\n",
        "\n",
        "    def collect_leaves(node_ids: List[str]) -> List[str]:\n",
        "        leaves = []\n",
        "        for nid in node_ids:\n",
        "            node = nodes.get(nid, {})\n",
        "            children = node.get(\"children\", [])\n",
        "            if not children:\n",
        "                leaves.append(nid)\n",
        "            else:\n",
        "                leaves.extend(collect_leaves(children))\n",
        "        return leaves\n",
        "\n",
        "    return collect_leaves(root_nodes)\n",
        "\n",
        "\n",
        "def topological_sort_nodes(skeleton: Dict[str, Any], node_ids: List[str]) -> List[str]:\n",
        "    \"\"\"Sort nodes by dependency order.\"\"\"\n",
        "    nodes = skeleton.get(\"nodes\", {})\n",
        "    node_id_set = set(node_ids)\n",
        "    remaining = set(node_ids)\n",
        "    sorted_nodes = []\n",
        "\n",
        "    while remaining:\n",
        "        ready = []\n",
        "        for nid in remaining:\n",
        "            node = nodes.get(nid, {})\n",
        "            deps = set(node.get(\"dependencies\", []))\n",
        "            internal_deps = deps & node_id_set\n",
        "            if internal_deps.issubset(set(sorted_nodes)):\n",
        "                ready.append(nid)\n",
        "\n",
        "        if not ready:\n",
        "            # Circular dependency - add remaining in document order\n",
        "            for nid in node_ids:\n",
        "                if nid in remaining:\n",
        "                    sorted_nodes.append(nid)\n",
        "            break\n",
        "\n",
        "        ready_ordered = [nid for nid in node_ids if nid in ready]\n",
        "        sorted_nodes.extend(ready_ordered)\n",
        "        remaining -= set(ready_ordered)\n",
        "\n",
        "    return sorted_nodes\n",
        "\n",
        "\n",
        "def get_adjacent_nodes(skeleton: Dict[str, Any], node_id: str) -> Tuple[Optional[str], Optional[str]]:\n",
        "    \"\"\"Get the previous and next node IDs in document order.\"\"\"\n",
        "    leaves = get_leaf_nodes(skeleton)\n",
        "    try:\n",
        "        idx = leaves.index(node_id)\n",
        "        prev_node = leaves[idx - 1] if idx > 0 else None\n",
        "        next_node = leaves[idx + 1] if idx < len(leaves) - 1 else None\n",
        "        return prev_node, next_node\n",
        "    except ValueError:\n",
        "        return None, None\n",
        "\n",
        "\n",
        "print(\"Helper functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Unified State Schema\n",
        "\n",
        "The SFEO state combines all phase requirements into a single TypedDict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SFEOState(TypedDict):\n",
        "    \"\"\"Complete state for the SFEO architecture.\"\"\"\n",
        "    \n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    # INPUT\n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    question: str\n",
        "    \n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    # PHASE A: STRATEGIC PLANNING\n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    preliminary_findings: List[str]  # Initial research results\n",
        "    skeleton: Dict[str, Any]  # DocumentSkeleton as dict\n",
        "    claims_registry: Dict[str, Dict[str, Any]]  # claim_id → Claim as dict\n",
        "    research_backlog: List[Dict[str, Any]]  # List of ResearchTask as dicts\n",
        "    \n",
        "    # Gate 1 tracking\n",
        "    gate_1_attempts: int\n",
        "    gate_1_passed: bool\n",
        "    \n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    # PHASE B: EVIDENCE GATHERING\n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    current_sprint: int\n",
        "    max_sprints: int\n",
        "    sprint_findings: Annotated[List[str], operator.add]  # Accumulates\n",
        "    \n",
        "    # Cache decisions for observability\n",
        "    cache_decisions: Annotated[List[Dict], operator.add]\n",
        "    \n",
        "    # Evidence tracking (accumulated)\n",
        "    source_urls: Annotated[List[str], operator.add]\n",
        "    evidence_map: Dict[str, List[str]]  # claim_id → [evidence snippets]\n",
        "    \n",
        "    # Retrospective notes\n",
        "    retrospective_notes: Annotated[List[str], operator.add]\n",
        "    should_stop_sprinting: bool\n",
        "    \n",
        "    # Gate 2 tracking\n",
        "    gate_2_attempts: int\n",
        "    gate_2_passed: bool\n",
        "    \n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    # PHASE C: DOCUMENT CONSTRUCTION\n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    prose_store: Dict[str, Dict[str, Any]]  # node_id → ProseEntry as dict\n",
        "    assembled_draft: str\n",
        "    \n",
        "    # Gate 3 tracking\n",
        "    gate_3_passed: bool\n",
        "    gate_3_scores: Dict[str, float]\n",
        "    \n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    # PHASE D: REFINEMENT\n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    iteration_count: int\n",
        "    noise_map: List[Dict[str, Any]]  # List of CritiqueIssue as dicts\n",
        "    nodes_to_patch: List[str]\n",
        "    quality_scores: Annotated[List[float], operator.add]\n",
        "    \n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    # OUTPUT\n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    final_report: str\n",
        "\n",
        "\n",
        "# Initialize global knowledge base (will be reset per session)\n",
        "knowledge_base: KnowledgeBase = None\n",
        "\n",
        "print(\"SFEOState schema defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Prompts Library\n",
        "\n",
        "Consolidated prompts for all SFEO phases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# PHASE A PROMPTS: STRATEGIC PLANNING\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "SKELETON_GENERATION_PROMPT = \"\"\"You are a research document architect designing a comprehensive report structure.\n",
        "\n",
        "RESEARCH QUESTION: {question}\n",
        "\n",
        "PRELIMINARY RESEARCH FINDINGS:\n",
        "{preliminary_findings}\n",
        "\n",
        "Create a hierarchical document skeleton with:\n",
        "1. A clear thesis statement (one sentence capturing the central argument)\n",
        "2. 5-7 main sections appropriate for a comprehensive research report\n",
        "3. Each section should have:\n",
        "   - node_id: Unique identifier (e.g., \"sec:intro\", \"sec:background\", \"sec:analysis\")\n",
        "   - title: Descriptive section title\n",
        "   - intent: 1-3 sentences describing what this section should accomplish\n",
        "   - target_word_count: 250-400 words per section\n",
        "   - dependencies: List of node_ids this section builds upon (empty for first sections)\n",
        "   - claim_placeholders: 2-3 specific claims this section must support (make these searchable)\n",
        "\n",
        "REQUIRED SECTIONS (adapt titles to topic):\n",
        "1. Introduction - Present the topic, context, and thesis\n",
        "2. Background/Context - Provide necessary foundation knowledge  \n",
        "3. Main Body (2-4 sections) - Cover key aspects in depth\n",
        "4. Analysis/Discussion - Synthesize findings and discuss implications\n",
        "5. Conclusion - Summarize key points and future directions\n",
        "\n",
        "IMPORTANT:\n",
        "- Node IDs must be unique and follow format: sec:topic\n",
        "- Dependencies must reference existing node_ids (no forward references)\n",
        "- claim_placeholders should be specific and searchable, not vague\n",
        "\"\"\"\n",
        "\n",
        "CLAIM_EXTRACTION_PROMPT = \"\"\"Extract all verifiable claims from this skeleton's claim_placeholders.\n",
        "\n",
        "SKELETON:\n",
        "{skeleton_json}\n",
        "\n",
        "For each claim placeholder across all sections, create a claim entry with:\n",
        "- claim_id: Unique ID using format claim_{{node_id}}_{{number}}\n",
        "- claim_text: The specific assertion to verify\n",
        "- source_node: The section node_id\n",
        "- verification_status: \"unverified\"\n",
        "\n",
        "Focus on claims that are:\n",
        "- Specific and factual (not opinions)\n",
        "- Searchable (can find evidence online)\n",
        "- Central to answering the research question\n",
        "\"\"\"\n",
        "\n",
        "GATE_1_EVALUATION_PROMPT = \"\"\"Evaluate this document skeleton for quality.\n",
        "\n",
        "RESEARCH QUESTION: {question}\n",
        "\n",
        "SKELETON:\n",
        "Thesis: {thesis}\n",
        "Sections: {sections_summary}\n",
        "\n",
        "Evaluate on these criteria (score 1-10 each):\n",
        "\n",
        "1. THESIS_CLARITY: Is the thesis clear, specific, and arguable?\n",
        "2. COVERAGE: Does the skeleton address ALL parts of the research question?\n",
        "3. STRUCTURE: Is the hierarchy logical? Do dependencies make sense?\n",
        "4. CLAIM_SPECIFICITY: Are claim_placeholders specific enough to research?\n",
        "\n",
        "Format response as:\n",
        "THESIS_CLARITY: [score]\n",
        "COVERAGE: [score]  \n",
        "STRUCTURE: [score]\n",
        "CLAIM_SPECIFICITY: [score]\n",
        "OVERALL: [average score]\n",
        "ISSUES: [List any problems, or \"None\"]\n",
        "\"\"\"\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# PHASE B PROMPTS: EVIDENCE GATHERING\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "SPRINT_PLANNING_PROMPT = \"\"\"Select claims to verify in this research sprint.\n",
        "\n",
        "REMAINING UNVERIFIED CLAIMS:\n",
        "{unverified_claims}\n",
        "\n",
        "ALREADY VERIFIED CLAIMS:\n",
        "{verified_claims}\n",
        "\n",
        "Select up to {claims_per_sprint} claims to verify in this sprint.\n",
        "Prioritize claims that are:\n",
        "1. Central to the thesis\n",
        "2. Quantitative or easily verifiable\n",
        "3. Related to each other (for cache efficiency)\n",
        "\n",
        "For each selected claim, provide a search query.\n",
        "\n",
        "Format as JSON array:\n",
        "[\n",
        "  {{\"claim_id\": \"...\", \"query\": \"specific search query\"}}\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "SPRINT_RETROSPECTIVE_PROMPT = \"\"\"Conduct a sprint retrospective for research progress.\n",
        "\n",
        "ORIGINAL QUESTION: {question}\n",
        "\n",
        "SPRINT {sprint_num} of {max_sprints} COMPLETED.\n",
        "\n",
        "CLAIMS VERIFIED THIS SPRINT:\n",
        "{verified_this_sprint}\n",
        "\n",
        "REMAINING UNVERIFIED:\n",
        "{remaining_unverified}\n",
        "\n",
        "CACHE PERFORMANCE:\n",
        "- Queries: {total_queries}\n",
        "- Cache hits: {cache_hits} ({hit_rate:.1f}%)\n",
        "\n",
        "Analyze and provide:\n",
        "\n",
        "## LEARNINGS\n",
        "What key insights did we gain?\n",
        "\n",
        "## GAPS  \n",
        "What is still unclear or needs investigation?\n",
        "\n",
        "## CONTINUE\n",
        "Should we continue with another sprint? YES or NO with justification.\n",
        "Consider: If most claims are verified or cache hit rate is high, we may stop.\n",
        "\n",
        "## PRIORITY_CLAIMS\n",
        "If continuing, list the 2-4 most important remaining claims to verify:\n",
        "\"\"\"\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# PHASE C PROMPTS: DOCUMENT CONSTRUCTION\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "PROSE_GENERATION_PROMPT = \"\"\"Generate prose content for a specific section of a research report.\n",
        "\n",
        "SECTION CONTEXT:\n",
        "- Node ID: {node_id}\n",
        "- Title: {title}\n",
        "- Intent: {intent}\n",
        "- Target Word Count: {target_words}\n",
        "\n",
        "CLAIMS TO SUPPORT (with evidence):\n",
        "{claims_with_evidence}\n",
        "\n",
        "CONTEXT FROM ADJACENT SECTIONS:\n",
        "- Previous Section Summary: {prev_summary}\n",
        "- Next Section Preview: {next_preview}\n",
        "\n",
        "WRITING REQUIREMENTS:\n",
        "1. Every factual statement must reference the provided evidence\n",
        "2. Use inline citations: [Source: URL]\n",
        "3. Create smooth transitions (bridge_in from previous, bridge_out to next)\n",
        "4. Include a 1-2 sentence summary of what this section establishes\n",
        "\n",
        "Output as JSON:\n",
        "{{\n",
        "  \"bridge_in\": \"1-2 transitional sentences from previous section\",\n",
        "  \"main_content\": \"The main prose (~{target_words} words)\",\n",
        "  \"bridge_out\": \"1-2 transitional sentences to next section\",\n",
        "  \"summary\": \"What this section establishes\",\n",
        "  \"citations_used\": [\"url1\", \"url2\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "GATE_3_EVALUATION_PROMPT = \"\"\"Evaluate this assembled draft for quality.\n",
        "\n",
        "ORIGINAL QUESTION: {question}\n",
        "\n",
        "DRAFT:\n",
        "{draft}\n",
        "\n",
        "Evaluate on these criteria (score 1-10 each):\n",
        "\n",
        "1. COHERENCE: Does the document flow logically from section to section?\n",
        "2. DEPTH: Does the analysis go beyond surface-level information?\n",
        "3. EVIDENCE: Are claims well-supported with citations?\n",
        "4. COMPLETENESS: Does it fully answer the research question?\n",
        "\n",
        "Format response as:\n",
        "COHERENCE: [score]\n",
        "DEPTH: [score]\n",
        "EVIDENCE: [score]\n",
        "COMPLETENESS: [score]\n",
        "OVERALL: [average]\n",
        "ISSUES: [List problems needing attention, or \"None\"]\n",
        "\"\"\"\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# PHASE D PROMPTS: REFINEMENT\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "STRUCTURED_CRITIQUE_PROMPT = \"\"\"Conduct a rigorous review of this research report.\n",
        "\n",
        "ORIGINAL QUESTION: {question}\n",
        "\n",
        "DOCUMENT SKELETON:\n",
        "{skeleton_summary}\n",
        "\n",
        "CLAIMS REGISTRY:\n",
        "{claims_summary}\n",
        "\n",
        "ASSEMBLED DRAFT:\n",
        "{draft}\n",
        "\n",
        "Analyze at THREE levels and produce a Noise Map:\n",
        "\n",
        "1. GLOBAL ISSUES: Thesis support, overall coherence, terminology consistency\n",
        "2. SECTION ISSUES: Weak claims, missing evidence, logical gaps, depth\n",
        "3. TRANSITION ISSUES: Abrupt shifts, weak bridges, broken references\n",
        "\n",
        "For EACH issue, provide:\n",
        "- issue_id: Unique ID (e.g., \"I001\")\n",
        "- scope: \"global\", \"section\", or \"transition\"\n",
        "- target_nodes: List of affected node IDs\n",
        "- issue_type: weak_claim, missing_evidence, logical_gap, unclear, coherence, depth, transition\n",
        "- severity: \"critical\", \"major\", or \"minor\"\n",
        "- description: What the problem is\n",
        "- suggestion: How to fix it\n",
        "- search_query: If evidence-related, a query to find supporting sources\n",
        "\n",
        "SCORING (1-10):\n",
        "- 9-10: Publication ready\n",
        "- 7-8: Good, minor issues only\n",
        "- 5-6: Acceptable, needs improvement\n",
        "- 3-4: Significant problems\n",
        "- 1-2: Major rework needed\n",
        "\n",
        "Output as JSON with overall_quality score and issues array.\n",
        "\"\"\"\n",
        "\n",
        "PATCH_APPLICATION_PROMPT = \"\"\"Revise this section based on critique feedback and new evidence.\n",
        "\n",
        "SECTION TO REVISE:\n",
        "- Node ID: {node_id}\n",
        "- Title: {title}\n",
        "- Intent: {intent}\n",
        "\n",
        "CURRENT CONTENT:\n",
        "{current_content}\n",
        "\n",
        "ISSUES TO ADDRESS:\n",
        "{issues}\n",
        "\n",
        "NEW EVIDENCE (incorporate with citations):\n",
        "{new_evidence}\n",
        "\n",
        "ADJACENT CONTEXT:\n",
        "- Previous section ends: {prev_bridge_out}\n",
        "- Next section starts: {next_bridge_in}\n",
        "\n",
        "REVISION REQUIREMENTS:\n",
        "1. Address ALL identified issues\n",
        "2. Incorporate new evidence with [Source: URL] citations\n",
        "3. Maintain smooth transitions with adjacent sections\n",
        "4. Keep approximately the same length\n",
        "\n",
        "Output revised section as JSON with bridge_in, main_content, bridge_out, summary.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Prompts library defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Phase A: Strategic Planning\n",
        "\n",
        "Phase A establishes the document \"contract\":\n",
        "1. Preliminary research to inform skeleton\n",
        "2. Generate document skeleton with semantic nodes\n",
        "3. Extract verifiable claims from skeleton\n",
        "4. Create research backlog from claims\n",
        "5. Gate 1: Validate skeleton quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def preliminary_research(state: SFEOState) -> dict:\n",
        "    \"\"\"Conduct lightweight initial research to inform skeleton generation.\"\"\"\n",
        "    question = state[\"question\"]\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Phase A.1: Preliminary Research\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Generate 3-4 broad search queries\n",
        "    query_prompt = f\"\"\"Generate 4 search queries to understand the scope of this research question:\n",
        "    \n",
        "Question: {question}\n",
        "\n",
        "Return 4 queries, one per line, covering different aspects.\"\"\"\n",
        "    \n",
        "    response = await llm.ainvoke([HumanMessage(content=query_prompt)])\n",
        "    queries = [q.strip() for q in response.content.split(\"\\n\") if q.strip()][:4]\n",
        "    \n",
        "    all_results = []\n",
        "    all_urls = []\n",
        "    \n",
        "    for query in queries:\n",
        "        print(f\"  Searching: {query[:50]}...\")\n",
        "        summary, results, urls = search_web(query, max_results=5)\n",
        "        all_results.extend(results)\n",
        "        all_urls.extend(urls)\n",
        "    \n",
        "    print(f\"  Collected {len(all_results)} preliminary results\")\n",
        "    \n",
        "    return {\n",
        "        \"preliminary_findings\": all_results,\n",
        "        \"source_urls\": all_urls,\n",
        "        \"gate_1_attempts\": 0,\n",
        "        \"gate_1_passed\": False\n",
        "    }\n",
        "\n",
        "\n",
        "async def generate_skeleton(state: SFEOState) -> dict:\n",
        "    \"\"\"Generate the document skeleton structure.\"\"\"\n",
        "    question = state[\"question\"]\n",
        "    preliminary_findings = state.get(\"preliminary_findings\", [])\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Phase A.2: Skeleton Generation\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    findings_summary = \"\\n\".join(preliminary_findings[:15])\n",
        "    \n",
        "    prompt = SKELETON_GENERATION_PROMPT.format(\n",
        "        question=question,\n",
        "        preliminary_findings=findings_summary\n",
        "    )\n",
        "    \n",
        "    structured_llm = llm.with_structured_output(SkeletonGenerationOutput)\n",
        "    result = await structured_llm.ainvoke([HumanMessage(content=prompt)])\n",
        "    \n",
        "    # Build skeleton dictionary\n",
        "    skeleton = {\n",
        "        \"thesis\": result.thesis,\n",
        "        \"root_nodes\": [],\n",
        "        \"nodes\": {},\n",
        "        \"style_constraints\": \"Academic tone, comprehensive analysis, evidence-based claims\"\n",
        "    }\n",
        "    \n",
        "    child_ids = set()\n",
        "    for section in result.sections:\n",
        "        child_ids.update(section.children)\n",
        "    \n",
        "    for section in result.sections:\n",
        "        skeleton[\"nodes\"][section.node_id] = section.model_dump()\n",
        "        if section.node_id not in child_ids:\n",
        "            skeleton[\"root_nodes\"].append(section.node_id)\n",
        "    \n",
        "    if not skeleton[\"root_nodes\"]:\n",
        "        for nid, node in skeleton[\"nodes\"].items():\n",
        "            if not node.get(\"dependencies\", []):\n",
        "                skeleton[\"root_nodes\"].append(nid)\n",
        "    \n",
        "    if not skeleton[\"root_nodes\"]:\n",
        "        skeleton[\"root_nodes\"] = list(skeleton[\"nodes\"].keys())\n",
        "    \n",
        "    print(f\"  Thesis: {result.thesis[:80]}...\")\n",
        "    print(f\"  Generated {len(skeleton['nodes'])} skeleton nodes:\")\n",
        "    for nid, node in skeleton[\"nodes\"].items():\n",
        "        deps = node.get('dependencies', [])\n",
        "        dep_str = f\" (depends: {', '.join(deps)})\" if deps else \"\"\n",
        "        print(f\"    - {nid}: {node['title']}{dep_str}\")\n",
        "    \n",
        "    return {\"skeleton\": skeleton}\n",
        "\n",
        "\n",
        "async def identify_claims(state: SFEOState) -> dict:\n",
        "    \"\"\"Extract verifiable claims from the skeleton's claim_placeholders.\"\"\"\n",
        "    skeleton = state[\"skeleton\"]\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Phase A.3: Claim Identification\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    claims_registry = {}\n",
        "    claim_counter = 1\n",
        "    \n",
        "    for node_id, node in skeleton[\"nodes\"].items():\n",
        "        placeholders = node.get(\"claim_placeholders\", [])\n",
        "        for placeholder in placeholders:\n",
        "            claim_id = f\"claim_{node_id}_{claim_counter}\"\n",
        "            claims_registry[claim_id] = {\n",
        "                \"claim_id\": claim_id,\n",
        "                \"claim_text\": placeholder,\n",
        "                \"source_node\": node_id,\n",
        "                \"verification_status\": \"unverified\",\n",
        "                \"supporting_evidence\": [],\n",
        "                \"claim_dependencies\": []\n",
        "            }\n",
        "            claim_counter += 1\n",
        "    \n",
        "    print(f\"  Extracted {len(claims_registry)} claims from skeleton:\")\n",
        "    for cid, claim in list(claims_registry.items())[:5]:\n",
        "        print(f\"    - {cid}: {claim['claim_text'][:50]}...\")\n",
        "    if len(claims_registry) > 5:\n",
        "        print(f\"    ... and {len(claims_registry) - 5} more\")\n",
        "    \n",
        "    return {\"claims_registry\": claims_registry}\n",
        "\n",
        "\n",
        "async def create_research_backlog(state: SFEOState) -> dict:\n",
        "    \"\"\"Create prioritized research backlog from claims.\"\"\"\n",
        "    claims_registry = state[\"claims_registry\"]\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Phase A.4: Research Backlog Creation\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    backlog = []\n",
        "    priority = 1\n",
        "    \n",
        "    for claim_id, claim in claims_registry.items():\n",
        "        # Generate search query from claim text\n",
        "        query = claim[\"claim_text\"]\n",
        "        if len(query) > 100:\n",
        "            query = query[:100]\n",
        "        \n",
        "        task = {\n",
        "            \"task_id\": f\"task_{claim_id}\",\n",
        "            \"claim_id\": claim_id,\n",
        "            \"query\": query,\n",
        "            \"priority\": priority,\n",
        "            \"status\": \"pending\",\n",
        "            \"evidence_found\": []\n",
        "        }\n",
        "        backlog.append(task)\n",
        "        priority += 1\n",
        "    \n",
        "    print(f\"  Created backlog with {len(backlog)} research tasks\")\n",
        "    \n",
        "    return {\n",
        "        \"research_backlog\": backlog,\n",
        "        \"evidence_map\": {},\n",
        "        \"current_sprint\": 1,\n",
        "        \"max_sprints\": SPRINT_CONFIG[\"max_sprints\"]\n",
        "    }\n",
        "\n",
        "\n",
        "async def skeleton_quality_gate(state: SFEOState) -> dict:\n",
        "    \"\"\"Gate 1: Evaluate skeleton quality.\"\"\"\n",
        "    skeleton = state[\"skeleton\"]\n",
        "    question = state[\"question\"]\n",
        "    attempts = state.get(\"gate_1_attempts\", 0) + 1\n",
        "    \n",
        "    print(f\"\\n--- Gate 1: Skeleton Quality (Attempt {attempts}/{GATE_1_CONFIG['max_attempts'] + 1}) ---\")\n",
        "    \n",
        "    # Build sections summary\n",
        "    sections_summary = \"\\n\".join([\n",
        "        f\"- {nid}: {node['title']} - {node.get('intent', '')[:60]}...\"\n",
        "        for nid, node in skeleton[\"nodes\"].items()\n",
        "    ])\n",
        "    \n",
        "    prompt = GATE_1_EVALUATION_PROMPT.format(\n",
        "        question=question,\n",
        "        thesis=skeleton.get(\"thesis\", \"\"),\n",
        "        sections_summary=sections_summary\n",
        "    )\n",
        "    \n",
        "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
        "    content = response.content\n",
        "    \n",
        "    # Parse scores\n",
        "    scores = {}\n",
        "    for metric in [\"THESIS_CLARITY\", \"COVERAGE\", \"STRUCTURE\", \"CLAIM_SPECIFICITY\", \"OVERALL\"]:\n",
        "        match = re.search(rf'{metric}:\\s*(\\d+(?:\\.\\d+)?)', content)\n",
        "        if match:\n",
        "            scores[metric.lower()] = float(match.group(1))\n",
        "    \n",
        "    overall = scores.get(\"overall\", 7.0)\n",
        "    \n",
        "    # Check if passes\n",
        "    passed = (\n",
        "        scores.get(\"thesis_clarity\", 0) >= GATE_1_CONFIG[\"thesis_clarity_threshold\"] and\n",
        "        scores.get(\"structure\", 0) >= GATE_1_CONFIG[\"structure_threshold\"] and\n",
        "        overall >= 6.5\n",
        "    )\n",
        "    \n",
        "    print(f\"  Thesis Clarity: {scores.get('thesis_clarity', 'N/A')}/10\")\n",
        "    print(f\"  Coverage: {scores.get('coverage', 'N/A')}/10\")\n",
        "    print(f\"  Structure: {scores.get('structure', 'N/A')}/10\")\n",
        "    print(f\"  Overall: {overall}/10\")\n",
        "    print(f\"  {'PASSED' if passed else 'FAILED'} Gate 1\")\n",
        "    \n",
        "    return {\n",
        "        \"gate_1_attempts\": attempts,\n",
        "        \"gate_1_passed\": passed\n",
        "    }\n",
        "\n",
        "\n",
        "def route_after_gate_1(state: SFEOState) -> Literal[\"initialize_cache\", \"refine_skeleton\", \"initialize_cache_anyway\"]:\n",
        "    \"\"\"Route based on Gate 1 result.\"\"\"\n",
        "    passed = state.get(\"gate_1_passed\", False)\n",
        "    attempts = state.get(\"gate_1_attempts\", 0)\n",
        "    \n",
        "    if passed:\n",
        "        return \"initialize_cache\"\n",
        "    elif attempts <= GATE_1_CONFIG[\"max_attempts\"]:\n",
        "        return \"refine_skeleton\"\n",
        "    else:\n",
        "        print(\"  Max Gate 1 attempts reached. Proceeding anyway.\")\n",
        "        return \"initialize_cache_anyway\"\n",
        "\n",
        "\n",
        "async def refine_skeleton(state: SFEOState) -> dict:\n",
        "    \"\"\"Refine skeleton after Gate 1 failure.\"\"\"\n",
        "    skeleton = state[\"skeleton\"]\n",
        "    question = state[\"question\"]\n",
        "    \n",
        "    print(f\"\\n  Refining skeleton...\")\n",
        "    \n",
        "    # Simple refinement: regenerate with emphasis on issues\n",
        "    prompt = f\"\"\"The previous skeleton for this question had quality issues. Generate an improved version.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Previous thesis: {skeleton.get('thesis', '')}\n",
        "\n",
        "Improve:\n",
        "1. Make the thesis more specific and arguable\n",
        "2. Ensure all question aspects are covered\n",
        "3. Make claim_placeholders more specific and searchable\n",
        "\"\"\"\n",
        "    \n",
        "    structured_llm = llm.with_structured_output(SkeletonGenerationOutput)\n",
        "    result = await structured_llm.ainvoke([HumanMessage(content=prompt)])\n",
        "    \n",
        "    # Rebuild skeleton\n",
        "    new_skeleton = {\n",
        "        \"thesis\": result.thesis,\n",
        "        \"root_nodes\": [],\n",
        "        \"nodes\": {},\n",
        "        \"style_constraints\": skeleton.get(\"style_constraints\", \"\")\n",
        "    }\n",
        "    \n",
        "    for section in result.sections:\n",
        "        new_skeleton[\"nodes\"][section.node_id] = section.model_dump()\n",
        "        new_skeleton[\"root_nodes\"].append(section.node_id)\n",
        "    \n",
        "    print(f\"  Refined skeleton with {len(new_skeleton['nodes'])} sections\")\n",
        "    \n",
        "    return {\"skeleton\": new_skeleton}\n",
        "\n",
        "\n",
        "print(\"Phase A node functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Phase B: Evidence Gathering\n",
        "\n",
        "Phase B executes sprint-based evidence collection with cache integration:\n",
        "1. Initialize session knowledge base\n",
        "2. Sprint loop: plan → cache-aware search → update claims → retrospective\n",
        "3. Gate 2: Validate evidence sufficiency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def initialize_cache(state: SFEOState) -> dict:\n",
        "    \"\"\"Initialize the session knowledge base.\"\"\"\n",
        "    global knowledge_base\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Phase B.0: Initialize Knowledge Cache\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    knowledge_base = KnowledgeBase()\n",
        "    \n",
        "    # Pre-populate cache with preliminary findings\n",
        "    preliminary = state.get(\"preliminary_findings\", [])\n",
        "    if preliminary:\n",
        "        content = \"\\n\\n\".join(preliminary)\n",
        "        knowledge_base.add_document(\n",
        "            \"preliminary://research\",\n",
        "            content,\n",
        "            title=\"Preliminary Research\",\n",
        "            source_query=state[\"question\"]\n",
        "        )\n",
        "        print(f\"  Pre-populated cache with {len(preliminary)} preliminary findings\")\n",
        "    \n",
        "    print(f\"  Knowledge base initialized\")\n",
        "    \n",
        "    return {\n",
        "        \"should_stop_sprinting\": False,\n",
        "        \"gate_2_attempts\": 0,\n",
        "        \"gate_2_passed\": False\n",
        "    }\n",
        "\n",
        "\n",
        "async def sprint_execute(state: SFEOState) -> dict:\n",
        "    \"\"\"Execute a research sprint with cache-aware searching.\"\"\"\n",
        "    global knowledge_base\n",
        "    \n",
        "    current_sprint = state.get(\"current_sprint\", 1)\n",
        "    max_sprints = state.get(\"max_sprints\", SPRINT_CONFIG[\"max_sprints\"])\n",
        "    claims_registry = state.get(\"claims_registry\", {})\n",
        "    evidence_map = state.get(\"evidence_map\", {})\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Phase B.1: Sprint {current_sprint}/{max_sprints}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Get unverified claims for this sprint\n",
        "    unverified = [\n",
        "        (cid, claim) for cid, claim in claims_registry.items()\n",
        "        if claim.get(\"verification_status\") == \"unverified\"\n",
        "    ]\n",
        "    \n",
        "    claims_to_verify = unverified[:SPRINT_CONFIG[\"claims_per_sprint\"]]\n",
        "    \n",
        "    if not claims_to_verify:\n",
        "        print(\"  No unverified claims remaining\")\n",
        "        return {\n",
        "            \"should_stop_sprinting\": True,\n",
        "            \"sprint_findings\": [\"No unverified claims remaining in sprint\"]\n",
        "        }\n",
        "    \n",
        "    print(f\"  Verifying {len(claims_to_verify)} claims this sprint\")\n",
        "    \n",
        "    findings = []\n",
        "    all_urls = []\n",
        "    cache_decisions = []\n",
        "    verified_count = 0\n",
        "    cache_hits = 0\n",
        "    \n",
        "    for claim_id, claim in claims_to_verify:\n",
        "        query = claim[\"claim_text\"][:150]  # Truncate for search\n",
        "        \n",
        "        print(f\"\\n  [{claim_id}] {query[:50]}...\")\n",
        "        \n",
        "        # Execute cache-aware search\n",
        "        content, urls, decision = await cascaded_search(query, knowledge_base)\n",
        "        \n",
        "        cache_decisions.append(decision.model_dump())\n",
        "        all_urls.extend(urls)\n",
        "        \n",
        "        if decision.action_taken == \"USE_CACHE\":\n",
        "            cache_hits += 1\n",
        "            print(f\"    CACHE HIT ({decision.layer_reached})\")\n",
        "        else:\n",
        "            print(f\"    WEB SEARCH ({decision.layer_reached})\")\n",
        "        \n",
        "        # Update claim verification\n",
        "        claims_registry[claim_id][\"verification_status\"] = \"verified\"\n",
        "        claims_registry[claim_id][\"supporting_evidence\"].append(content[:500])\n",
        "        verified_count += 1\n",
        "        \n",
        "        # Update evidence map\n",
        "        if claim_id not in evidence_map:\n",
        "            evidence_map[claim_id] = []\n",
        "        evidence_map[claim_id].append(content[:1000])\n",
        "        \n",
        "        findings.append(f\"[{claim_id}] {claim['claim_text'][:80]}... - VERIFIED with {len(urls)} sources\")\n",
        "    \n",
        "    hit_rate = cache_hits / len(claims_to_verify) * 100 if claims_to_verify else 0\n",
        "    \n",
        "    print(f\"\\n  Sprint {current_sprint} Complete:\")\n",
        "    print(f\"    Claims verified: {verified_count}\")\n",
        "    print(f\"    Cache hits: {cache_hits}/{len(claims_to_verify)} ({hit_rate:.1f}%)\")\n",
        "    print(f\"    New URLs collected: {len(all_urls)}\")\n",
        "    \n",
        "    finding_summary = f\"## Sprint {current_sprint} Findings\\n\\n\" + \"\\n\".join(findings)\n",
        "    \n",
        "    return {\n",
        "        \"claims_registry\": claims_registry,\n",
        "        \"evidence_map\": evidence_map,\n",
        "        \"sprint_findings\": [finding_summary],\n",
        "        \"source_urls\": all_urls,\n",
        "        \"cache_decisions\": cache_decisions\n",
        "    }\n",
        "\n",
        "\n",
        "async def sprint_retrospective(state: SFEOState) -> dict:\n",
        "    \"\"\"Conduct retrospective after sprint to decide continuation.\"\"\"\n",
        "    global knowledge_base\n",
        "    \n",
        "    current_sprint = state.get(\"current_sprint\", 1)\n",
        "    max_sprints = state.get(\"max_sprints\", SPRINT_CONFIG[\"max_sprints\"])\n",
        "    claims_registry = state.get(\"claims_registry\", {})\n",
        "    question = state[\"question\"]\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Phase B.2: Sprint {current_sprint} Retrospective\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Count verified vs unverified\n",
        "    verified = [c for c in claims_registry.values() if c.get(\"verification_status\") == \"verified\"]\n",
        "    unverified = [c for c in claims_registry.values() if c.get(\"verification_status\") == \"unverified\"]\n",
        "    \n",
        "    total_queries = knowledge_base.stats[\"total_queries\"]\n",
        "    cache_hits = knowledge_base.stats[\"web_searches_avoided\"]\n",
        "    hit_rate = cache_hits / total_queries * 100 if total_queries else 0\n",
        "    \n",
        "    print(f\"  Claims: {len(verified)} verified, {len(unverified)} remaining\")\n",
        "    print(f\"  Cache: {cache_hits}/{total_queries} hits ({hit_rate:.1f}%)\")\n",
        "    \n",
        "    # Decide whether to continue\n",
        "    should_stop = False\n",
        "    reason = \"\"\n",
        "    \n",
        "    if current_sprint >= max_sprints:\n",
        "        should_stop = True\n",
        "        reason = \"Max sprints reached\"\n",
        "    elif len(unverified) == 0:\n",
        "        should_stop = True\n",
        "        reason = \"All claims verified\"\n",
        "    elif len(verified) >= len(claims_registry) * 0.8:  # 80% verified\n",
        "        should_stop = True\n",
        "        reason = \"Sufficient coverage (80%+ verified)\"\n",
        "    \n",
        "    if should_stop:\n",
        "        print(f\"  Decision: STOP ({reason})\")\n",
        "    else:\n",
        "        print(f\"  Decision: CONTINUE to sprint {current_sprint + 1}\")\n",
        "    \n",
        "    retro_note = f\"### Sprint {current_sprint} Retrospective\\n\"\n",
        "    retro_note += f\"- Verified: {len(verified)}/{len(claims_registry)}\\n\"\n",
        "    retro_note += f\"- Cache hit rate: {hit_rate:.1f}%\\n\"\n",
        "    retro_note += f\"- Decision: {'STOP' if should_stop else 'CONTINUE'}\"\n",
        "    \n",
        "    return {\n",
        "        \"current_sprint\": current_sprint + 1,\n",
        "        \"should_stop_sprinting\": should_stop,\n",
        "        \"retrospective_notes\": [retro_note]\n",
        "    }\n",
        "\n",
        "\n",
        "def route_after_sprint(state: SFEOState) -> Literal[\"sprint_execute\", \"evidence_quality_gate\"]:\n",
        "    \"\"\"Route based on sprint retrospective decision.\"\"\"\n",
        "    should_stop = state.get(\"should_stop_sprinting\", False)\n",
        "    \n",
        "    if should_stop:\n",
        "        return \"evidence_quality_gate\"\n",
        "    else:\n",
        "        return \"sprint_execute\"\n",
        "\n",
        "\n",
        "async def evidence_quality_gate(state: SFEOState) -> dict:\n",
        "    \"\"\"Gate 2: Validate evidence sufficiency.\"\"\"\n",
        "    claims_registry = state.get(\"claims_registry\", {})\n",
        "    source_urls = state.get(\"source_urls\", [])\n",
        "    attempts = state.get(\"gate_2_attempts\", 0) + 1\n",
        "    skeleton = state.get(\"skeleton\", {})\n",
        "    \n",
        "    print(f\"\\n--- Gate 2: Evidence Sufficiency (Attempt {attempts}/{GATE_2_CONFIG['max_emergency_attempts'] + 1}) ---\")\n",
        "    \n",
        "    # Check verification rate\n",
        "    verified = [c for c in claims_registry.values() if c.get(\"verification_status\") == \"verified\"]\n",
        "    verification_rate = len(verified) / len(claims_registry) if claims_registry else 0\n",
        "    \n",
        "    # Check domain diversity\n",
        "    domains = set(extract_domain(url) for url in source_urls)\n",
        "    \n",
        "    # Check section coverage\n",
        "    covered_sections = set()\n",
        "    for claim in verified:\n",
        "        covered_sections.add(claim.get(\"source_node\"))\n",
        "    section_coverage = len(covered_sections) / len(skeleton.get(\"nodes\", {})) if skeleton.get(\"nodes\") else 0\n",
        "    \n",
        "    print(f\"  Verification rate: {verification_rate:.1%} (threshold: {GATE_2_CONFIG['verification_rate_threshold']:.0%})\")\n",
        "    print(f\"  Domain diversity: {len(domains)} (threshold: {GATE_2_CONFIG['min_domain_diversity']})\")\n",
        "    print(f\"  Section coverage: {section_coverage:.1%} (threshold: {GATE_2_CONFIG['section_coverage_threshold']:.0%})\")\n",
        "    \n",
        "    passed = (\n",
        "        verification_rate >= GATE_2_CONFIG[\"verification_rate_threshold\"] and\n",
        "        len(domains) >= GATE_2_CONFIG[\"min_domain_diversity\"] and\n",
        "        section_coverage >= GATE_2_CONFIG[\"section_coverage_threshold\"]\n",
        "    )\n",
        "    \n",
        "    print(f\"  {'PASSED' if passed else 'FAILED'} Gate 2\")\n",
        "    \n",
        "    return {\n",
        "        \"gate_2_attempts\": attempts,\n",
        "        \"gate_2_passed\": passed\n",
        "    }\n",
        "\n",
        "\n",
        "def route_after_gate_2(state: SFEOState) -> Literal[\"generate_prose\", \"emergency_research\", \"generate_prose_anyway\"]:\n",
        "    \"\"\"Route based on Gate 2 result.\"\"\"\n",
        "    passed = state.get(\"gate_2_passed\", False)\n",
        "    attempts = state.get(\"gate_2_attempts\", 0)\n",
        "    \n",
        "    if passed:\n",
        "        return \"generate_prose\"\n",
        "    elif attempts <= GATE_2_CONFIG[\"max_emergency_attempts\"]:\n",
        "        return \"emergency_research\"\n",
        "    else:\n",
        "        print(\"  Max Gate 2 attempts reached. Proceeding with available evidence.\")\n",
        "        return \"generate_prose_anyway\"\n",
        "\n",
        "\n",
        "async def emergency_research(state: SFEOState) -> dict:\n",
        "    \"\"\"Conduct targeted research to fill evidence gaps.\"\"\"\n",
        "    global knowledge_base\n",
        "    \n",
        "    claims_registry = state.get(\"claims_registry\", {})\n",
        "    skeleton = state.get(\"skeleton\", {})\n",
        "    \n",
        "    print(f\"\\n  Emergency research for evidence gaps...\")\n",
        "    \n",
        "    # Find uncovered sections\n",
        "    covered_sections = set()\n",
        "    for claim in claims_registry.values():\n",
        "        if claim.get(\"verification_status\") == \"verified\":\n",
        "            covered_sections.add(claim.get(\"source_node\"))\n",
        "    \n",
        "    uncovered = [nid for nid in skeleton.get(\"nodes\", {}).keys() if nid not in covered_sections]\n",
        "    \n",
        "    new_urls = []\n",
        "    cache_decisions = []\n",
        "    \n",
        "    for node_id in uncovered[:3]:  # Limit emergency searches\n",
        "        node = skeleton[\"nodes\"].get(node_id, {})\n",
        "        query = f\"{node.get('title', '')} {node.get('intent', '')[:50]}\"\n",
        "        \n",
        "        print(f\"  Emergency search for {node_id}: {query[:40]}...\")\n",
        "        \n",
        "        content, urls, decision = await cascaded_search(query, knowledge_base)\n",
        "        new_urls.extend(urls)\n",
        "        cache_decisions.append(decision.model_dump())\n",
        "    \n",
        "    print(f\"  Emergency research collected {len(new_urls)} new URLs\")\n",
        "    \n",
        "    return {\n",
        "        \"source_urls\": new_urls,\n",
        "        \"cache_decisions\": cache_decisions\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Phase B node functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Phase C: Document Construction\n",
        "\n",
        "Phase C generates the document from verified claims:\n",
        "1. Generate prose patches for each skeleton node\n",
        "2. Create bridge sentences for transitions\n",
        "3. Assemble complete draft\n",
        "4. Gate 3: Validate prose quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def generate_prose(state: SFEOState) -> dict:\n",
        "    \"\"\"Generate prose patches for each skeleton node.\"\"\"\n",
        "    skeleton = state[\"skeleton\"]\n",
        "    claims_registry = state.get(\"claims_registry\", {})\n",
        "    evidence_map = state.get(\"evidence_map\", {})\n",
        "    source_urls = state.get(\"source_urls\", [])\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Phase C.1: Prose Generation\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    leaf_nodes = get_leaf_nodes(skeleton)\n",
        "    sorted_nodes = topological_sort_nodes(skeleton, leaf_nodes)\n",
        "    \n",
        "    prose_store = {}\n",
        "    \n",
        "    for i, node_id in enumerate(sorted_nodes):\n",
        "        node = skeleton[\"nodes\"][node_id]\n",
        "        \n",
        "        print(f\"  [{i+1}/{len(sorted_nodes)}] Generating: {node_id} - {node['title']}\")\n",
        "        \n",
        "        # Get claims for this node with their evidence\n",
        "        node_claims = [\n",
        "            (cid, claim) for cid, claim in claims_registry.items()\n",
        "            if claim.get(\"source_node\") == node_id\n",
        "        ]\n",
        "        \n",
        "        claims_with_evidence = []\n",
        "        for cid, claim in node_claims:\n",
        "            evidence = evidence_map.get(cid, [])\n",
        "            evidence_text = evidence[0][:500] if evidence else \"No evidence found\"\n",
        "            claims_with_evidence.append(\n",
        "                f\"- {claim['claim_text']}\\n  Evidence: {evidence_text}\"\n",
        "            )\n",
        "        \n",
        "        # Get context from adjacent nodes\n",
        "        prev_node, next_node = get_adjacent_nodes(skeleton, node_id)\n",
        "        prev_summary = \"\"\n",
        "        next_preview = \"\"\n",
        "        \n",
        "        if prev_node and prev_node in prose_store:\n",
        "            prev_summary = prose_store[prev_node].get(\"summary\", \"\")\n",
        "        if next_node:\n",
        "            next_node_data = skeleton[\"nodes\"].get(next_node, {})\n",
        "            next_preview = next_node_data.get(\"intent\", \"\")\n",
        "        \n",
        "        prompt = PROSE_GENERATION_PROMPT.format(\n",
        "            node_id=node_id,\n",
        "            title=node.get(\"title\", \"\"),\n",
        "            intent=node.get(\"intent\", \"\"),\n",
        "            target_words=node.get(\"target_word_count\", 300),\n",
        "            claims_with_evidence=\"\\n\\n\".join(claims_with_evidence) if claims_with_evidence else \"No specific claims assigned\",\n",
        "            prev_summary=prev_summary if prev_summary else \"(First section)\",\n",
        "            next_preview=next_preview if next_preview else \"(Last section)\"\n",
        "        )\n",
        "        \n",
        "        structured_llm = llm.with_structured_output(ProseGenerationOutput)\n",
        "        \n",
        "        try:\n",
        "            result = await structured_llm.ainvoke([HumanMessage(content=prompt)])\n",
        "            \n",
        "            prose_store[node_id] = {\n",
        "                \"node_id\": node_id,\n",
        "                \"main_content\": result.main_content,\n",
        "                \"bridge_in\": result.bridge_in,\n",
        "                \"bridge_out\": result.bridge_out,\n",
        "                \"summary\": result.summary,\n",
        "                \"revision_count\": 0,\n",
        "                \"citations_used\": result.citations_used\n",
        "            }\n",
        "            \n",
        "            print(f\"      Generated {len(result.main_content)} chars\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"      Error generating prose: {e}\")\n",
        "            prose_store[node_id] = {\n",
        "                \"node_id\": node_id,\n",
        "                \"main_content\": f\"[Content generation failed for {node['title']}]\",\n",
        "                \"bridge_in\": \"\",\n",
        "                \"bridge_out\": \"\",\n",
        "                \"summary\": \"\",\n",
        "                \"revision_count\": 0,\n",
        "                \"citations_used\": []\n",
        "            }\n",
        "        \n",
        "        # Mark as expanded in skeleton\n",
        "        skeleton[\"nodes\"][node_id][\"is_expanded\"] = True\n",
        "    \n",
        "    return {\n",
        "        \"skeleton\": skeleton,\n",
        "        \"prose_store\": prose_store\n",
        "    }\n",
        "\n",
        "\n",
        "async def assemble_draft(state: SFEOState) -> dict:\n",
        "    \"\"\"Assemble all prose patches into a complete draft.\"\"\"\n",
        "    skeleton = state[\"skeleton\"]\n",
        "    prose_store = state.get(\"prose_store\", {})\n",
        "    source_urls = state.get(\"source_urls\", [])\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Phase C.2: Draft Assembly\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    leaf_nodes = get_leaf_nodes(skeleton)\n",
        "    \n",
        "    # Build document\n",
        "    document_parts = []\n",
        "    \n",
        "    # Title and thesis\n",
        "    document_parts.append(f\"# Research Report\\n\")\n",
        "    document_parts.append(f\"**Thesis:** {skeleton.get('thesis', '')}\\n\\n\")\n",
        "    \n",
        "    # Sections\n",
        "    for node_id in leaf_nodes:\n",
        "        if node_id not in prose_store:\n",
        "            continue\n",
        "        \n",
        "        node = skeleton[\"nodes\"].get(node_id, {})\n",
        "        prose = prose_store[node_id]\n",
        "        \n",
        "        document_parts.append(f\"## {node.get('title', node_id)}\\n\\n\")\n",
        "        \n",
        "        # Bridge in flows naturally\n",
        "        if prose.get(\"bridge_in\"):\n",
        "            document_parts.append(f\"{prose['bridge_in']} \")\n",
        "        \n",
        "        document_parts.append(f\"{prose.get('main_content', '')}\")\n",
        "        \n",
        "        if prose.get(\"bridge_out\"):\n",
        "            document_parts.append(f\" {prose['bridge_out']}\")\n",
        "        \n",
        "        document_parts.append(\"\\n\\n\")\n",
        "    \n",
        "    # References section\n",
        "    unique_urls = list(set(source_urls))\n",
        "    if unique_urls:\n",
        "        document_parts.append(\"## References\\n\\n\")\n",
        "        for i, url in enumerate(unique_urls[:25], 1):\n",
        "            document_parts.append(f\"{i}. {url}\\n\")\n",
        "    \n",
        "    assembled_draft = \"\".join(document_parts)\n",
        "    word_count = len(assembled_draft.split())\n",
        "    \n",
        "    print(f\"  Assembled draft: {len(assembled_draft)} chars, {word_count} words\")\n",
        "    print(f\"  Sections: {len(leaf_nodes)}\")\n",
        "    print(f\"  References: {len(unique_urls)}\")\n",
        "    \n",
        "    return {\n",
        "        \"assembled_draft\": assembled_draft,\n",
        "        \"gate_3_passed\": False,\n",
        "        \"gate_3_scores\": {}\n",
        "    }\n",
        "\n",
        "\n",
        "async def prose_quality_gate(state: SFEOState) -> dict:\n",
        "    \"\"\"Gate 3: Evaluate prose quality.\"\"\"\n",
        "    assembled_draft = state.get(\"assembled_draft\", \"\")\n",
        "    question = state[\"question\"]\n",
        "    \n",
        "    print(f\"\\n--- Gate 3: Prose Quality ---\")\n",
        "    \n",
        "    # Truncate draft for evaluation\n",
        "    draft_for_eval = assembled_draft[:8000] if len(assembled_draft) > 8000 else assembled_draft\n",
        "    \n",
        "    prompt = GATE_3_EVALUATION_PROMPT.format(\n",
        "        question=question,\n",
        "        draft=draft_for_eval\n",
        "    )\n",
        "    \n",
        "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
        "    content = response.content\n",
        "    \n",
        "    # Parse scores\n",
        "    scores = {}\n",
        "    for metric in [\"COHERENCE\", \"DEPTH\", \"EVIDENCE\", \"COMPLETENESS\", \"OVERALL\"]:\n",
        "        match = re.search(rf'{metric}:\\s*(\\d+(?:\\.\\d+)?)', content)\n",
        "        if match:\n",
        "            scores[metric.lower()] = float(match.group(1))\n",
        "    \n",
        "    overall = scores.get(\"overall\", 7.0)\n",
        "    \n",
        "    passed = (\n",
        "        scores.get(\"coherence\", 0) >= GATE_3_CONFIG[\"coherence_threshold\"] and\n",
        "        scores.get(\"depth\", 0) >= GATE_3_CONFIG[\"depth_threshold\"] and\n",
        "        overall >= 6.5\n",
        "    )\n",
        "    \n",
        "    print(f\"  Coherence: {scores.get('coherence', 'N/A')}/10\")\n",
        "    print(f\"  Depth: {scores.get('depth', 'N/A')}/10\")\n",
        "    print(f\"  Evidence: {scores.get('evidence', 'N/A')}/10\")\n",
        "    print(f\"  Overall: {overall}/10\")\n",
        "    print(f\"  {'PASSED' if passed else 'FAILED'} Gate 3\")\n",
        "    \n",
        "    return {\n",
        "        \"gate_3_passed\": passed,\n",
        "        \"gate_3_scores\": scores,\n",
        "        \"quality_scores\": [overall],\n",
        "        \"iteration_count\": 0,\n",
        "        \"noise_map\": [],\n",
        "        \"nodes_to_patch\": []\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Phase C node functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Phase D: Refinement & Polish\n",
        "\n",
        "Phase D implements the critique-patch loop:\n",
        "1. Structured critique producing noise map\n",
        "2. Targeted retrieval for evidence gaps\n",
        "3. Apply patches to specific nodes\n",
        "4. Check for cascades and convergence\n",
        "5. Final polish for output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def structured_critique(state: SFEOState) -> dict:\n",
        "    \"\"\"Perform multi-level critique producing noise map.\"\"\"\n",
        "    skeleton = state[\"skeleton\"]\n",
        "    prose_store = state.get(\"prose_store\", {})\n",
        "    claims_registry = state.get(\"claims_registry\", {})\n",
        "    assembled_draft = state.get(\"assembled_draft\", \"\")\n",
        "    question = state[\"question\"]\n",
        "    iteration_count = state.get(\"iteration_count\", 0)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Phase D.1: Structured Critique (Iteration {iteration_count})\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Build skeleton summary\n",
        "    skeleton_summary = \"\\n\".join([\n",
        "        f\"- {nid}: {node['title']}\"\n",
        "        for nid, node in skeleton[\"nodes\"].items()\n",
        "    ])\n",
        "    \n",
        "    # Build claims summary\n",
        "    claims_summary = \"\\n\".join([\n",
        "        f\"- [{c['verification_status']}] {cid}: {c['claim_text'][:60]}...\"\n",
        "        for cid, c in list(claims_registry.items())[:15]\n",
        "    ])\n",
        "    \n",
        "    # Truncate draft for critique\n",
        "    draft_for_critique = assembled_draft[:10000] if len(assembled_draft) > 10000 else assembled_draft\n",
        "    \n",
        "    prompt = STRUCTURED_CRITIQUE_PROMPT.format(\n",
        "        question=question,\n",
        "        skeleton_summary=skeleton_summary,\n",
        "        claims_summary=claims_summary,\n",
        "        draft=draft_for_critique\n",
        "    )\n",
        "    \n",
        "    structured_llm = llm.with_structured_output(CritiqueResult)\n",
        "    \n",
        "    try:\n",
        "        result = await structured_llm.ainvoke([HumanMessage(content=prompt)])\n",
        "        \n",
        "        noise_map = [issue.model_dump() for issue in result.issues]\n",
        "        \n",
        "        # Identify nodes needing patches\n",
        "        nodes_to_patch = list(set(\n",
        "            node_id\n",
        "            for issue in result.issues\n",
        "            for node_id in issue.target_nodes\n",
        "            if issue.severity in [\"critical\", \"major\"]\n",
        "        ))\n",
        "        \n",
        "        print(f\"  Quality Score: {result.overall_quality}/10\")\n",
        "        print(f\"  Issues found: {len(result.issues)}\")\n",
        "        print(f\"    - Critical: {sum(1 for i in result.issues if i.severity == 'critical')}\")\n",
        "        print(f\"    - Major: {sum(1 for i in result.issues if i.severity == 'major')}\")\n",
        "        print(f\"    - Minor: {sum(1 for i in result.issues if i.severity == 'minor')}\")\n",
        "        print(f\"  Nodes to patch: {nodes_to_patch}\")\n",
        "        \n",
        "        return {\n",
        "            \"noise_map\": noise_map,\n",
        "            \"nodes_to_patch\": nodes_to_patch,\n",
        "            \"quality_scores\": [result.overall_quality]\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  Error in critique: {e}\")\n",
        "        return {\n",
        "            \"noise_map\": [],\n",
        "            \"nodes_to_patch\": [],\n",
        "            \"quality_scores\": [7.0]  # Default score\n",
        "        }\n",
        "\n",
        "\n",
        "def should_continue_refinement(state: SFEOState) -> Literal[\"targeted_retrieval\", \"final_polish\"]:\n",
        "    \"\"\"Determine if another refinement iteration is needed.\"\"\"\n",
        "    iteration_count = state.get(\"iteration_count\", 0)\n",
        "    quality_scores = state.get(\"quality_scores\", [])\n",
        "    nodes_to_patch = state.get(\"nodes_to_patch\", [])\n",
        "    noise_map = state.get(\"noise_map\", [])\n",
        "    \n",
        "    latest_score = quality_scores[-1] if quality_scores else 0\n",
        "    \n",
        "    print(f\"\\n--- Convergence Check ---\")\n",
        "    print(f\"  Iteration: {iteration_count}/{REFINEMENT_CONFIG['max_iterations']}\")\n",
        "    print(f\"  Quality: {latest_score}/{REFINEMENT_CONFIG['quality_threshold']}\")\n",
        "    print(f\"  Nodes to patch: {len(nodes_to_patch)}\")\n",
        "    \n",
        "    # Convergence conditions\n",
        "    if iteration_count >= REFINEMENT_CONFIG[\"max_iterations\"]:\n",
        "        print(f\"  -> Max iterations reached. Finalizing.\")\n",
        "        return \"final_polish\"\n",
        "    \n",
        "    if latest_score >= REFINEMENT_CONFIG[\"quality_threshold\"]:\n",
        "        print(f\"  -> Quality threshold met. Finalizing.\")\n",
        "        return \"final_polish\"\n",
        "    \n",
        "    if not nodes_to_patch:\n",
        "        print(f\"  -> No critical/major issues. Finalizing.\")\n",
        "        return \"final_polish\"\n",
        "    \n",
        "    # Check for diminishing returns\n",
        "    if len(quality_scores) >= 2:\n",
        "        improvement = quality_scores[-1] - quality_scores[-2]\n",
        "        if improvement < REFINEMENT_CONFIG[\"min_improvement_threshold\"]:\n",
        "            print(f\"  -> Diminishing returns ({improvement:.2f}). Finalizing.\")\n",
        "            return \"final_polish\"\n",
        "    \n",
        "    print(f\"  -> Continuing refinement.\")\n",
        "    return \"targeted_retrieval\"\n",
        "\n",
        "\n",
        "async def targeted_retrieval(state: SFEOState) -> dict:\n",
        "    \"\"\"Search for evidence to address specific issues in the noise map.\"\"\"\n",
        "    global knowledge_base\n",
        "    \n",
        "    noise_map = state.get(\"noise_map\", [])\n",
        "    nodes_to_patch = state.get(\"nodes_to_patch\", [])\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Phase D.2: Targeted Retrieval\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    if not nodes_to_patch:\n",
        "        print(\"  No nodes need patching - skipping retrieval\")\n",
        "        return {\"source_urls\": [], \"cache_decisions\": []}\n",
        "    \n",
        "    # Collect search queries from issues\n",
        "    targeted_evidence = {}\n",
        "    all_urls = []\n",
        "    cache_decisions = []\n",
        "    \n",
        "    for issue in noise_map:\n",
        "        if issue.get(\"search_query\") and issue.get(\"severity\") in [\"critical\", \"major\"]:\n",
        "            query = issue[\"search_query\"]\n",
        "            \n",
        "            for node_id in issue.get(\"target_nodes\", []):\n",
        "                if node_id in nodes_to_patch:\n",
        "                    print(f\"  Searching for {node_id}: {query[:40]}...\")\n",
        "                    \n",
        "                    content, urls, decision = await cascaded_search(query, knowledge_base)\n",
        "                    \n",
        "                    if node_id not in targeted_evidence:\n",
        "                        targeted_evidence[node_id] = []\n",
        "                    targeted_evidence[node_id].append(content[:800])\n",
        "                    \n",
        "                    all_urls.extend(urls)\n",
        "                    cache_decisions.append(decision.model_dump())\n",
        "    \n",
        "    print(f\"  Targeted retrieval complete: {len(all_urls)} new URLs\")\n",
        "    \n",
        "    return {\n",
        "        \"source_urls\": all_urls,\n",
        "        \"cache_decisions\": cache_decisions\n",
        "    }\n",
        "\n",
        "\n",
        "async def apply_patches(state: SFEOState) -> dict:\n",
        "    \"\"\"Apply targeted patches to nodes with issues.\"\"\"\n",
        "    skeleton = state[\"skeleton\"]\n",
        "    prose_store = state.get(\"prose_store\", {})\n",
        "    noise_map = state.get(\"noise_map\", [])\n",
        "    nodes_to_patch = state.get(\"nodes_to_patch\", [])\n",
        "    iteration_count = state.get(\"iteration_count\", 0)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Phase D.3: Patch Application\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    if not nodes_to_patch:\n",
        "        print(\"  No patches to apply\")\n",
        "        return {\n",
        "            \"prose_store\": prose_store,\n",
        "            \"iteration_count\": iteration_count + 1\n",
        "        }\n",
        "    \n",
        "    sorted_patch_nodes = topological_sort_nodes(skeleton, nodes_to_patch)\n",
        "    patched_count = 0\n",
        "    \n",
        "    for node_id in sorted_patch_nodes[:REFINEMENT_CONFIG[\"max_cascades_per_iteration\"]]:\n",
        "        if node_id not in prose_store:\n",
        "            continue\n",
        "        \n",
        "        node = skeleton[\"nodes\"].get(node_id, {})\n",
        "        current_prose = prose_store[node_id]\n",
        "        \n",
        "        print(f\"  Patching: {node_id}\")\n",
        "        \n",
        "        # Collect issues for this node\n",
        "        node_issues = [\n",
        "            issue for issue in noise_map\n",
        "            if node_id in issue.get(\"target_nodes\", [])\n",
        "        ]\n",
        "        issues_text = \"\\n\".join([\n",
        "            f\"- [{i.get('severity', '')}] {i.get('issue_type', '')}: {i.get('description', '')}\"\n",
        "            for i in node_issues\n",
        "        ])\n",
        "        \n",
        "        # Get adjacent context\n",
        "        prev_node, next_node = get_adjacent_nodes(skeleton, node_id)\n",
        "        prev_bridge_out = \"\"\n",
        "        next_bridge_in = \"\"\n",
        "        \n",
        "        if prev_node and prev_node in prose_store:\n",
        "            prev_bridge_out = prose_store[prev_node].get(\"bridge_out\", \"\")\n",
        "        if next_node and next_node in prose_store:\n",
        "            next_bridge_in = prose_store[next_node].get(\"bridge_in\", \"\")\n",
        "        \n",
        "        prompt = PATCH_APPLICATION_PROMPT.format(\n",
        "            node_id=node_id,\n",
        "            title=node.get(\"title\", \"\"),\n",
        "            intent=node.get(\"intent\", \"\"),\n",
        "            current_content=current_prose.get(\"main_content\", \"\"),\n",
        "            issues=issues_text,\n",
        "            new_evidence=\"[See targeted retrieval results above]\",\n",
        "            prev_bridge_out=prev_bridge_out if prev_bridge_out else \"(First section)\",\n",
        "            next_bridge_in=next_bridge_in if next_bridge_in else \"(Last section)\"\n",
        "        )\n",
        "        \n",
        "        structured_llm = llm.with_structured_output(ProseGenerationOutput)\n",
        "        \n",
        "        try:\n",
        "            result = await structured_llm.ainvoke([HumanMessage(content=prompt)])\n",
        "            \n",
        "            # Update prose store\n",
        "            prose_store[node_id] = {\n",
        "                \"node_id\": node_id,\n",
        "                \"main_content\": result.main_content,\n",
        "                \"bridge_in\": result.bridge_in,\n",
        "                \"bridge_out\": result.bridge_out,\n",
        "                \"summary\": result.summary,\n",
        "                \"revision_count\": current_prose.get(\"revision_count\", 0) + 1,\n",
        "                \"citations_used\": result.citations_used\n",
        "            }\n",
        "            \n",
        "            patched_count += 1\n",
        "            print(f\"    Patched: {len(result.main_content)} chars (rev #{prose_store[node_id]['revision_count']})\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"    Error patching {node_id}: {e}\")\n",
        "    \n",
        "    print(f\"  Patched {patched_count} nodes\")\n",
        "    \n",
        "    # Reassemble draft after patches\n",
        "    leaf_nodes = get_leaf_nodes(skeleton)\n",
        "    document_parts = []\n",
        "    document_parts.append(f\"# Research Report\\n\")\n",
        "    document_parts.append(f\"**Thesis:** {skeleton.get('thesis', '')}\\n\\n\")\n",
        "    \n",
        "    for nid in leaf_nodes:\n",
        "        if nid not in prose_store:\n",
        "            continue\n",
        "        node = skeleton[\"nodes\"].get(nid, {})\n",
        "        prose = prose_store[nid]\n",
        "        document_parts.append(f\"## {node.get('title', nid)}\\n\\n\")\n",
        "        if prose.get(\"bridge_in\"):\n",
        "            document_parts.append(f\"{prose['bridge_in']} \")\n",
        "        document_parts.append(f\"{prose.get('main_content', '')}\")\n",
        "        if prose.get(\"bridge_out\"):\n",
        "            document_parts.append(f\" {prose['bridge_out']}\")\n",
        "        document_parts.append(\"\\n\\n\")\n",
        "    \n",
        "    # Add references\n",
        "    source_urls = state.get(\"source_urls\", [])\n",
        "    unique_urls = list(set(source_urls))\n",
        "    if unique_urls:\n",
        "        document_parts.append(\"## References\\n\\n\")\n",
        "        for i, url in enumerate(unique_urls[:25], 1):\n",
        "            document_parts.append(f\"{i}. {url}\\n\")\n",
        "    \n",
        "    assembled_draft = \"\".join(document_parts)\n",
        "    \n",
        "    return {\n",
        "        \"prose_store\": prose_store,\n",
        "        \"assembled_draft\": assembled_draft,\n",
        "        \"iteration_count\": iteration_count + 1\n",
        "    }\n",
        "\n",
        "\n",
        "async def final_polish(state: SFEOState) -> dict:\n",
        "    \"\"\"Apply final polish and formatting to the report.\"\"\"\n",
        "    global knowledge_base\n",
        "    \n",
        "    assembled_draft = state.get(\"assembled_draft\", \"\")\n",
        "    skeleton = state.get(\"skeleton\", {})\n",
        "    quality_scores = state.get(\"quality_scores\", [])\n",
        "    source_urls = state.get(\"source_urls\", [])\n",
        "    iteration_count = state.get(\"iteration_count\", 0)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Phase D.4: Final Polish\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Final report is the assembled draft with stats\n",
        "    final_report = assembled_draft\n",
        "    \n",
        "    # Calculate stats\n",
        "    word_count = len(final_report.split())\n",
        "    unique_urls = list(set(source_urls))\n",
        "    leaf_nodes = get_leaf_nodes(skeleton)\n",
        "    \n",
        "    # Cache stats\n",
        "    cache_stats = knowledge_base.get_stats_summary() if knowledge_base else \"N/A\"\n",
        "    \n",
        "    print(f\"  Final report: {len(final_report)} chars, {word_count} words\")\n",
        "    print(f\"  Sections: {len(leaf_nodes)}\")\n",
        "    print(f\"  Iterations: {iteration_count}\")\n",
        "    print(f\"  Quality progression: {' -> '.join([f'{s:.1f}' for s in quality_scores])}\")\n",
        "    print(f\"  Sources: {len(unique_urls)}\")\n",
        "    print(f\"  {cache_stats}\")\n",
        "    \n",
        "    return {\"final_report\": final_report}\n",
        "\n",
        "\n",
        "print(\"Phase D node functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Graph Construction\n",
        "\n",
        "Build and compile the complete SFEO StateGraph with all phases and routing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the SFEO Research Agent graph\n",
        "sfeo_builder = StateGraph(SFEOState)\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# PHASE A NODES\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "sfeo_builder.add_node(\"preliminary_research\", preliminary_research)\n",
        "sfeo_builder.add_node(\"generate_skeleton\", generate_skeleton)\n",
        "sfeo_builder.add_node(\"identify_claims\", identify_claims)\n",
        "sfeo_builder.add_node(\"create_research_backlog\", create_research_backlog)\n",
        "sfeo_builder.add_node(\"skeleton_quality_gate\", skeleton_quality_gate)\n",
        "sfeo_builder.add_node(\"refine_skeleton\", refine_skeleton)\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# PHASE B NODES\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "sfeo_builder.add_node(\"initialize_cache\", initialize_cache)\n",
        "sfeo_builder.add_node(\"sprint_execute\", sprint_execute)\n",
        "sfeo_builder.add_node(\"sprint_retrospective\", sprint_retrospective)\n",
        "sfeo_builder.add_node(\"evidence_quality_gate\", evidence_quality_gate)\n",
        "sfeo_builder.add_node(\"emergency_research\", emergency_research)\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# PHASE C NODES\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "sfeo_builder.add_node(\"generate_prose\", generate_prose)\n",
        "sfeo_builder.add_node(\"assemble_draft\", assemble_draft)\n",
        "sfeo_builder.add_node(\"prose_quality_gate\", prose_quality_gate)\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# PHASE D NODES\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "sfeo_builder.add_node(\"structured_critique\", structured_critique)\n",
        "sfeo_builder.add_node(\"targeted_retrieval\", targeted_retrieval)\n",
        "sfeo_builder.add_node(\"apply_patches\", apply_patches)\n",
        "sfeo_builder.add_node(\"final_polish\", final_polish)\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# PHASE A EDGES\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "sfeo_builder.add_edge(START, \"preliminary_research\")\n",
        "sfeo_builder.add_edge(\"preliminary_research\", \"generate_skeleton\")\n",
        "sfeo_builder.add_edge(\"generate_skeleton\", \"identify_claims\")\n",
        "sfeo_builder.add_edge(\"identify_claims\", \"create_research_backlog\")\n",
        "sfeo_builder.add_edge(\"create_research_backlog\", \"skeleton_quality_gate\")\n",
        "\n",
        "# Gate 1 conditional routing\n",
        "sfeo_builder.add_conditional_edges(\n",
        "    \"skeleton_quality_gate\",\n",
        "    route_after_gate_1,\n",
        "    {\n",
        "        \"initialize_cache\": \"initialize_cache\",\n",
        "        \"refine_skeleton\": \"refine_skeleton\",\n",
        "        \"initialize_cache_anyway\": \"initialize_cache\"\n",
        "    }\n",
        ")\n",
        "sfeo_builder.add_edge(\"refine_skeleton\", \"identify_claims\")\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# PHASE B EDGES\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "sfeo_builder.add_edge(\"initialize_cache\", \"sprint_execute\")\n",
        "sfeo_builder.add_edge(\"sprint_execute\", \"sprint_retrospective\")\n",
        "\n",
        "# Sprint loop conditional routing\n",
        "sfeo_builder.add_conditional_edges(\n",
        "    \"sprint_retrospective\",\n",
        "    route_after_sprint,\n",
        "    {\n",
        "        \"sprint_execute\": \"sprint_execute\",\n",
        "        \"evidence_quality_gate\": \"evidence_quality_gate\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Gate 2 conditional routing\n",
        "sfeo_builder.add_conditional_edges(\n",
        "    \"evidence_quality_gate\",\n",
        "    route_after_gate_2,\n",
        "    {\n",
        "        \"generate_prose\": \"generate_prose\",\n",
        "        \"emergency_research\": \"emergency_research\",\n",
        "        \"generate_prose_anyway\": \"generate_prose\"\n",
        "    }\n",
        ")\n",
        "sfeo_builder.add_edge(\"emergency_research\", \"evidence_quality_gate\")\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# PHASE C EDGES\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "sfeo_builder.add_edge(\"generate_prose\", \"assemble_draft\")\n",
        "sfeo_builder.add_edge(\"assemble_draft\", \"prose_quality_gate\")\n",
        "sfeo_builder.add_edge(\"prose_quality_gate\", \"structured_critique\")\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# PHASE D EDGES\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# Refinement loop conditional routing\n",
        "sfeo_builder.add_conditional_edges(\n",
        "    \"structured_critique\",\n",
        "    should_continue_refinement,\n",
        "    {\n",
        "        \"targeted_retrieval\": \"targeted_retrieval\",\n",
        "        \"final_polish\": \"final_polish\"\n",
        "    }\n",
        ")\n",
        "sfeo_builder.add_edge(\"targeted_retrieval\", \"apply_patches\")\n",
        "sfeo_builder.add_edge(\"apply_patches\", \"structured_critique\")\n",
        "\n",
        "# Final\n",
        "sfeo_builder.add_edge(\"final_polish\", END)\n",
        "\n",
        "# Compile the graph\n",
        "sfeo_graph = sfeo_builder.compile()\n",
        "\n",
        "print(\"SFEO Research Agent compiled successfully\")\n",
        "print(\"\\nArchitecture Flow:\")\n",
        "print(\"  Phase A: preliminary_research → generate_skeleton → identify_claims → create_backlog → Gate 1\")\n",
        "print(\"  Phase B: initialize_cache → [sprint_execute ↔ sprint_retrospective] → Gate 2\")\n",
        "print(\"  Phase C: generate_prose → assemble_draft → Gate 3\")\n",
        "print(\"  Phase D: structured_critique → [targeted_retrieval → apply_patches] → final_polish\")\n",
        "print(\"\\nKey features:\")\n",
        "print(\"  - Skeleton-first approach with claim-driven research\")\n",
        "print(\"  - 3-layer cascading knowledge cache\")\n",
        "print(\"  - Sprint-based evidence gathering with retrospectives\")\n",
        "print(\"  - Quality gates at phase transitions\")\n",
        "print(\"  - Patch-based refinement with cascade detection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the graph\n",
        "from IPython.display import Image, display, Markdown\n",
        "\n",
        "try:\n",
        "    display(Image(sfeo_graph.get_graph().draw_mermaid_png()))\n",
        "except Exception as e:\n",
        "    print(f\"Could not display graph: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Agent Wrappers\n",
        "\n",
        "Sync and async wrappers for evaluation harness compatibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def sfeo_agent_async(inputs: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Async version of the SFEO research agent.\n",
        "    Use this version when calling from Jupyter notebooks.\n",
        "    \"\"\"\n",
        "    global knowledge_base\n",
        "    knowledge_base = None  # Will be initialized during execution\n",
        "    \n",
        "    question = inputs.get(\"question\", \"\")\n",
        "    \n",
        "    result = await sfeo_graph.ainvoke(\n",
        "        {\"question\": question},\n",
        "        config={\"recursion_limit\": 100}\n",
        "    )\n",
        "    \n",
        "    # Gather statistics\n",
        "    cache_stats = knowledge_base.stats.copy() if knowledge_base else {}\n",
        "    \n",
        "    return {\n",
        "        \"output\": result.get(\"final_report\", \"\"),\n",
        "        \"source_urls\": list(set(result.get(\"source_urls\", []))),\n",
        "        \"skeleton\": result.get(\"skeleton\", {}),\n",
        "        \"claims_registry\": result.get(\"claims_registry\", {}),\n",
        "        \"quality_scores\": result.get(\"quality_scores\", []),\n",
        "        \"iteration_count\": result.get(\"iteration_count\", 0),\n",
        "        \"cache_stats\": cache_stats,\n",
        "        \"gate_1_passed\": result.get(\"gate_1_passed\", False),\n",
        "        \"gate_2_passed\": result.get(\"gate_2_passed\", False),\n",
        "        \"gate_3_passed\": result.get(\"gate_3_passed\", False),\n",
        "        \"gate_3_scores\": result.get(\"gate_3_scores\", {})\n",
        "    }\n",
        "\n",
        "\n",
        "def sfeo_agent(inputs: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Sync wrapper function for SFEO research agent.\n",
        "    \n",
        "    Compatible with evaluation harness.\n",
        "    \n",
        "    Args:\n",
        "        inputs: Dictionary with 'question' key\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with 'output' key containing final report\n",
        "    \"\"\"\n",
        "    question = inputs.get(\"question\", \"\")\n",
        "    \n",
        "    async def _execute():\n",
        "        global knowledge_base\n",
        "        knowledge_base = None\n",
        "        \n",
        "        return await sfeo_graph.ainvoke(\n",
        "            {\"question\": question},\n",
        "            config={\"recursion_limit\": 100}\n",
        "        )\n",
        "    \n",
        "    try:\n",
        "        loop = asyncio.get_running_loop()\n",
        "        import concurrent.futures\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            future = executor.submit(asyncio.run, _execute())\n",
        "            result = future.result()\n",
        "    except RuntimeError:\n",
        "        result = asyncio.run(_execute())\n",
        "    \n",
        "    cache_stats = knowledge_base.stats.copy() if knowledge_base else {}\n",
        "    \n",
        "    return {\n",
        "        \"output\": result.get(\"final_report\", \"\"),\n",
        "        \"source_urls\": list(set(result.get(\"source_urls\", []))),\n",
        "        \"skeleton\": result.get(\"skeleton\", {}),\n",
        "        \"claims_registry\": result.get(\"claims_registry\", {}),\n",
        "        \"quality_scores\": result.get(\"quality_scores\", []),\n",
        "        \"iteration_count\": result.get(\"iteration_count\", 0),\n",
        "        \"cache_stats\": cache_stats\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Agent wrappers defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Manual Test\n",
        "\n",
        "Run this cell to verify the SFEO agent works correctly with a sample research question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual test with sample question\n",
        "test_question = \"What are the key benefits and challenges of using large language models in enterprise applications?\"\n",
        "\n",
        "print(f\"Testing SFEO Agent\")\n",
        "print(f\"Question: {test_question}\")\n",
        "print(\"\\nRunning SFEO research (this may take several minutes)...\\n\")\n",
        "\n",
        "try:\n",
        "    result = await sfeo_agent_async({\"question\": test_question})\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"FINAL REPORT\")\n",
        "    print(\"=\" * 80)\n",
        "    print(result[\"output\"][:4000] + \"...\" if len(result[\"output\"]) > 4000 else result[\"output\"])\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"EXECUTION SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Report length: {len(result['output'])} chars, {len(result['output'].split())} words\")\n",
        "    print(f\"Skeleton nodes: {len(result.get('skeleton', {}).get('nodes', {}))}\")\n",
        "    print(f\"Claims tracked: {len(result.get('claims_registry', {}))}\")\n",
        "    print(f\"Refinement iterations: {result.get('iteration_count', 0)}\")\n",
        "    print(f\"Quality progression: {result.get('quality_scores', [])}\")\n",
        "    print(f\"Unique sources: {len(result.get('source_urls', []))}\")\n",
        "    \n",
        "    print(f\"\\nGate Results:\")\n",
        "    print(f\"  Gate 1 (Skeleton): {'PASSED' if result.get('gate_1_passed') else 'FAILED'}\")\n",
        "    print(f\"  Gate 2 (Evidence): {'PASSED' if result.get('gate_2_passed') else 'FAILED'}\")\n",
        "    print(f\"  Gate 3 (Prose): {'PASSED' if result.get('gate_3_passed') else 'FAILED'}\")\n",
        "    \n",
        "    cache_stats = result.get(\"cache_stats\", {})\n",
        "    if cache_stats:\n",
        "        total = cache_stats.get(\"total_queries\", 0)\n",
        "        avoided = cache_stats.get(\"web_searches_avoided\", 0)\n",
        "        hit_rate = avoided / total * 100 if total else 0\n",
        "        print(f\"\\nCache Performance:\")\n",
        "        print(f\"  Total queries: {total}\")\n",
        "        print(f\"  Web searches avoided: {avoided} ({hit_rate:.1f}%)\")\n",
        "        print(f\"  L1 hits: {cache_stats.get('l1_hits', 0)}\")\n",
        "        print(f\"  L2 high: {cache_stats.get('l2_high', 0)}\")\n",
        "        print(f\"  L2 medium: {cache_stats.get('l2_medium', 0)}\")\n",
        "        print(f\"  L2 low: {cache_stats.get('l2_low', 0)}\")\n",
        "    \n",
        "    print(\"\\nAgent test PASSED\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Agent test FAILED: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Evaluation Harness Integration\n",
        "\n",
        "Use the evaluation harness to formally benchmark the SFEO agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to run evaluation harness\n",
        "# import sys\n",
        "# sys.path.insert(0, '../evaluation')\n",
        "# from harness import evaluate_agent\n",
        "\n",
        "# Run evaluation (uncomment when ready)\n",
        "# eval_result = evaluate_agent(\n",
        "#     agent_fn=sfeo_agent,\n",
        "#     agent_name=\"sfeo_combined_tier1\",\n",
        "#     questions=[\n",
        "#         {\"question\": \"What is the current state of quantum computing technology and its near-term applications?\"},\n",
        "#         {\"question\": \"How do self-driving cars detect and respond to pedestrians?\"},\n",
        "#     ],\n",
        "#     output_path=\"../results/sfeo_combined_tier1_results.json\"\n",
        "# )\n",
        "# print(eval_result)\n",
        "\n",
        "print(\"Evaluation harness integration ready\")\n",
        "print(\"Uncomment the code above to run formal benchmarks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook implements the **Skeleton-First Evidence Orchestration (SFEO)** architecture, combining all validated Tier 1 paradigms:\n",
        "\n",
        "### Architecture Overview\n",
        "\n",
        "| Phase | Components | Key Innovation |\n",
        "|-------|------------|----------------|\n",
        "| **A: Strategic Planning** | Preliminary Research → Skeleton → Claims → Backlog → Gate 1 | Document structure drives research |\n",
        "| **B: Evidence Gathering** | Cache Init → Sprint Loop → Gate 2 | Cascading 3-layer cache + agile sprints |\n",
        "| **C: Document Construction** | Prose Generation → Assembly → Gate 3 | Claim-evidence driven prose |\n",
        "| **D: Refinement** | Critique → Targeted Retrieval → Patch → Converge | Semantic addressing + cascade detection |\n",
        "\n",
        "### Key Features\n",
        "\n",
        "1. **Skeleton-First**: The document skeleton is generated BEFORE evidence gathering, focusing research on specific claims\n",
        "2. **Quality Gates**: Three gates prevent error propagation between phases\n",
        "3. **Cascading Cache**: 3-layer cache (L1: exact, L2: semantic, L3: LLM judgment) optimizes searches\n",
        "4. **Sprint-Based Research**: Agile-style sprints with retrospectives enable adaptive evidence gathering\n",
        "5. **Patch-Based Refinement**: Targeted updates with cascade detection maintain document coherence\n",
        "\n",
        "### Expected Performance\n",
        "\n",
        "- **Search Efficiency**: 30-50% cache hit rate in later phases\n",
        "- **Quality Scores**: Target 7.5+/10 overall quality\n",
        "- **Token Efficiency**: More focused research than baseline approaches\n",
        "- **Document Coherence**: Bridge sentences and cascade detection ensure flow\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Run manual test to verify agent execution\n",
        "2. Use evaluation harness for formal benchmarking\n",
        "3. Compare results against Baseline A/B"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
