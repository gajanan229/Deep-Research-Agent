{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Paradigm 04: Quality Gates Research Agent\n",
                "\n",
                "This notebook implements the **Industrial Control Theory (Quality Gates)** paradigm from the Research Paradigms document.\n",
                "\n",
                "## Core Concept\n",
                "\n",
                "Quality gates act as checkpoints between research phases, enforcing quality criteria before proceeding:\n",
                "- **Post-Search Gate**: Validates source quantity and diversity\n",
                "- **Post-Synthesis Gate**: Validates claim coverage and evidence support\n",
                "- **Retry Logic**: Modifies queries on gate failure\n",
                "\n",
                "## Literature Validation\n",
                "\n",
                "> \"Quality gates for research validation have emerged as a key architectural pattern in AI Scientist and Agent Laboratory... demonstrating the effectiveness of staged quality checkpoints.\" —Feasibility Report\n",
                "\n",
                "## Technology Stack\n",
                "\n",
                "- **LLM**: `gpt-5-mini-2025-08-07`\n",
                "- **Web Search**: Tavily API\n",
                "- **Tracing**: LangSmith\n",
                "- **Framework**: LangGraph"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import operator\n",
                "import asyncio\n",
                "import re\n",
                "from pathlib import Path\n",
                "from typing import List, Annotated, TypedDict, Literal\n",
                "from collections import Counter\n",
                "\n",
                "from dotenv import load_dotenv\n",
                "from pydantic import BaseModel, Field\n",
                "\n",
                "from langchain_openai import ChatOpenAI\n",
                "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
                "from tavily import TavilyClient\n",
                "\n",
                "from langgraph.graph import StateGraph, START, END\n",
                "\n",
                "# Load environment variables\n",
                "env_path = Path(\"../.env\")\n",
                "load_dotenv(env_path)\n",
                "\n",
                "# Configure LangSmith tracing\n",
                "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
                "os.environ[\"LANGSMITH_PROJECT\"] = \"deep_research_new\"\n",
                "\n",
                "print(\"Environment configured successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize LLM and Tavily client\n",
                "MODEL_NAME = \"gpt-5-mini-2025-08-07\"\n",
                "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
                "tavily_client = TavilyClient()\n",
                "\n",
                "# Quality Gate Configuration\n",
                "MIN_SOURCES = 5  # Minimum sources required to pass Gate 1\n",
                "MIN_DOMAIN_DIVERSITY = 3  # Minimum unique domains\n",
                "MAX_RETRIES = 2  # Maximum retry attempts per gate\n",
                "\n",
                "print(f\"Using model: {MODEL_NAME}\")\n",
                "print(f\"Gate 1: Min {MIN_SOURCES} sources, {MIN_DOMAIN_DIVERSITY} unique domains\")\n",
                "print(f\"Max retries: {MAX_RETRIES}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. State Definitions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SearchResult(BaseModel):\n",
                "    \"\"\"A search result with source metadata.\"\"\"\n",
                "    title: str = Field(description=\"Title of the source\")\n",
                "    content: str = Field(description=\"Content snippet\")\n",
                "    url: str = Field(description=\"Source URL\")\n",
                "    domain: str = Field(default=\"\", description=\"Domain of the source\")\n",
                "\n",
                "class GateResult(BaseModel):\n",
                "    \"\"\"Result of a quality gate check.\"\"\"\n",
                "    passed: bool = Field(description=\"Whether the gate was passed\")\n",
                "    reason: str = Field(description=\"Explanation of the gate result\")\n",
                "    suggestions: List[str] = Field(default_factory=list, description=\"Suggestions for improvement\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class QualityGateState(TypedDict):\n",
                "    \"\"\"State for the Quality Gates Research Agent.\"\"\"\n",
                "    # Input\n",
                "    question: str\n",
                "    \n",
                "    # Research plan\n",
                "    search_queries: List[str]\n",
                "    \n",
                "    # Search results\n",
                "    search_results: Annotated[List[str], operator.add]\n",
                "    source_urls: Annotated[List[str], operator.add]\n",
                "    \n",
                "    # Gate tracking\n",
                "    gate1_attempts: int\n",
                "    gate2_attempts: int\n",
                "    gate1_passed: bool\n",
                "    gate2_passed: bool\n",
                "    \n",
                "    # Synthesis\n",
                "    synthesis: str\n",
                "    \n",
                "    # Output\n",
                "    final_report: str"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_domain(url: str) -> str:\n",
                "    \"\"\"Extract domain from URL.\"\"\"\n",
                "    try:\n",
                "        from urllib.parse import urlparse\n",
                "        parsed = urlparse(url)\n",
                "        domain = parsed.netloc\n",
                "        # Remove www. prefix\n",
                "        if domain.startswith(\"www.\"):\n",
                "            domain = domain[4:]\n",
                "        return domain\n",
                "    except:\n",
                "        return \"unknown\"\n",
                "\n",
                "def search_web(query: str, max_results: int = 10) -> tuple[List[str], List[str]]:\n",
                "    \"\"\"Execute web search using Tavily. Returns (results, urls).\"\"\"\n",
                "    try:\n",
                "        # Truncate query if too long\n",
                "        if len(query) > 400:\n",
                "            query = query[:400]\n",
                "        \n",
                "        response = tavily_client.search(\n",
                "            query=query,\n",
                "            max_results=max_results,\n",
                "            include_answer=True\n",
                "        )\n",
                "        \n",
                "        results = []\n",
                "        urls = []\n",
                "        \n",
                "        if response.get(\"answer\"):\n",
                "            results.append(f\"Summary: {response['answer']}\")\n",
                "        \n",
                "        for r in response.get(\"results\", []):\n",
                "            url = r.get('url', '')\n",
                "            urls.append(url)\n",
                "            domain = extract_domain(url)\n",
                "            results.append(f\"- [{domain}] {r.get('title', 'No title')}: {r.get('content', '')[:500]}... (Source: {url})\")\n",
                "        \n",
                "        return results, urls\n",
                "    except Exception as e:\n",
                "        return [f\"Search error: {str(e)}\"], []"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Node Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prompts\n",
                "PLAN_QUERIES_PROMPT = \"\"\"You are a research planning expert. Given a research question,\n",
                "generate 5-7 specific web search queries that will gather comprehensive information.\n",
                "\n",
                "Research Question: {question}\n",
                "\n",
                "Consider different angles:\n",
                "- Definition and background\n",
                "- Current state and trends\n",
                "- Key challenges and solutions\n",
                "- Case studies and examples\n",
                "- Expert opinions and analysis\n",
                "\n",
                "Return ONLY the search queries, one per line.\n",
                "\"\"\"\n",
                "\n",
                "SYNTHESIS_PROMPT = \"\"\"You are a research analyst synthesizing findings.\n",
                "\n",
                "Research Question: {question}\n",
                "\n",
                "Search Results:\n",
                "{search_results}\n",
                "\n",
                "Synthesize these findings into a coherent analysis (600-800 words):\n",
                "1. Identify key themes and patterns\n",
                "2. Note any contradictions or debates\n",
                "3. Highlight important statistics or facts\n",
                "4. Reference sources appropriately\n",
                "\"\"\"\n",
                "\n",
                "IMPROVE_QUERIES_PROMPT = \"\"\"The previous search queries did not yield enough diverse sources.\n",
                "\n",
                "Original Question: {question}\n",
                "Previous Queries: {previous_queries}\n",
                "Gap Analysis: {gap_analysis}\n",
                "\n",
                "Generate 3 NEW search queries that will:\n",
                "1. Target different types of sources (academic, industry, news)\n",
                "2. Cover gaps identified in previous searches\n",
                "3. Be more specific or use different terminology\n",
                "\n",
                "Return ONLY the new search queries, one per line.\n",
                "\"\"\"\n",
                "\n",
                "FINAL_REPORT_PROMPT = \"\"\"You are a senior research analyst writing a final report.\n",
                "\n",
                "Research Question: {question}\n",
                "\n",
                "Synthesis:\n",
                "{synthesis}\n",
                "\n",
                "Source URLs:\n",
                "{sources}\n",
                "\n",
                "Write a comprehensive research report (1000-1500 words) that:\n",
                "1. Directly answers the research question\n",
                "2. Provides evidence-based analysis\n",
                "3. Includes proper citations\n",
                "4. Acknowledges limitations or uncertainties\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def generate_plan(state: QualityGateState) -> dict:\n",
                "    \"\"\"Generate initial research plan with search queries.\"\"\"\n",
                "    question = state[\"question\"]\n",
                "    \n",
                "    prompt = PLAN_QUERIES_PROMPT.format(question=question)\n",
                "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
                "    \n",
                "    # Parse queries from response\n",
                "    queries = [q.strip() for q in response.content.split(\"\\n\") if q.strip() and not q.strip().startswith(\"#\")]\n",
                "    queries = queries[:7]  # Limit to 7 queries\n",
                "    \n",
                "    print(f\"Generated {len(queries)} search queries\")\n",
                "    \n",
                "    return {\n",
                "        \"search_queries\": queries,\n",
                "        \"gate1_attempts\": 0,\n",
                "        \"gate2_attempts\": 0,\n",
                "        \"gate1_passed\": False,\n",
                "        \"gate2_passed\": False\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def execute_searches(state: QualityGateState) -> dict:\n",
                "    \"\"\"Execute all search queries.\"\"\"\n",
                "    queries = state.get(\"search_queries\", [])\n",
                "    \n",
                "    all_results = []\n",
                "    all_urls = []\n",
                "    \n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Executing {len(queries)} searches...\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    for i, query in enumerate(queries):\n",
                "        print(f\"  [{i+1}/{len(queries)}] {query[:60]}...\")\n",
                "        results, urls = search_web(query)\n",
                "        all_results.extend(results)\n",
                "        all_urls.extend(urls)\n",
                "    \n",
                "    print(f\"  Collected {len(all_results)} results from {len(set(all_urls))} unique URLs\")\n",
                "    \n",
                "    return {\n",
                "        \"search_results\": all_results,\n",
                "        \"source_urls\": all_urls\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def quality_gate_1(state: QualityGateState) -> dict:\n",
                "    \"\"\"Quality Gate 1: Validate source quantity and diversity.\"\"\"\n",
                "    urls = state.get(\"source_urls\", [])\n",
                "    attempts = state.get(\"gate1_attempts\", 0) + 1\n",
                "    \n",
                "    # Calculate metrics\n",
                "    unique_urls = list(set(urls))\n",
                "    domains = [extract_domain(url) for url in unique_urls]\n",
                "    unique_domains = list(set(domains))\n",
                "    \n",
                "    print(f\"\\n--- Quality Gate 1 (Attempt {attempts}/{MAX_RETRIES + 1}) ---\")\n",
                "    print(f\"  Unique sources: {len(unique_urls)} (min: {MIN_SOURCES})\")\n",
                "    print(f\"  Unique domains: {len(unique_domains)} (min: {MIN_DOMAIN_DIVERSITY})\")\n",
                "    \n",
                "    # Check gate criteria\n",
                "    passed = len(unique_urls) >= MIN_SOURCES and len(unique_domains) >= MIN_DOMAIN_DIVERSITY\n",
                "    \n",
                "    if passed:\n",
                "        print(f\"  ✓ Gate 1 PASSED\")\n",
                "    else:\n",
                "        print(f\"  ✗ Gate 1 FAILED\")\n",
                "    \n",
                "    return {\n",
                "        \"gate1_attempts\": attempts,\n",
                "        \"gate1_passed\": passed\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def route_after_gate1(state: QualityGateState) -> Literal[\"synthesize\", \"refine_queries\", \"synthesize_anyway\"]:\n",
                "    \"\"\"Route based on Gate 1 result.\"\"\"\n",
                "    passed = state.get(\"gate1_passed\", False)\n",
                "    attempts = state.get(\"gate1_attempts\", 0)\n",
                "    \n",
                "    if passed:\n",
                "        return \"synthesize\"\n",
                "    elif attempts <= MAX_RETRIES:\n",
                "        return \"refine_queries\"\n",
                "    else:\n",
                "        print(f\"  Max retries reached. Proceeding with available sources.\")\n",
                "        return \"synthesize_anyway\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def refine_queries(state: QualityGateState) -> dict:\n",
                "    \"\"\"Refine search queries after Gate 1 failure.\"\"\"\n",
                "    question = state[\"question\"]\n",
                "    previous_queries = state.get(\"search_queries\", [])\n",
                "    urls = state.get(\"source_urls\", [])\n",
                "    \n",
                "    # Analyze gaps\n",
                "    domains = [extract_domain(url) for url in urls]\n",
                "    domain_counts = Counter(domains)\n",
                "    \n",
                "    gap_analysis = f\"Found {len(set(urls))} unique sources from {len(set(domains))} domains. \"\n",
                "    gap_analysis += f\"Most common domains: {dict(domain_counts.most_common(3))}. \"\n",
                "    gap_analysis += \"Need more diverse sources.\"\n",
                "    \n",
                "    prompt = IMPROVE_QUERIES_PROMPT.format(\n",
                "        question=question,\n",
                "        previous_queries=\"\\n\".join(previous_queries),\n",
                "        gap_analysis=gap_analysis\n",
                "    )\n",
                "    \n",
                "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
                "    new_queries = [q.strip() for q in response.content.split(\"\\n\") if q.strip()]\n",
                "    \n",
                "    print(f\"  Generated {len(new_queries)} refined queries for retry\")\n",
                "    \n",
                "    return {\n",
                "        \"search_queries\": new_queries[:3]\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def synthesize(state: QualityGateState) -> dict:\n",
                "    \"\"\"Synthesize search results into coherent analysis.\"\"\"\n",
                "    question = state[\"question\"]\n",
                "    search_results = state.get(\"search_results\", [])\n",
                "    \n",
                "    prompt = SYNTHESIS_PROMPT.format(\n",
                "        question=question,\n",
                "        search_results=\"\\n\\n\".join(search_results[:30])  # Limit to avoid context overflow\n",
                "    )\n",
                "    \n",
                "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
                "    \n",
                "    print(f\"\\nSynthesis complete: {len(response.content)} characters\")\n",
                "    \n",
                "    return {\n",
                "        \"synthesis\": response.content\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def quality_gate_2(state: QualityGateState) -> dict:\n",
                "    \"\"\"Quality Gate 2: Validate synthesis quality.\"\"\"\n",
                "    synthesis = state.get(\"synthesis\", \"\")\n",
                "    question = state[\"question\"]\n",
                "    attempts = state.get(\"gate2_attempts\", 0) + 1\n",
                "    \n",
                "    # Check synthesis quality using LLM\n",
                "    check_prompt = f\"\"\"Evaluate this research synthesis on a scale of 1-10:\n",
                "    \n",
                "Question: {question}\n",
                "\n",
                "Synthesis:\n",
                "{synthesis[:2000]}\n",
                "\n",
                "Rate on:\n",
                "1. Does it address the question? (1-10)\n",
                "2. Is evidence provided? (1-10)\n",
                "3. Is it coherent? (1-10)\n",
                "\n",
                "Respond with ONLY three numbers separated by commas (e.g., \"8, 7, 9\").\"\"\"\n",
                "    \n",
                "    response = await llm.ainvoke([HumanMessage(content=check_prompt)])\n",
                "    \n",
                "    # Parse scores\n",
                "    try:\n",
                "        scores = [int(s.strip()) for s in response.content.split(\",\")][:3]\n",
                "        avg_score = sum(scores) / len(scores)\n",
                "    except:\n",
                "        avg_score = 7  # Default if parsing fails\n",
                "        scores = [7, 7, 7]\n",
                "    \n",
                "    print(f\"\\n--- Quality Gate 2 (Attempt {attempts}/{MAX_RETRIES + 1}) ---\")\n",
                "    print(f\"  Relevance: {scores[0]}/10, Evidence: {scores[1]}/10, Coherence: {scores[2]}/10\")\n",
                "    print(f\"  Average: {avg_score:.1f}/10 (threshold: 6.0)\")\n",
                "    \n",
                "    passed = avg_score >= 6.0\n",
                "    \n",
                "    if passed:\n",
                "        print(f\"  ✓ Gate 2 PASSED\")\n",
                "    else:\n",
                "        print(f\"  ✗ Gate 2 FAILED\")\n",
                "    \n",
                "    return {\n",
                "        \"gate2_attempts\": attempts,\n",
                "        \"gate2_passed\": passed\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def route_after_gate2(state: QualityGateState) -> Literal[\"write_report\", \"additional_research\", \"write_report_anyway\"]:\n",
                "    \"\"\"Route based on Gate 2 result.\"\"\"\n",
                "    passed = state.get(\"gate2_passed\", False)\n",
                "    attempts = state.get(\"gate2_attempts\", 0)\n",
                "    \n",
                "    if passed:\n",
                "        return \"write_report\"\n",
                "    elif attempts <= MAX_RETRIES:\n",
                "        return \"additional_research\"\n",
                "    else:\n",
                "        print(f\"  Max retries reached. Proceeding with current synthesis.\")\n",
                "        return \"write_report_anyway\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def additional_research(state: QualityGateState) -> dict:\n",
                "    \"\"\"Conduct additional research to improve synthesis.\"\"\"\n",
                "    question = state[\"question\"]\n",
                "    synthesis = state.get(\"synthesis\", \"\")\n",
                "    \n",
                "    # Identify gaps in current synthesis\n",
                "    gap_prompt = f\"\"\"Identify what's missing from this research synthesis:\n",
                "    \n",
                "Question: {question}\n",
                "Current Synthesis: {synthesis[:1500]}\n",
                "\n",
                "What specific topics or evidence are missing? Provide 2 focused search queries to fill the gaps.\n",
                "Return ONLY the search queries, one per line.\"\"\"\n",
                "    \n",
                "    response = await llm.ainvoke([HumanMessage(content=gap_prompt)])\n",
                "    gap_queries = [q.strip() for q in response.content.split(\"\\n\") if q.strip()][:2]\n",
                "    \n",
                "    print(f\"\\nAdditional research: {len(gap_queries)} gap-filling queries\")\n",
                "    \n",
                "    return {\n",
                "        \"search_queries\": gap_queries\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def write_report(state: QualityGateState) -> dict:\n",
                "    \"\"\"Write the final research report.\"\"\"\n",
                "    question = state[\"question\"]\n",
                "    synthesis = state.get(\"synthesis\", \"\")\n",
                "    urls = list(set(state.get(\"source_urls\", [])))\n",
                "    \n",
                "    prompt = FINAL_REPORT_PROMPT.format(\n",
                "        question=question,\n",
                "        synthesis=synthesis,\n",
                "        sources=\"\\n\".join(urls[:20])  # Limit sources in prompt\n",
                "    )\n",
                "    \n",
                "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
                "    \n",
                "    print(f\"\\nFinal report generated: {len(response.content)} characters\")\n",
                "    \n",
                "    return {\n",
                "        \"final_report\": response.content\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Graph Construction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build the Quality Gates Research Agent graph\n",
                "qg_builder = StateGraph(QualityGateState)\n",
                "\n",
                "# Add nodes\n",
                "qg_builder.add_node(\"generate_plan\", generate_plan)\n",
                "qg_builder.add_node(\"execute_searches\", execute_searches)\n",
                "qg_builder.add_node(\"quality_gate_1\", quality_gate_1)\n",
                "qg_builder.add_node(\"refine_queries\", refine_queries)\n",
                "qg_builder.add_node(\"synthesize\", synthesize)\n",
                "qg_builder.add_node(\"quality_gate_2\", quality_gate_2)\n",
                "qg_builder.add_node(\"additional_research\", additional_research)\n",
                "qg_builder.add_node(\"write_report\", write_report)\n",
                "\n",
                "# Add edges\n",
                "qg_builder.add_edge(START, \"generate_plan\")\n",
                "qg_builder.add_edge(\"generate_plan\", \"execute_searches\")\n",
                "qg_builder.add_edge(\"execute_searches\", \"quality_gate_1\")\n",
                "\n",
                "# Gate 1 conditional routing\n",
                "qg_builder.add_conditional_edges(\n",
                "    \"quality_gate_1\",\n",
                "    route_after_gate1,\n",
                "    {\n",
                "        \"synthesize\": \"synthesize\",\n",
                "        \"refine_queries\": \"refine_queries\",\n",
                "        \"synthesize_anyway\": \"synthesize\"\n",
                "    }\n",
                ")\n",
                "\n",
                "qg_builder.add_edge(\"refine_queries\", \"execute_searches\")\n",
                "qg_builder.add_edge(\"synthesize\", \"quality_gate_2\")\n",
                "\n",
                "# Gate 2 conditional routing\n",
                "qg_builder.add_conditional_edges(\n",
                "    \"quality_gate_2\",\n",
                "    route_after_gate2,\n",
                "    {\n",
                "        \"write_report\": \"write_report\",\n",
                "        \"additional_research\": \"additional_research\",\n",
                "        \"write_report_anyway\": \"write_report\"\n",
                "    }\n",
                ")\n",
                "\n",
                "qg_builder.add_edge(\"additional_research\", \"execute_searches\")\n",
                "qg_builder.add_edge(\"write_report\", END)\n",
                "\n",
                "# Compile\n",
                "quality_gate_graph = qg_builder.compile()\n",
                "\n",
                "print(\"Quality Gates Research Agent compiled successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the graph\n",
                "from IPython.display import Image, display\n",
                "\n",
                "try:\n",
                "    display(Image(quality_gate_graph.get_graph().draw_mermaid_png()))\n",
                "except Exception as e:\n",
                "    print(f\"Could not display graph: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Agent Wrapper for Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def quality_gates_agent(inputs: dict) -> dict:\n",
                "    \"\"\"\n",
                "    Wrapper function for Quality Gates research agent.\n",
                "    \n",
                "    Compatible with evaluation harness.\n",
                "    \n",
                "    Args:\n",
                "        inputs: Dictionary with 'question' key\n",
                "        \n",
                "    Returns:\n",
                "        Dictionary with 'output' key containing final report\n",
                "    \"\"\"\n",
                "    question = inputs.get(\"question\", \"\")\n",
                "    \n",
                "    # Run with recursion limit\n",
                "    result = asyncio.run(\n",
                "        quality_gate_graph.ainvoke(\n",
                "            {\"question\": question},\n",
                "            config={\"recursion_limit\": 50}\n",
                "        )\n",
                "    )\n",
                "    \n",
                "    return {\n",
                "        \"output\": result.get(\"final_report\", \"\"),\n",
                "        \"synthesis\": result.get(\"synthesis\", \"\"),\n",
                "        \"source_urls\": result.get(\"source_urls\", []),\n",
                "        \"gate1_passed\": result.get(\"gate1_passed\", False),\n",
                "        \"gate2_passed\": result.get(\"gate2_passed\", False)\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Manual Test\n",
                "\n",
                "Run this cell to verify the agent works correctly with a simple test question."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple test\n",
                "test_question = \"What are the key benefits and challenges of using large language models in enterprise applications?\"\n",
                "\n",
                "print(f\"Testing Quality Gates Agent with question:\\n{test_question}\\n\")\n",
                "print(\"Running quality-gated research (this may take several minutes)...\\n\")\n",
                "\n",
                "try:\n",
                "    result = quality_gates_agent({\"question\": test_question})\n",
                "    \n",
                "    print(\"\\n\" + \"=\" * 80)\n",
                "    print(\"FINAL REPORT\")\n",
                "    print(\"=\" * 80)\n",
                "    print(result[\"output\"][:3000] + \"...\" if len(result[\"output\"]) > 3000 else result[\"output\"])\n",
                "    print(\"\\n\" + \"=\" * 80)\n",
                "    print(f\"Report length: {len(result['output'])} characters\")\n",
                "    print(f\"Gate 1 passed: {result.get('gate1_passed', 'N/A')}\")\n",
                "    print(f\"Gate 2 passed: {result.get('gate2_passed', 'N/A')}\")\n",
                "    print(f\"Unique sources: {len(set(result.get('source_urls', [])))}\")\n",
                "    print(\"Agent test PASSED ✓\")\n",
                "except Exception as e:\n",
                "    print(f\"Agent test FAILED: {e}\")\n",
                "    import traceback\n",
                "    traceback.print_exc()\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Evaluation Harness Integration\n",
                "\n",
                "Once the manual test passes, uncomment and run the cells below for full evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import evaluation harness and metrics\n",
                "import sys\n",
                "sys.path.insert(0, \"..\")\n",
                "from evaluation import (\n",
                "    ExperimentHarness, \n",
                "    fact_recall, \n",
                "    citation_precision,\n",
                "    coherence_judge, \n",
                "    depth_judge, \n",
                "    relevance_judge,\n",
                "    minimum_sources_check\n",
                ")\n",
                "\n",
                "# Initialize harness with the golden test dataset\n",
                "harness = ExperimentHarness(\n",
                "    dataset_path=\"../data/deep_research_agent_test_dataset.yaml\",\n",
                "    langsmith_dataset_name=\"deep-research-golden-v2\"\n",
                ")\n",
                "\n",
                "print(\"Evaluation harness initialized successfully!\")\n",
                "print(f\"Dataset: {harness.dataset_path}\")\n",
                "print(f\"LangSmith dataset name: {harness.langsmith_dataset_name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Full Evaluation on All 20 Questions\n",
                "# ⚠️ EXPENSIVE - Only uncomment when ready for full evaluation\n",
                "# Uncomment to run:\n",
                "\n",
                "# # Define comprehensive evaluator suite\n",
                "# evaluators = [\n",
                "#     fact_recall,              # Required facts coverage\n",
                "#     citation_precision,       # Citation URL validity\n",
                "#     minimum_sources_check,    # Minimum source count\n",
                "#     coherence_judge,          # Logical structure\n",
                "#     depth_judge,              # Analysis depth\n",
                "#     relevance_judge,          # Addresses question\n",
                "# ]\n",
                "# \n",
                "# # Run full evaluation\n",
                "# print(\"Starting FULL evaluation on all 20 questions...\")\n",
                "# print(\"Quality Gates Agent - this will take 1-2 hours.\")\n",
                "# print(\"=\" * 80 + \"\\n\")\n",
                "# \n",
                "# results = harness.run_evaluation(\n",
                "#     agent_fn=quality_gates_agent,\n",
                "#     evaluators=evaluators,\n",
                "#     experiment_name=\"quality_gates_v1\",\n",
                "#     monte_carlo_runs=1,  # Single run to reduce cost\n",
                "#     max_concurrency=2,   # Lower concurrency for stability\n",
                "#     description=\"Quality Gates paradigm evaluation on all difficulty tiers\"\n",
                "# )\n",
                "# \n",
                "# # Display comprehensive results\n",
                "# print(\"\\n\" + \"=\" * 80)\n",
                "# print(\"FULL EVALUATION RESULTS\")\n",
                "# print(\"=\" * 80)\n",
                "# print(f\"Experiment: {results.experiment_name}\")\n",
                "# print(f\"Questions evaluated: {results.num_questions}\")\n",
                "# print(f\"Runs per question: {results.num_runs}\")\n",
                "# \n",
                "# print(f\"\\n{'Metric':<30} {'Mean':<10}\")\n",
                "# print(\"-\" * 40)\n",
                "# for metric_name in sorted(results.metrics.keys()):\n",
                "#     if not metric_name.endswith('_std'):\n",
                "#         value = results.metrics.get(metric_name, 0)\n",
                "#         print(f\"{metric_name:<30} {value:<10.3f}\")\n",
                "# \n",
                "# # Save results to file\n",
                "# import json\n",
                "# from datetime import datetime\n",
                "# \n",
                "# results_file = Path(\"../results\") / f\"quality_gates_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
                "# results_file.parent.mkdir(exist_ok=True)\n",
                "# \n",
                "# with open(results_file, 'w') as f:\n",
                "#     json.dump({\n",
                "#         \"experiment_name\": results.experiment_name,\n",
                "#         \"num_questions\": results.num_questions,\n",
                "#         \"num_runs\": results.num_runs,\n",
                "#         \"metrics\": results.metrics,\n",
                "#         \"per_question\": results.per_question_results\n",
                "#     }, f, indent=2)\n",
                "# \n",
                "# print(f\"\\nResults saved to: {results_file}\")\n",
                "\n",
                "print(\"Full evaluation cell ready. Uncomment to run when ready.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}