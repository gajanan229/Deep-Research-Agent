{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8cc3ddf",
   "metadata": {},
   "source": [
    "# Paradigm 08-1: Combined Tier 1 Research Agent\n",
    "\n",
    "This notebook implements the **Combined Tier 1 Architecture** combining:\n",
    "- **Cascading Knowledge Cache** (global search layer)\n",
    "- **Agile Sprints** (research phase, max 2 sprints)\n",
    "- **Quality Gates** (strategic checkpoints)\n",
    "- **Iterative Refinement V2** (document structure + patch-based refinement)\n",
    "\n",
    "## Architecture Phases\n",
    "\n",
    "1. **Phase 1: RESEARCH** (Agile Sprints + Knowledge Cache)\n",
    "2. **Phase 2: STRUCTURE** (Skeleton + Claim Placeholders)\n",
    "3. **Phase 3: EXPANSION** (Node-by-Node Prose)\n",
    "4. **Phase 4: VERIFICATION** (Quality Gates + Patches)\n",
    "5. **Phase 5: FINALIZATION** (Assembly + Polish)\n",
    "\n",
    "## Technology Stack\n",
    "- **LLM**: gpt-5-mini-2025-08-07\n",
    "- **Web Search**: Tavily API\n",
    "- **Embeddings**: OpenAI text-embedding-3-small\n",
    "- **Tracing**: LangSmith\n",
    "- **Framework**: LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293a20a7",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d241269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "import asyncio\n",
    "import hashlib\n",
    "import re\n",
    "import json\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Annotated, TypedDict, Literal, Optional, Any\n",
    "from urllib.parse import urlparse\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from tavily import TavilyClient\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables\n",
    "env_path = Path(\"../.env\")\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Configure LangSmith tracing\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"deep_research_new\"\n",
    "\n",
    "print(\"Environment configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112d2e0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Initialize LLM, Tavily, and Embeddings\n",
    "MODEL_NAME = \"gpt-5-mini-2025-08-07\"\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0, max_retries=10)\n",
    "tavily_client = TavilyClient()\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# ===== CONFIGURATION PARAMETERS =====\n",
    "\n",
    "# Research Phase (Agile Sprints)\n",
    "MAX_RESEARCH_SPRINTS = 2  # Reduced based on user observation\n",
    "QUESTIONS_PER_SPRINT = 4\n",
    "SEARCHES_PER_QUESTION = 3\n",
    "MIN_SOURCES_FOR_GATE1 = 15\n",
    "MIN_DOMAINS_FOR_GATE1 = 5\n",
    "\n",
    "# Skeleton (Iterative Refinement)\n",
    "TARGET_WORDS_PER_NODE = 300\n",
    "MIN_SECTIONS = 5\n",
    "MAX_SECTIONS = 8\n",
    "\n",
    "# Knowledge Cache\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "HIGH_CONFIDENCE_THRESHOLD = 0.75\n",
    "LOW_CONFIDENCE_THRESHOLD = 0.40\n",
    "SPECIFICITY_ADJUSTMENT_FACTOR = 0.2\n",
    "TOP_K_RETRIEVAL = 5\n",
    "\n",
    "# Verification (Quality Gates)\n",
    "MAX_VERIFICATION_ITERATIONS = 2\n",
    "QUALITY_THRESHOLD = 7.5\n",
    "MIN_EVIDENCE_SCORE = 6.0\n",
    "SKIP_CASCADE_ON_FIRST_ITERATION = True\n",
    "\n",
    "# Token Management\n",
    "MAX_CONTEXT_CHARS = 12000\n",
    "MAX_FINDINGS_CHARS = 10000\n",
    "\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "print(f\"Max research sprints: {MAX_RESEARCH_SPRINTS}\")\n",
    "print(f\"Max verification iterations: {MAX_VERIFICATION_ITERATIONS}\")\n",
    "print(f\"Quality threshold: {QUALITY_THRESHOLD}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff07137",
   "metadata": {},
   "source": [
    "## 2. Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b37b0be",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === Knowledge Cache Models ===\n",
    "class CachedDocument(BaseModel):\n",
    "    \"\"\"A cached web document.\"\"\"\n",
    "    url: str = Field(description=\"Original URL\")\n",
    "    normalized_url: str = Field(description=\"Normalized URL for lookup\")\n",
    "    content: str = Field(description=\"Full text content\")\n",
    "    content_hash: str = Field(description=\"SHA-256 hash of content\")\n",
    "    title: str = Field(default=\"\", description=\"Page title\")\n",
    "    retrieval_timestamp: str = Field(description=\"When this was retrieved\")\n",
    "    source_query: str = Field(default=\"\", description=\"Query that led to this content\")\n",
    "\n",
    "\n",
    "class CachedChunk(BaseModel):\n",
    "    \"\"\"A chunk of content with embedding.\"\"\"\n",
    "    chunk_id: str = Field(description=\"Unique identifier\")\n",
    "    text: str = Field(description=\"Chunk text content\")\n",
    "    embedding: List[float] = Field(description=\"Vector embedding\")\n",
    "    source_url: str = Field(description=\"Source document URL\")\n",
    "    position: int = Field(description=\"Position within source document\")\n",
    "\n",
    "\n",
    "class QueryCacheEntry(BaseModel):\n",
    "    \"\"\"A cached query and its results.\"\"\"\n",
    "    original_query: str\n",
    "    light_normalized: str\n",
    "    aggressive_normalized: str\n",
    "    timestamp: str\n",
    "    result_urls: List[str]\n",
    "    result_summary: str\n",
    "\n",
    "\n",
    "class CacheDecision(BaseModel):\n",
    "    \"\"\"Record of a cache decision for observability.\"\"\"\n",
    "    query: str\n",
    "    layer_reached: Literal[\"L1\", \"L2\", \"L3\"]\n",
    "    decision: Literal[\"HIT\", \"HIGH_CONF\", \"MEDIUM_CONF\", \"LOW_CONF\",\n",
    "                      \"SUFFICIENT\", \"PARTIAL\", \"INSUFFICIENT\"]\n",
    "    confidence_score: float = 0.0\n",
    "    action_taken: Literal[\"USE_CACHE\", \"SEARCH\", \"TARGETED_SEARCH\"]\n",
    "    reasoning: str = \"\"\n",
    "    timestamp: str = \"\"\n",
    "\n",
    "\n",
    "class QueryAnalysis(BaseModel):\n",
    "    \"\"\"Analysis of a query's characteristics.\"\"\"\n",
    "    original_query: str\n",
    "    specificity_score: float = Field(description=\"0.0 (general) to 1.0 (very specific)\")\n",
    "    temporal_intent_score: float = Field(description=\"0.0 (no temporal) to 1.0 (time-sensitive)\")\n",
    "    adjusted_high_threshold: float\n",
    "    adjusted_low_threshold: float\n",
    "    extracted_entities: List[str] = Field(default_factory=list)\n",
    "    extracted_dates: List[str] = Field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0804cb0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === Skeleton and Prose Models (from Iterative Refinement) ===\n",
    "class SkeletonNode(BaseModel):\n",
    "    \"\"\"A node in the document skeleton hierarchy.\"\"\"\n",
    "    node_id: str = Field(description=\"Unique identifier like 'sec:intro'\")\n",
    "    title: str = Field(description=\"Section title for the final document\")\n",
    "    intent: str = Field(description=\"1-3 sentence description of what this section should accomplish\")\n",
    "    target_word_count: int = Field(default=300, description=\"Approximate target length\")\n",
    "    dependencies: List[str] = Field(default_factory=list, description=\"Node IDs this section depends on\")\n",
    "    children: List[str] = Field(default_factory=list, description=\"Child node IDs (empty for leaf nodes)\")\n",
    "    is_expanded: bool = Field(default=False, description=\"Whether prose has been generated\")\n",
    "\n",
    "\n",
    "class DocumentSkeleton(BaseModel):\n",
    "    \"\"\"The complete document skeleton structure.\"\"\"\n",
    "    thesis: str = Field(description=\"One-sentence statement of the document's central purpose\")\n",
    "    root_nodes: List[str] = Field(description=\"Top-level section node IDs in document order\")\n",
    "    nodes: Dict[str, SkeletonNode] = Field(default_factory=dict, description=\"All nodes by ID\")\n",
    "    style_constraints: str = Field(default=\"\", description=\"Global style guidelines\")\n",
    "\n",
    "\n",
    "class SkeletonGenerationOutput(BaseModel):\n",
    "    \"\"\"Output schema for skeleton generation.\"\"\"\n",
    "    thesis: str = Field(description=\"One-sentence thesis statement\")\n",
    "    sections: List[SkeletonNode] = Field(description=\"All sections in document order\")\n",
    "\n",
    "\n",
    "class ProseEntry(BaseModel):\n",
    "    \"\"\"Content stored for each expanded node.\"\"\"\n",
    "    node_id: str = Field(description=\"The skeleton node this prose belongs to\")\n",
    "    main_content: str = Field(description=\"The substantive prose for this section\")\n",
    "    bridge_in: str = Field(default=\"\", description=\"Transitional sentences connecting from previous section\")\n",
    "    bridge_out: str = Field(default=\"\", description=\"Transitional sentences leading to next section\")\n",
    "    summary: str = Field(default=\"\", description=\"1-2 sentence compression of content\")\n",
    "    revision_count: int = Field(default=0, description=\"How many times this node has been revised\")\n",
    "\n",
    "\n",
    "class ProseGenerationOutput(BaseModel):\n",
    "    \"\"\"Output schema for prose generation.\"\"\"\n",
    "    bridge_in: str = Field(description=\"1-2 transitional sentences\")\n",
    "    main_content: str = Field(description=\"The main prose content for this section\")\n",
    "    bridge_out: str = Field(description=\"1-2 transitional sentences leading to next section\")\n",
    "    summary: str = Field(description=\"1-2 sentence summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d76eb5a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === Claims Models ===\n",
    "class Claim(BaseModel):\n",
    "    \"\"\"A verifiable assertion in the document.\"\"\"\n",
    "    claim_id: str = Field(description=\"Unique identifier for this claim\")\n",
    "    claim_text: str = Field(description=\"The assertion itself, stated precisely\")\n",
    "    source_node: str = Field(description=\"Skeleton node ID where this claim appears\")\n",
    "    verification_status: Literal[\"placeholder\", \"unverified\", \"verified\", \"contested\", \"retracted\"] = Field(\n",
    "        default=\"unverified\", description=\"Current verification state\"\n",
    "    )\n",
    "    supporting_evidence: List[str] = Field(default_factory=list, description=\"Sources supporting this claim\")\n",
    "    claim_dependencies: List[str] = Field(default_factory=list, description=\"Other claim IDs this depends on\")\n",
    "\n",
    "\n",
    "class ClaimExtractionOutput(BaseModel):\n",
    "    \"\"\"Output schema for claim extraction.\"\"\"\n",
    "    claims: List[Claim] = Field(description=\"All factual claims extracted from the prose\")\n",
    "\n",
    "\n",
    "class ClaimPlaceholderOutput(BaseModel):\n",
    "    \"\"\"Output schema for identifying claim placeholders from skeleton.\"\"\"\n",
    "    claims: List[Claim] = Field(description=\"Placeholder claims to research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66927309",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === Critique Models ===\n",
    "class CritiqueIssue(BaseModel):\n",
    "    \"\"\"An issue identified during critique.\"\"\"\n",
    "    issue_id: str = Field(description=\"Unique identifier\")\n",
    "    scope: Literal[\"global\", \"section\", \"transition\"] = Field(description=\"Level of the issue\")\n",
    "    target_nodes: List[str] = Field(description=\"Affected skeleton node IDs\")\n",
    "    issue_type: Literal[\"weak_claim\", \"missing_evidence\", \"logical_gap\", \"unclear\", \"coherence\", \"depth\", \"transition\"] = Field(\n",
    "        description=\"Category of issue\"\n",
    "    )\n",
    "    severity: Literal[\"critical\", \"major\", \"minor\"] = Field(description=\"How serious the issue is\")\n",
    "    affected_claims: List[str] = Field(default_factory=list, description=\"Claim IDs affected\")\n",
    "    description: str = Field(description=\"What the problem is\")\n",
    "    suggestion: str = Field(description=\"How to fix it\")\n",
    "    search_query: str = Field(default=\"\", description=\"Specific query to find evidence\")\n",
    "\n",
    "\n",
    "class CritiqueResult(BaseModel):\n",
    "    \"\"\"Complete critique output.\"\"\"\n",
    "    overall_quality: float = Field(description=\"Quality score 1-10\")\n",
    "    issues: List[CritiqueIssue] = Field(default_factory=list, description=\"All identified issues\")\n",
    "    strengths: str = Field(default=\"\", description=\"What the document does well\")\n",
    "    summary: str = Field(description=\"Overall assessment\")\n",
    "\n",
    "\n",
    "class QualityScores(BaseModel):\n",
    "    \"\"\"Multi-dimensional quality scores for Gate 2 (from V1).\"\"\"\n",
    "    question_coverage: float = Field(description=\"1-10: Does it address ALL parts of the question?\")\n",
    "    evidence_quality: float = Field(description=\"1-10: Are major claims supported by cited sources?\")\n",
    "    coherence: float = Field(description=\"1-10: Is the argument logical and well-structured?\")\n",
    "    depth: float = Field(description=\"1-10: Does it go beyond surface-level information?\")\n",
    "    overall: float = Field(description=\"Average of all scores\")\n",
    "    passed: bool = Field(description=\"True if overall >= 7.5 AND evidence_quality >= 6\")\n",
    "    weak_claims: List[str] = Field(default_factory=list, description=\"Claims needing more support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f5abc",
   "metadata": {},
   "source": [
    "## 3. Knowledge Base Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d117831",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class KnowledgeBase:\n",
    "    \"\"\"Session-scoped knowledge base with cascading cache capabilities.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.url_registry: Dict[str, CachedDocument] = {}\n",
    "        self.query_cache: Dict[str, QueryCacheEntry] = {}\n",
    "        self.chunks: List[CachedChunk] = []\n",
    "        self.chunk_embeddings: Optional[np.ndarray] = None\n",
    "\n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            \"total_queries\": 0,\n",
    "            \"l1_hits\": 0,\n",
    "            \"l2_high\": 0,\n",
    "            \"l2_medium\": 0,\n",
    "            \"l2_low\": 0,\n",
    "            \"l3_sufficient\": 0,\n",
    "            \"l3_partial\": 0,\n",
    "            \"l3_insufficient\": 0,\n",
    "            \"web_searches_executed\": 0,\n",
    "            \"web_searches_avoided\": 0\n",
    "        }\n",
    "\n",
    "    def normalize_url(self, url: str) -> str:\n",
    "        \"\"\"Normalize URL for consistent lookup.\"\"\"\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            host = parsed.netloc.lower()\n",
    "            if host.startswith(\"www.\"):\n",
    "                host = host[4:]\n",
    "            path = parsed.path.rstrip(\"/\")\n",
    "            query_params = sorted(parsed.query.split(\"&\")) if parsed.query else []\n",
    "            tracking_params = {\"utm_source\", \"utm_medium\", \"utm_campaign\", \"ref\", \"fbclid\"}\n",
    "            query_params = [p for p in query_params if p.split(\"=\")[0] not in tracking_params]\n",
    "            query = \"&\".join(query_params)\n",
    "            normalized = f\"https://{host}{path}\"\n",
    "            if query:\n",
    "                normalized += f\"?{query}\"\n",
    "            return normalized\n",
    "        except:\n",
    "            return url.lower()\n",
    "\n",
    "    def normalize_query_light(self, query: str) -> str:\n",
    "        \"\"\"Light normalization: lowercase, collapse whitespace.\"\"\"\n",
    "        return \" \".join(query.lower().split())\n",
    "\n",
    "    def normalize_query_aggressive(self, query: str) -> str:\n",
    "        \"\"\"Aggressive normalization: remove stop words, sort terms.\"\"\"\n",
    "        stop_words = {\"the\", \"a\", \"an\", \"is\", \"are\", \"of\", \"in\", \"to\", \"for\", \"and\", \"or\", \"what\", \"how\", \"why\", \"when\", \"where\"}\n",
    "        light = self.normalize_query_light(query)\n",
    "        terms = [t for t in light.split() if t not in stop_words and len(t) > 1]\n",
    "        return \" \".join(sorted(terms))\n",
    "\n",
    "    def compute_content_hash(self, content: str) -> str:\n",
    "        \"\"\"Compute SHA-256 hash of content.\"\"\"\n",
    "        return hashlib.sha256(content.encode()).hexdigest()\n",
    "\n",
    "    def add_document(self, url: str, content: str, title: str = \"\", source_query: str = \"\"):\n",
    "        \"\"\"Add a document to the knowledge base.\"\"\"\n",
    "        normalized_url = self.normalize_url(url)\n",
    "        doc = CachedDocument(\n",
    "            url=url,\n",
    "            normalized_url=normalized_url,\n",
    "            content=content,\n",
    "            content_hash=self.compute_content_hash(content),\n",
    "            title=title,\n",
    "            retrieval_timestamp=datetime.now().isoformat(),\n",
    "            source_query=source_query\n",
    "        )\n",
    "        self.url_registry[normalized_url] = doc\n",
    "        self._chunk_and_embed(doc)\n",
    "        return doc\n",
    "\n",
    "    def _chunk_and_embed(self, doc: CachedDocument):\n",
    "        \"\"\"Chunk document and compute embeddings.\"\"\"\n",
    "        content = doc.content\n",
    "        chunks_text = []\n",
    "        for i in range(0, len(content), CHUNK_SIZE - CHUNK_OVERLAP):\n",
    "            chunk_text = content[i:i + CHUNK_SIZE]\n",
    "            if len(chunk_text) > 50:\n",
    "                chunks_text.append(chunk_text)\n",
    "        if not chunks_text:\n",
    "            return\n",
    "        embeddings = embeddings_model.embed_documents(chunks_text)\n",
    "        for i, (text, embedding) in enumerate(zip(chunks_text, embeddings)):\n",
    "            chunk = CachedChunk(\n",
    "                chunk_id=f\"{doc.content_hash[:8]}_{i}\",\n",
    "                text=text,\n",
    "                embedding=embedding,\n",
    "                source_url=doc.url,\n",
    "                position=i\n",
    "            )\n",
    "            self.chunks.append(chunk)\n",
    "        self._update_embedding_matrix()\n",
    "\n",
    "    def _update_embedding_matrix(self):\n",
    "        \"\"\"Update the numpy matrix of embeddings for fast search.\"\"\"\n",
    "        if self.chunks:\n",
    "            self.chunk_embeddings = np.array([c.embedding for c in self.chunks])\n",
    "\n",
    "    def add_query(self, query: str, result_urls: List[str], result_summary: str):\n",
    "        \"\"\"Add a query to the cache.\"\"\"\n",
    "        entry = QueryCacheEntry(\n",
    "            original_query=query,\n",
    "            light_normalized=self.normalize_query_light(query),\n",
    "            aggressive_normalized=self.normalize_query_aggressive(query),\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            result_urls=result_urls,\n",
    "            result_summary=result_summary\n",
    "        )\n",
    "        self.query_cache[entry.light_normalized] = entry\n",
    "        self.query_cache[entry.aggressive_normalized] = entry\n",
    "        return entry\n",
    "\n",
    "    def lookup_query_exact(self, query: str) -> Optional[QueryCacheEntry]:\n",
    "        \"\"\"Check for exact query match.\"\"\"\n",
    "        light = self.normalize_query_light(query)\n",
    "        return self.query_cache.get(light)\n",
    "\n",
    "    def lookup_query_aggressive(self, query: str) -> Optional[QueryCacheEntry]:\n",
    "        \"\"\"Check for bag-of-words query match.\"\"\"\n",
    "        aggressive = self.normalize_query_aggressive(query)\n",
    "        return self.query_cache.get(aggressive)\n",
    "\n",
    "    def semantic_search(self, query: str, top_k: int = TOP_K_RETRIEVAL) -> List[Tuple[CachedChunk, float]]:\n",
    "        \"\"\"Find semantically similar chunks.\"\"\"\n",
    "        if not self.chunks or self.chunk_embeddings is None:\n",
    "            return []\n",
    "        query_embedding = np.array(embeddings_model.embed_query(query))\n",
    "        similarities = np.dot(self.chunk_embeddings, query_embedding) / (\n",
    "            np.linalg.norm(self.chunk_embeddings, axis=1) * np.linalg.norm(query_embedding) + 1e-8\n",
    "        )\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append((self.chunks[idx], float(similarities[idx])))\n",
    "        return results\n",
    "\n",
    "    def get_stats_summary(self) -> str:\n",
    "        \"\"\"Get human-readable stats summary.\"\"\"\n",
    "        total = self.stats[\"total_queries\"]\n",
    "        if total == 0:\n",
    "            return \"No queries processed yet.\"\n",
    "        avoided = self.stats[\"web_searches_avoided\"]\n",
    "        executed = self.stats[\"web_searches_executed\"]\n",
    "        hit_rate = avoided / total * 100 if total > 0 else 0\n",
    "        return f\"\"\"\n",
    "Cache Statistics:\n",
    "- Total queries: {total}\n",
    "- Web searches avoided: {avoided} ({hit_rate:.1f}% hit rate)\n",
    "- Web searches executed: {executed}\n",
    "- Documents cached: {len(self.url_registry)}\n",
    "- Chunks indexed: {len(self.chunks)}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Initialize global knowledge base (session-scoped)\n",
    "knowledge_base = KnowledgeBase()\n",
    "print(\"Knowledge base initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbdf516",
   "metadata": {},
   "source": [
    "## 4. Combined State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5caea51",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CombinedTier1State(TypedDict):\n",
    "    \"\"\"State for the Combined Tier 1 Research Agent.\"\"\"\n",
    "    # Input\n",
    "    question: str\n",
    "\n",
    "    # ===== PHASE 1: Research Sprints =====\n",
    "    research_backlog: List[str]  # REPLACED each sprint\n",
    "    current_research_sprint: int\n",
    "    max_research_sprints: int\n",
    "    sprint_findings: Annotated[List[str], operator.add]  # ACCUMULATED\n",
    "    research_source_urls: Annotated[List[str], operator.add]\n",
    "    research_retrospective_notes: Annotated[List[str], operator.add]\n",
    "    research_brief: str  # Compressed findings\n",
    "    gate1_passed: bool\n",
    "    gate1_attempts: int\n",
    "\n",
    "    # ===== PHASE 2: Skeleton + Claims =====\n",
    "    skeleton: Dict[str, Any]  # DocumentSkeleton as dict\n",
    "    skeleton_validated: bool\n",
    "    claims_registry: Dict[str, Dict[str, Any]]  # claim_id -> Claim\n",
    "\n",
    "    # ===== PHASE 3: Node Expansion =====\n",
    "    prose_store: Dict[str, Dict[str, Any]]  # node_id -> ProseEntry\n",
    "    nodes_expanded: List[str]\n",
    "    assembled_draft: str\n",
    "\n",
    "    # ===== PHASE 4: Verification =====\n",
    "    noise_map: List[Dict[str, Any]]  # CritiqueIssue list\n",
    "    nodes_to_patch: List[str]\n",
    "    targeted_evidence: Dict[str, List[str]]  # node_id -> evidence\n",
    "    current_verification_iteration: int\n",
    "    max_verification_iterations: int\n",
    "    quality_scores: Annotated[List[Dict[str, float]], operator.add]\n",
    "    gate2_passed: bool\n",
    "    limitations_noted: List[str]\n",
    "\n",
    "    # ===== PHASE 5: Output =====\n",
    "    final_report: str\n",
    "\n",
    "    # ===== METRICS =====\n",
    "    total_searches: int\n",
    "    cache_hits: int\n",
    "    cache_decisions: Annotated[List[Dict], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701557e2",
   "metadata": {},
   "source": [
    "## 5. Query Analysis and Cache Layer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74b4c59",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_specificity(query: str) -> float:\n",
    "    \"\"\"Compute query specificity score (0.0 to 1.0).\"\"\"\n",
    "    score = 0.0\n",
    "    if re.search(r'\\b\\d{4}\\b', query):\n",
    "        score += 0.2\n",
    "    if re.search(r'\\d+%|\\$\\d+|\\d+\\s*(billion|million|thousand)', query, re.IGNORECASE):\n",
    "        score += 0.2\n",
    "    if re.search(r'Q[1-4]\\s*\\d{4}|FY\\d{4}', query, re.IGNORECASE):\n",
    "        score += 0.2\n",
    "    proper_nouns = re.findall(r'(?<!^)(?<!\\. )[A-Z][a-z]+', query)\n",
    "    if len(proper_nouns) > 1:\n",
    "        score += 0.15\n",
    "    if '\"' in query or \"'\" in query:\n",
    "        score += 0.15\n",
    "    if re.search(r'\\b(exact|precise|specific|exactly|how many|what is the)\\b', query, re.IGNORECASE):\n",
    "        score += 0.1\n",
    "    return min(score, 1.0)\n",
    "\n",
    "\n",
    "def compute_temporal_intent(query: str) -> float:\n",
    "    \"\"\"Compute temporal intent score (0.0 to 1.0).\"\"\"\n",
    "    score = 0.0\n",
    "    if re.search(r'\\b(current|latest|now|today|recent|present|this week|this month|this year)\\b', query, re.IGNORECASE):\n",
    "        score += 0.4\n",
    "    if re.search(r'\\b(who is the|what is the current|is .+ still)\\b', query, re.IGNORECASE):\n",
    "        score += 0.3\n",
    "    if re.search(r'\\b(how has .+ changed|compared to|versus last)\\b', query, re.IGNORECASE):\n",
    "        score += 0.2\n",
    "    if re.search(r'\\b(stock price|election|weather|score|breaking)\\b', query, re.IGNORECASE):\n",
    "        score += 0.3\n",
    "    return min(score, 1.0)\n",
    "\n",
    "\n",
    "def analyze_query(query: str) -> QueryAnalysis:\n",
    "    \"\"\"Perform full query analysis.\"\"\"\n",
    "    specificity = compute_specificity(query)\n",
    "    temporal_intent = compute_temporal_intent(query)\n",
    "    high_adjustment = specificity * SPECIFICITY_ADJUSTMENT_FACTOR\n",
    "    adjusted_high = min(HIGH_CONFIDENCE_THRESHOLD + high_adjustment, 0.95)\n",
    "    adjusted_low = LOW_CONFIDENCE_THRESHOLD\n",
    "    entities = re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b', query)\n",
    "    dates = re.findall(r'\\b\\d{4}\\b|\\bQ[1-4]\\s*\\d{4}\\b', query, re.IGNORECASE)\n",
    "    return QueryAnalysis(\n",
    "        original_query=query,\n",
    "        specificity_score=specificity,\n",
    "        temporal_intent_score=temporal_intent,\n",
    "        adjusted_high_threshold=adjusted_high,\n",
    "        adjusted_low_threshold=adjusted_low,\n",
    "        extracted_entities=entities[:10],\n",
    "        extracted_dates=dates[:5]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff65dc31",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_layer1(query: str, kb: KnowledgeBase) -> Tuple[Literal[\"HIT\", \"MISS\"], Optional[str], Optional[List[str]]]:\n",
    "    \"\"\"Layer 1: Deterministic deduplication.\"\"\"\n",
    "    exact_match = kb.lookup_query_exact(query)\n",
    "    if exact_match:\n",
    "        return \"HIT\", exact_match.result_summary, exact_match.result_urls\n",
    "    aggressive_match = kb.lookup_query_aggressive(query)\n",
    "    if aggressive_match:\n",
    "        return \"HIT\", aggressive_match.result_summary, aggressive_match.result_urls\n",
    "    return \"MISS\", None, None\n",
    "\n",
    "\n",
    "def compute_confidence(\n",
    "    top_results: List[Tuple[CachedChunk, float]],\n",
    "    query: str,\n",
    "    analysis: QueryAnalysis\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"Compute multi-signal confidence score.\"\"\"\n",
    "    if not top_results:\n",
    "        return 0.0, {}\n",
    "    top_score = top_results[0][1]\n",
    "    score_gap = top_results[0][1] - top_results[1][1] if len(top_results) > 1 else top_score\n",
    "    query_terms = set(query.lower().split())\n",
    "    top_chunk_terms = set(top_results[0][0].text.lower().split())\n",
    "    term_overlap = len(query_terms & top_chunk_terms) / len(query_terms | top_chunk_terms) if query_terms | top_chunk_terms else 0\n",
    "    weights = {\"top_score\": 0.5, \"score_gap\": 0.25, \"term_overlap\": 0.25}\n",
    "    raw_confidence = (\n",
    "        weights[\"top_score\"] * top_score +\n",
    "        weights[\"score_gap\"] * min(score_gap * 2, 1.0) +\n",
    "        weights[\"term_overlap\"] * term_overlap\n",
    "    )\n",
    "    confidence = max(0.0, min(raw_confidence, 1.0))\n",
    "    signals = {\n",
    "        \"top_score\": top_score,\n",
    "        \"score_gap\": score_gap,\n",
    "        \"term_overlap\": term_overlap,\n",
    "        \"final_confidence\": confidence\n",
    "    }\n",
    "    return confidence, signals\n",
    "\n",
    "\n",
    "def check_layer2(\n",
    "    query: str,\n",
    "    kb: KnowledgeBase,\n",
    "    analysis: QueryAnalysis\n",
    ") -> Tuple[Literal[\"HIGH\", \"MEDIUM\", \"LOW\"], float, List[Tuple[CachedChunk, float]], Dict]:\n",
    "    \"\"\"Layer 2: Semantic retrieval with confidence scoring.\"\"\"\n",
    "    results = kb.semantic_search(query, top_k=TOP_K_RETRIEVAL)\n",
    "    if not results:\n",
    "        return \"LOW\", 0.0, [], {}\n",
    "    confidence, signals = compute_confidence(results, query, analysis)\n",
    "    if confidence >= analysis.adjusted_high_threshold:\n",
    "        decision = \"HIGH\"\n",
    "    elif confidence >= analysis.adjusted_low_threshold:\n",
    "        decision = \"MEDIUM\"\n",
    "    else:\n",
    "        decision = \"LOW\"\n",
    "    return decision, confidence, results, signals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274986d",
   "metadata": {},
   "source": [
    "## 6. Layer 3 and Cascaded Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0200c93e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "LAYER3_JUDGMENT_PROMPT = \"\"\"You are evaluating whether cached content answers a research query.\n",
    "\n",
    "QUERY: {query}\n",
    "\n",
    "CACHED CONTENT (top {num_chunks} relevant chunks):\n",
    "{chunks_text}\n",
    "\n",
    "Analyze on these dimensions:\n",
    "1. TOPICAL RELEVANCE: Is the cached content about the same subject?\n",
    "2. SPECIFICITY MATCH: Does it address the specific aspect asked about?\n",
    "3. COMPLETENESS: Does it provide a complete or only partial answer?\n",
    "4. FACTUAL DENSITY: Does it contain concrete facts that answer the query?\n",
    "\n",
    "Provide your assessment in this exact format:\n",
    "VERDICT: [SUFFICIENT|PARTIAL|INSUFFICIENT]\n",
    "RELEVANCE: [0.0-1.0]\n",
    "COMPLETENESS: [0.0-1.0]\n",
    "REASONING: [Your explanation in 2-3 sentences]\n",
    "GAPS: [List specific missing information, or \"None\" if sufficient]\n",
    "\"\"\"\n",
    "\n",
    "GAP_ANALYSIS_PROMPT = \"\"\"Based on your analysis, the cached content only partially answers the query.\n",
    "\n",
    "QUERY: {query}\n",
    "IDENTIFIED GAPS: {gaps}\n",
    "\n",
    "Generate 1-2 highly targeted search queries that would fill these specific gaps.\n",
    "The refined queries should:\n",
    "- Target ONLY the missing information (not repeat what we already have)\n",
    "- Be specific and searchable\n",
    "- Different from the original query\n",
    "\n",
    "Return queries one per line, no numbering or bullets.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "async def check_layer3(\n",
    "    query: str,\n",
    "    chunks: List[Tuple[CachedChunk, float]],\n",
    "    kb: KnowledgeBase\n",
    ") -> Tuple[Literal[\"SUFFICIENT\", \"PARTIAL\", \"INSUFFICIENT\"], List[str], Optional[List[str]]]:\n",
    "    \"\"\"Layer 3: LLM-augmented judgment with gap analysis.\"\"\"\n",
    "    chunks_text = \"\\n\\n---\\n\\n\".join([\n",
    "        f\"[Chunk {i+1}, similarity: {score:.3f}]\\n{chunk.text}\"\n",
    "        for i, (chunk, score) in enumerate(chunks[:5])\n",
    "    ])\n",
    "    prompt = LAYER3_JUDGMENT_PROMPT.format(\n",
    "        query=query,\n",
    "        num_chunks=min(len(chunks), 5),\n",
    "        chunks_text=chunks_text\n",
    "    )\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "    content = response.content\n",
    "    verdict_match = re.search(r'VERDICT:\\s*(SUFFICIENT|PARTIAL|INSUFFICIENT)', content, re.IGNORECASE)\n",
    "    verdict = verdict_match.group(1).upper() if verdict_match else \"INSUFFICIENT\"\n",
    "    gaps = []\n",
    "    gaps_match = re.search(r'GAPS:\\s*(.+?)(?=\\n\\n|$)', content, re.DOTALL | re.IGNORECASE)\n",
    "    if gaps_match:\n",
    "        gaps_text = gaps_match.group(1).strip()\n",
    "        if gaps_text.lower() != \"none\":\n",
    "            gaps = [g.strip() for g in re.split(r'[-â€¢\\n]', gaps_text) if g.strip()]\n",
    "    refined_queries = None\n",
    "    if verdict == \"PARTIAL\" and gaps:\n",
    "        refine_prompt = GAP_ANALYSIS_PROMPT.format(\n",
    "            query=query,\n",
    "            gaps=\"\\n\".join(f\"- {g}\" for g in gaps)\n",
    "        )\n",
    "        refine_response = await llm.ainvoke([HumanMessage(content=refine_prompt)])\n",
    "        refined_queries = [q.strip() for q in refine_response.content.split(\"\\n\") if q.strip()][:2]\n",
    "    return verdict, gaps, refined_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87036f5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def search_web(query: str, max_results: int = 8) -> Tuple[str, List[str], List[str]]:\n",
    "    \"\"\"Execute web search using Tavily. Returns (summary, results, urls).\"\"\"\n",
    "    try:\n",
    "        if len(query) > 400:\n",
    "            query = query[:400]\n",
    "        response = tavily_client.search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            include_answer=True\n",
    "        )\n",
    "        results = []\n",
    "        urls = []\n",
    "        summary = response.get(\"answer\", \"\")\n",
    "        for r in response.get(\"results\", []):\n",
    "            url = r.get('url', '')\n",
    "            urls.append(url)\n",
    "            content = r.get('content', '')[:500]\n",
    "            title = r.get('title', 'No title')\n",
    "            results.append(f\"[{title}] {content}... (Source: {url})\")\n",
    "        return summary, results, urls\n",
    "    except Exception as e:\n",
    "        return f\"Search error: {str(e)}\", [], []\n",
    "\n",
    "\n",
    "async def cascaded_search(\n",
    "    query: str,\n",
    "    kb: KnowledgeBase\n",
    ") -> Tuple[str, List[str], CacheDecision]:\n",
    "    \"\"\"Execute the full cascading cache check and search if needed.\"\"\"\n",
    "    kb.stats[\"total_queries\"] += 1\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    analysis = analyze_query(query)\n",
    "    \n",
    "    # Layer 1\n",
    "    l1_decision, l1_summary, l1_urls = check_layer1(query, kb)\n",
    "    if l1_decision == \"HIT\":\n",
    "        kb.stats[\"l1_hits\"] += 1\n",
    "        kb.stats[\"web_searches_avoided\"] += 1\n",
    "        decision = CacheDecision(\n",
    "            query=query, layer_reached=\"L1\", decision=\"HIT\",\n",
    "            confidence_score=1.0, action_taken=\"USE_CACHE\",\n",
    "            reasoning=\"Exact query match found in cache\", timestamp=timestamp\n",
    "        )\n",
    "        return l1_summary, l1_urls, decision\n",
    "    \n",
    "    # Layer 2\n",
    "    l2_decision, confidence, chunks, signals = check_layer2(query, kb, analysis)\n",
    "    \n",
    "    if l2_decision == \"HIGH\":\n",
    "        kb.stats[\"l2_high\"] += 1\n",
    "        kb.stats[\"web_searches_avoided\"] += 1\n",
    "        content = \"\\n\\n\".join([f\"[From: {c.source_url}]\\n{c.text}\" for c, _ in chunks[:3]])\n",
    "        urls = list(set([c.source_url for c, _ in chunks]))\n",
    "        decision = CacheDecision(\n",
    "            query=query, layer_reached=\"L2\", decision=\"HIGH_CONF\",\n",
    "            confidence_score=confidence, action_taken=\"USE_CACHE\",\n",
    "            reasoning=f\"High semantic similarity (conf={confidence:.3f})\", timestamp=timestamp\n",
    "        )\n",
    "        return content, urls, decision\n",
    "    \n",
    "    elif l2_decision == \"LOW\":\n",
    "        kb.stats[\"l2_low\"] += 1\n",
    "        summary, results, urls = search_web(query)\n",
    "        kb.stats[\"web_searches_executed\"] += 1\n",
    "        query_content = f\"Query: {query}\\n\\nAnswer: {summary}\\n\\nResults:\\n\" + \"\\n\\n\".join(results)\n",
    "        synthetic_url = f\"search://{kb.compute_content_hash(query)[:16]}\"\n",
    "        kb.add_document(synthetic_url, query_content, title=f\"Search: {query[:50]}\", source_query=query)\n",
    "        kb.add_query(query, urls, summary)\n",
    "        decision = CacheDecision(\n",
    "            query=query, layer_reached=\"L2\", decision=\"LOW_CONF\",\n",
    "            confidence_score=confidence, action_taken=\"SEARCH\",\n",
    "            reasoning=f\"Low confidence ({confidence:.3f}), executed web search\", timestamp=timestamp\n",
    "        )\n",
    "        return query_content, urls, decision\n",
    "    \n",
    "    # Layer 3 (MEDIUM confidence)\n",
    "    kb.stats[\"l2_medium\"] += 1\n",
    "    verdict, gaps, refined_queries = await check_layer3(query, chunks, kb)\n",
    "    \n",
    "    if verdict == \"SUFFICIENT\":\n",
    "        kb.stats[\"l3_sufficient\"] += 1\n",
    "        kb.stats[\"web_searches_avoided\"] += 1\n",
    "        content = \"\\n\\n\".join([f\"[From: {c.source_url}]\\n{c.text}\" for c, _ in chunks[:3]])\n",
    "        urls = list(set([c.source_url for c, _ in chunks]))\n",
    "        decision = CacheDecision(\n",
    "            query=query, layer_reached=\"L3\", decision=\"SUFFICIENT\",\n",
    "            confidence_score=confidence, action_taken=\"USE_CACHE\",\n",
    "            reasoning=\"LLM judged cached content sufficient\", timestamp=timestamp\n",
    "        )\n",
    "        return content, urls, decision\n",
    "    \n",
    "    elif verdict == \"PARTIAL\" and refined_queries:\n",
    "        kb.stats[\"l3_partial\"] += 1\n",
    "        cached_content = \"\\n\\n\".join([f\"[Cached: {c.source_url}]\\n{c.text}\" for c, _ in chunks[:2]])\n",
    "        cached_urls = [c.source_url for c, _ in chunks[:2]]\n",
    "        for refined_query in refined_queries:\n",
    "            summary, results, new_urls = search_web(refined_query, max_results=4)\n",
    "            kb.stats[\"web_searches_executed\"] += 1\n",
    "            gap_content = f\"Query: {refined_query}\\n\\nAnswer: {summary}\\n\\nResults:\\n\" + \"\\n\\n\".join(results)\n",
    "            synthetic_url = f\"search://{kb.compute_content_hash(refined_query)[:16]}\"\n",
    "            kb.add_document(synthetic_url, gap_content, title=f\"Search: {refined_query[:50]}\", source_query=refined_query)\n",
    "            kb.add_query(refined_query, new_urls, summary)\n",
    "            cached_content += f\"\\n\\n[Gap-fill search: {refined_query}]\\n{gap_content}\"\n",
    "            cached_urls.extend(new_urls)\n",
    "        decision = CacheDecision(\n",
    "            query=query, layer_reached=\"L3\", decision=\"PARTIAL\",\n",
    "            confidence_score=confidence, action_taken=\"TARGETED_SEARCH\",\n",
    "            reasoning=f\"LLM identified gaps. Executed {len(refined_queries)} targeted searches.\", timestamp=timestamp\n",
    "        )\n",
    "        return cached_content, list(set(cached_urls)), decision\n",
    "    \n",
    "    else:\n",
    "        kb.stats[\"l3_insufficient\"] += 1\n",
    "        summary, results, urls = search_web(query)\n",
    "        kb.stats[\"web_searches_executed\"] += 1\n",
    "        query_content = f\"Query: {query}\\n\\nAnswer: {summary}\\n\\nResults:\\n\" + \"\\n\\n\".join(results)\n",
    "        synthetic_url = f\"search://{kb.compute_content_hash(query)[:16]}\"\n",
    "        kb.add_document(synthetic_url, query_content, title=f\"Search: {query[:50]}\", source_query=query)\n",
    "        kb.add_query(query, urls, summary)\n",
    "        decision = CacheDecision(\n",
    "            query=query, layer_reached=\"L3\", decision=\"INSUFFICIENT\",\n",
    "            confidence_score=confidence, action_taken=\"SEARCH\",\n",
    "            reasoning=\"LLM judged cached content insufficient\", timestamp=timestamp\n",
    "        )\n",
    "        return query_content, urls, decision\n",
    "\n",
    "print(\"Cache layer functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54c52a8",
   "metadata": {},
   "source": [
    "## 7. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06410bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain(url: str) -> str:\n",
    "    \"\"\"Extract domain from URL.\"\"\"\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        return parsed.netloc\n",
    "    except:\n",
    "        return url\n",
    "\n",
    "\n",
    "def get_leaf_nodes(skeleton: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"Get all leaf node IDs (nodes without children) in document order.\"\"\"\n",
    "    nodes = skeleton.get(\"nodes\", {})\n",
    "    root_nodes = skeleton.get(\"root_nodes\", [])\n",
    "\n",
    "    def collect_leaves(node_ids: List[str]) -> List[str]:\n",
    "        leaves = []\n",
    "        for nid in node_ids:\n",
    "            node = nodes.get(nid, {})\n",
    "            children = node.get(\"children\", [])\n",
    "            if not children:\n",
    "                leaves.append(nid)\n",
    "            else:\n",
    "                leaves.extend(collect_leaves(children))\n",
    "        return leaves\n",
    "    return collect_leaves(root_nodes)\n",
    "\n",
    "\n",
    "def topological_sort_nodes(skeleton: Dict[str, Any], node_ids: List[str]) -> List[str]:\n",
    "    \"\"\"Sort nodes by dependency order.\"\"\"\n",
    "    nodes = skeleton.get(\"nodes\", {})\n",
    "    node_id_set = set(node_ids)\n",
    "    remaining = set(node_ids)\n",
    "    sorted_nodes = []\n",
    "    while remaining:\n",
    "        ready = []\n",
    "        for nid in remaining:\n",
    "            node = nodes.get(nid, {})\n",
    "            deps = set(node.get(\"dependencies\", []))\n",
    "            internal_deps = deps & node_id_set\n",
    "            if internal_deps.issubset(set(sorted_nodes)):\n",
    "                ready.append(nid)\n",
    "        if not ready:\n",
    "            for nid in node_ids:\n",
    "                if nid in remaining:\n",
    "                    sorted_nodes.append(nid)\n",
    "            break\n",
    "        ready_ordered = [nid for nid in node_ids if nid in ready]\n",
    "        sorted_nodes.extend(ready_ordered)\n",
    "        remaining -= set(ready_ordered)\n",
    "    return sorted_nodes\n",
    "\n",
    "\n",
    "def get_adjacent_nodes(skeleton: Dict[str, Any], node_id: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"Get the previous and next node IDs in document order.\"\"\"\n",
    "    leaves = get_leaf_nodes(skeleton)\n",
    "    try:\n",
    "        idx = leaves.index(node_id)\n",
    "        prev_node = leaves[idx - 1] if idx > 0 else None\n",
    "        next_node = leaves[idx + 1] if idx < len(leaves) - 1 else None\n",
    "        return prev_node, next_node\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def validate_skeleton(skeleton: Dict[str, Any]) -> Tuple[bool, List[str]]:\n",
    "    \"\"\"Validate skeleton structure.\"\"\"\n",
    "    issues = []\n",
    "    nodes = skeleton.get(\"nodes\", {})\n",
    "    root_nodes = skeleton.get(\"root_nodes\", [])\n",
    "    \n",
    "    for root_id in root_nodes:\n",
    "        if root_id not in nodes:\n",
    "            issues.append(f\"Root node '{root_id}' not found in nodes\")\n",
    "    \n",
    "    # Check dependencies exist and no cycles\n",
    "    visited = set()\n",
    "    rec_stack = set()\n",
    "    \n",
    "    def has_cycle(node_id: str) -> bool:\n",
    "        visited.add(node_id)\n",
    "        rec_stack.add(node_id)\n",
    "        node = nodes.get(node_id, {})\n",
    "        for dep_id in node.get(\"dependencies\", []):\n",
    "            if dep_id not in nodes:\n",
    "                issues.append(f\"Node '{node_id}' depends on non-existent node '{dep_id}'\")\n",
    "                continue\n",
    "            if dep_id not in visited:\n",
    "                if has_cycle(dep_id):\n",
    "                    return True\n",
    "            elif dep_id in rec_stack:\n",
    "                issues.append(f\"Circular dependency detected: {node_id} -> {dep_id}\")\n",
    "                return True\n",
    "        rec_stack.discard(node_id)\n",
    "        return False\n",
    "    \n",
    "    for node_id in nodes:\n",
    "        if node_id not in visited:\n",
    "            has_cycle(node_id)\n",
    "    \n",
    "    # Check reasonable total word count\n",
    "    total_words = sum(n.get(\"target_word_count\", 300) for n in nodes.values())\n",
    "    if total_words < 1000:\n",
    "        issues.append(f\"Total word count too low: {total_words}\")\n",
    "    if total_words > 5000:\n",
    "        issues.append(f\"Total word count too high: {total_words}\")\n",
    "    \n",
    "    return len(issues) == 0, issues\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee68dc7",
   "metadata": {},
   "source": [
    "## 8. Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66f46fd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === Phase 1: Research Prompts ===\n",
    "DECOMPOSE_QUESTION_PROMPT = \"\"\"You are a research planning expert. Given a research question, \n",
    "decompose it into 4-6 specific sub-questions that need to be investigated.\n",
    "\n",
    "Research Question: {question}\n",
    "\n",
    "Generate a prioritized list of specific, focused research questions that together will answer the main question.\n",
    "Each question should be independently searchable and cover a different aspect.\n",
    "\n",
    "Return your response as a numbered list (highest priority first):\n",
    "1. [Most critical sub-question]\n",
    "2. [Second priority sub-question]\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "SPRINT_RESEARCH_PROMPT = \"\"\"You are a research agent conducting Sprint {sprint_num} of {max_sprints}.\n",
    "\n",
    "Current research focus: {current_question}\n",
    "\n",
    "Based on the search results below, extract the key findings that address the research question.\n",
    "Be specific and cite sources with URLs where relevant.\n",
    "\n",
    "Search Results:\n",
    "{search_results}\n",
    "\n",
    "Provide a comprehensive summary of findings (400-600 words). Include specific facts, statistics, and source URLs.\n",
    "\"\"\"\n",
    "\n",
    "RETROSPECTIVE_PROMPT = \"\"\"You are conducting a sprint retrospective for a research project.\n",
    "\n",
    "Original Question: {original_question}\n",
    "\n",
    "Sprint {sprint_num} of {max_sprints} has completed.\n",
    "\n",
    "Summary of findings so far:\n",
    "{findings_summary}\n",
    "\n",
    "Current remaining questions in backlog:\n",
    "{remaining_backlog}\n",
    "\n",
    "Analyze the progress and provide a STRUCTURED response:\n",
    "\n",
    "## LEARNINGS\n",
    "What key insights did we gain this sprint?\n",
    "\n",
    "## GAPS\n",
    "What is still unclear or needs more investigation?\n",
    "\n",
    "## CONTINUE\n",
    "Should we continue with another sprint? Answer YES or NO with brief justification.\n",
    "\n",
    "## REPRIORITIZED_BACKLOG\n",
    "Reorder the remaining questions by priority, incorporating any new questions:\n",
    "1. [Highest priority question]\n",
    "2. [Next priority]\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "COMPRESS_FINDINGS_PROMPT = \"\"\"Summarize the research findings into a concise brief for document generation.\n",
    "\n",
    "All Findings:\n",
    "{all_findings}\n",
    "\n",
    "Create a structured brief (max 800 words) capturing:\n",
    "- Key facts and statistics discovered\n",
    "- Main themes and evidence\n",
    "- Important sources (include URLs)\n",
    "- Any contradictions or debates\n",
    "\n",
    "Focus on information that will be useful for writing a comprehensive research report.\n",
    "\"\"\"\n",
    "\n",
    "# === Phase 2: Skeleton Prompts ===\n",
    "SKELETON_GENERATION_PROMPT = \"\"\"You are a research document architect. Create a detailed document skeleton for answering this research question.\n",
    "\n",
    "Research Question: {question}\n",
    "\n",
    "Research Brief (completed research findings):\n",
    "{research_brief}\n",
    "\n",
    "Create a hierarchical document structure with:\n",
    "1. A clear thesis statement (one sentence capturing the document's central argument)\n",
    "2. 5-7 main sections appropriate for a comprehensive research report\n",
    "3. Each section should have:\n",
    "   - node_id: Unique identifier using format \"sec:topic\" (e.g., \"sec:intro\", \"sec:background\")\n",
    "   - title: Descriptive section title\n",
    "   - intent: 1-3 sentences describing what this section should accomplish\n",
    "   - target_word_count: 250-400 words per section\n",
    "   - dependencies: List of node_ids that this section builds upon\n",
    "   - children: Empty list (flat structure)\n",
    "\n",
    "REQUIRED SECTIONS:\n",
    "1. Introduction - Present the topic, context, and thesis\n",
    "2. Background/Context - Provide necessary foundation knowledge  \n",
    "3. Main Body (2-4 sections) - Cover key aspects in depth\n",
    "4. Analysis/Discussion - Synthesize findings\n",
    "5. Conclusion - Summarize key points\n",
    "\n",
    "IMPORTANT: Map research findings to appropriate sections. Each finding should have a home.\n",
    "\"\"\"\n",
    "\n",
    "CLAIM_PLACEHOLDER_PROMPT = \"\"\"Analyze this document skeleton and identify claims that will need evidence.\n",
    "\n",
    "Document Thesis: {thesis}\n",
    "\n",
    "Skeleton Sections:\n",
    "{skeleton_summary}\n",
    "\n",
    "Research Brief:\n",
    "{research_brief}\n",
    "\n",
    "For each section, identify 2-4 key claims that the section will need to make and verify.\n",
    "These are \"placeholder\" claims - assertions that the document structure implies but haven't been written yet.\n",
    "\n",
    "For each claim:\n",
    "- claim_id: Format \"claim_placeholder_{section}_{number}\"\n",
    "- claim_text: The anticipated assertion\n",
    "- source_node: The section where this claim belongs\n",
    "- verification_status: \"placeholder\" \n",
    "- supporting_evidence: Empty list (to be filled during research)\n",
    "\n",
    "Focus on substantive factual claims that will require evidence.\n",
    "\"\"\"\n",
    "\n",
    "# === Phase 3: Expansion Prompts ===\n",
    "PROSE_GENERATION_PROMPT = \"\"\"You are a research writer generating content for a specific section.\n",
    "\n",
    "DOCUMENT CONTEXT:\n",
    "Research Question: {question}\n",
    "Document Thesis: {thesis}\n",
    "\n",
    "SECTION TO WRITE:\n",
    "Node ID: {node_id}\n",
    "Title: {title}\n",
    "Intent: {intent}\n",
    "Target Length: ~{target_words} words\n",
    "\n",
    "DOCUMENT STRUCTURE:\n",
    "{skeleton_summary}\n",
    "\n",
    "PREVIOUS SECTION'S ENDING:\n",
    "{previous_bridge_out}\n",
    "\n",
    "CONTENT FROM DEPENDENCY SECTIONS:\n",
    "{dependency_summaries}\n",
    "\n",
    "RELEVANT RESEARCH FINDINGS:\n",
    "{research_findings}\n",
    "\n",
    "WRITING REQUIREMENTS:\n",
    "1. **bridge_in** (1-2 sentences): Transition from previous section\n",
    "2. **main_content** (~{target_words} words): Substantive prose with citations (Source: URL)\n",
    "3. **bridge_out** (1-2 sentences): Setup the next section\n",
    "4. **summary** (1-2 sentences): What this section establishes\n",
    "\n",
    "Be comprehensive, specific, and well-sourced.\n",
    "\"\"\"\n",
    "\n",
    "CLAIM_EXTRACTION_PROMPT = \"\"\"Extract all factual claims from this research prose.\n",
    "\n",
    "Section Node ID: {node_id}\n",
    "\n",
    "Prose Content:\n",
    "{prose}\n",
    "\n",
    "A claim is any statement that:\n",
    "- Has a truth value (could be verified or refuted)\n",
    "- Contributes to the document's argument\n",
    "\n",
    "For each claim:\n",
    "- claim_id: Format \"claim_{node_id}_{number}\"\n",
    "- claim_text: The precise assertion\n",
    "- source_node: \"{node_id}\"\n",
    "- verification_status: \"verified\" if it cites a source, \"unverified\" otherwise\n",
    "- supporting_evidence: List of cited sources\n",
    "\n",
    "Extract 3-8 key claims per section.\n",
    "\"\"\"\n",
    "\n",
    "# === Phase 4: Verification Prompts ===\n",
    "CRITIQUE_PROMPT = \"\"\"You are a critical reviewer evaluating a research document.\n",
    "\n",
    "ORIGINAL QUESTION: {question}\n",
    "DOCUMENT THESIS: {thesis}\n",
    "\n",
    "DOCUMENT STRUCTURE:\n",
    "{skeleton_summary}\n",
    "\n",
    "CLAIMS REGISTRY:\n",
    "{claims_summary}\n",
    "\n",
    "FULL DOCUMENT:\n",
    "{document_content}\n",
    "\n",
    "Analyze at THREE levels:\n",
    "\n",
    "## 1. GLOBAL ISSUES\n",
    "- Is the thesis clearly stated and supported?\n",
    "- Does the argument flow logically?\n",
    "\n",
    "## 2. SECTION ISSUES (per node)\n",
    "- weak_claim: Claims lacking evidence\n",
    "- missing_evidence: Key assertions need sources\n",
    "- logical_gap: Reasoning jumps\n",
    "- depth: Insufficient detail\n",
    "\n",
    "## 3. TRANSITION ISSUES\n",
    "- Abrupt topic shifts\n",
    "- Broken references\n",
    "\n",
    "For each issue provide:\n",
    "- issue_id, scope, target_nodes, issue_type, severity\n",
    "- affected_claims, description, suggestion, search_query\n",
    "\n",
    "SCORING (1-10):\n",
    "- 9-10: Publication ready\n",
    "- 7-8: Good, minor issues\n",
    "- 5-6: Needs improvement\n",
    "- 3-4: Significant problems\n",
    "- 1-2: Major rework needed\n",
    "\"\"\"\n",
    "\n",
    "# Gate 2 prompt with V1's multi-dimensional scoring\n",
    "GATE2_CHECK_PROMPT = \"\"\"Evaluate this research report on four dimensions (1-10 each):\n",
    "\n",
    "Research Question: {question}\n",
    "\n",
    "Report:\n",
    "{document_content}\n",
    "\n",
    "Claims and Evidence:\n",
    "{claims_summary}\n",
    "\n",
    "1. QUESTION_COVERAGE: Does it address ALL parts of the research question?\n",
    "2. EVIDENCE_QUALITY: Are major claims supported by cited sources?\n",
    "3. COHERENCE: Is the argument logical and well-structured?\n",
    "4. DEPTH: Does it go beyond surface-level information?\n",
    "\n",
    "Also identify any claims that still need evidence.\n",
    "\n",
    "Format your response EXACTLY as:\n",
    "QUESTION_COVERAGE: [score]\n",
    "EVIDENCE_QUALITY: [score]\n",
    "COHERENCE: [score]\n",
    "DEPTH: [score]\n",
    "OVERALL: [average]\n",
    "PASS: [YES/NO] (YES if OVERALL >= 7.5 AND EVIDENCE_QUALITY >= 6)\n",
    "WEAK_CLAIMS: [list of claims needing more support, or \"None\"]\n",
    "\"\"\"\n",
    "\n",
    "PATCH_PROMPT = \"\"\"Revise this section based on critique feedback and new evidence.\n",
    "\n",
    "SECTION TO REVISE:\n",
    "Node ID: {node_id}\n",
    "Title: {title}\n",
    "Intent: {intent}\n",
    "\n",
    "CURRENT CONTENT:\n",
    "Bridge In: {current_bridge_in}\n",
    "Main Content: {current_main_content}\n",
    "Bridge Out: {current_bridge_out}\n",
    "\n",
    "ISSUES TO ADDRESS:\n",
    "{issues_for_node}\n",
    "\n",
    "NEW EVIDENCE:\n",
    "{new_evidence}\n",
    "\n",
    "ADJACENT SECTIONS:\n",
    "Previous ends with: {prev_bridge_out}\n",
    "Next starts with: {next_bridge_in}\n",
    "\n",
    "Output complete revised section with:\n",
    "- bridge_in, main_content, bridge_out, summary\n",
    "\"\"\"\n",
    "\n",
    "print(\"Prompts defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e0462c",
   "metadata": {},
   "source": [
    "## 9. Phase 1 Node Functions (Research)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae3dddc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "async def decompose_question(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Decompose research question into sub-questions.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 1a: Decomposing Research Question\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    prompt = DECOMPOSE_QUESTION_PROMPT.format(question=question)\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    lines = response.content.strip().split(\"\\n\")\n",
    "    backlog = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and (line[0].isdigit() or line.startswith(\"-\")):\n",
    "            clean = line.lstrip(\"0123456789.-) \").strip()\n",
    "            if clean:\n",
    "                backlog.append(clean)\n",
    "    \n",
    "    print(f\"  Created backlog with {len(backlog)} research questions\")\n",
    "    \n",
    "    return {\n",
    "        \"research_backlog\": backlog,\n",
    "        \"current_research_sprint\": 1,\n",
    "        \"max_research_sprints\": MAX_RESEARCH_SPRINTS,\n",
    "        \"gate1_attempts\": 0\n",
    "    }\n",
    "\n",
    "\n",
    "async def execute_research_sprint(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Execute a research sprint using Knowledge Cache.\"\"\"\n",
    "    backlog = state.get(\"research_backlog\", [])\n",
    "    current_sprint = state.get(\"current_research_sprint\", 1)\n",
    "    max_sprints = state.get(\"max_research_sprints\", MAX_RESEARCH_SPRINTS)\n",
    "    \n",
    "    if not backlog:\n",
    "        return {\"sprint_findings\": [\"No questions in backlog.\"]}\n",
    "    \n",
    "    current_question = backlog[0]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Sprint {current_sprint}/{max_sprints}: {current_question[:70]}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Generate search queries\n",
    "    all_results = []\n",
    "    all_urls = []\n",
    "    all_decisions = []\n",
    "    queries = [current_question]\n",
    "    \n",
    "    if SEARCHES_PER_QUESTION > 1:\n",
    "        query_prompt = f\"\"\"Generate {SEARCHES_PER_QUESTION - 1} specific search queries for:\n",
    "Question: {current_question}\n",
    "\n",
    "Return queries one per line.\"\"\"\n",
    "        query_response = await llm.ainvoke([HumanMessage(content=query_prompt)])\n",
    "        additional = [q.strip() for q in query_response.content.split(\"\\n\") if q.strip()]\n",
    "        queries.extend(additional[:SEARCHES_PER_QUESTION - 1])\n",
    "    \n",
    "    # Execute searches through cache\n",
    "    for query in queries:\n",
    "        print(f\"  Searching: {query[:50]}...\")\n",
    "        content, urls, decision = await cascaded_search(query, knowledge_base)\n",
    "        all_results.append(content)\n",
    "        all_urls.extend(urls)\n",
    "        all_decisions.append(decision.model_dump())\n",
    "    \n",
    "    combined_results = \"\\n\\n---\\n\\n\".join(all_results)\n",
    "    \n",
    "    # Synthesize findings\n",
    "    synthesis_prompt = SPRINT_RESEARCH_PROMPT.format(\n",
    "        sprint_num=current_sprint,\n",
    "        max_sprints=max_sprints,\n",
    "        current_question=current_question,\n",
    "        search_results=combined_results[:MAX_FINDINGS_CHARS]\n",
    "    )\n",
    "    synthesis = await llm.ainvoke([HumanMessage(content=synthesis_prompt)])\n",
    "    \n",
    "    finding = f\"## Sprint {current_sprint} Findings: {current_question}\\n\\n{synthesis.content}\"\n",
    "    print(f\"  Synthesized {len(synthesis.content)} characters\")\n",
    "    print(f\"  Collected {len(all_urls)} source URLs\")\n",
    "    \n",
    "    updated_backlog = backlog[1:] if len(backlog) > 1 else []\n",
    "    \n",
    "    return {\n",
    "        \"sprint_findings\": [finding],\n",
    "        \"research_source_urls\": all_urls,\n",
    "        \"research_backlog\": updated_backlog,\n",
    "        \"cache_decisions\": all_decisions\n",
    "    }\n",
    "\n",
    "\n",
    "async def research_retrospective(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Conduct retrospective after sprint.\"\"\"\n",
    "    original_question = state[\"question\"]\n",
    "    current_sprint = state.get(\"current_research_sprint\", 1)\n",
    "    max_sprints = state.get(\"max_research_sprints\", MAX_RESEARCH_SPRINTS)\n",
    "    all_findings = \"\\n\\n\".join(state.get(\"sprint_findings\", []))\n",
    "    remaining_backlog = state.get(\"research_backlog\", [])\n",
    "    \n",
    "    # Summarize if too long\n",
    "    if len(all_findings) > 4000:\n",
    "        summary_prompt = COMPRESS_FINDINGS_PROMPT.format(all_findings=all_findings[:8000])\n",
    "        summary_response = await llm.ainvoke([HumanMessage(content=summary_prompt)])\n",
    "        findings_summary = summary_response.content\n",
    "    else:\n",
    "        findings_summary = all_findings\n",
    "    \n",
    "    prompt = RETROSPECTIVE_PROMPT.format(\n",
    "        original_question=original_question,\n",
    "        sprint_num=current_sprint,\n",
    "        max_sprints=max_sprints,\n",
    "        findings_summary=findings_summary,\n",
    "        remaining_backlog=\"\\n\".join(f\"- {q}\" for q in remaining_backlog) if remaining_backlog else \"None\"\n",
    "    )\n",
    "    \n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    # Parse reprioritized backlog\n",
    "    backlog_match = re.search(r'## REPRIORITIZED_BACKLOG\\s*(.*?)(?=##|$)', response.content, re.DOTALL | re.IGNORECASE)\n",
    "    reprioritized = []\n",
    "    if backlog_match:\n",
    "        for line in backlog_match.group(1).strip().split(\"\\n\"):\n",
    "            line = line.strip()\n",
    "            if line and (line[0].isdigit() or line.startswith(\"-\")):\n",
    "                clean = re.sub(r'^[\\d\\.\\-\\)\\s]+', '', line).strip()\n",
    "                if clean and len(clean) > 10:\n",
    "                    reprioritized.append(clean)\n",
    "    \n",
    "    if not reprioritized and remaining_backlog:\n",
    "        reprioritized = remaining_backlog\n",
    "    \n",
    "    # Parse continue decision\n",
    "    continue_match = re.search(r'## CONTINUE\\s*(.*?)(?=##|$)', response.content, re.DOTALL | re.IGNORECASE)\n",
    "    should_continue = True\n",
    "    if continue_match:\n",
    "        continue_text = continue_match.group(1).strip().lower()\n",
    "        if re.search(r'^no\\b|should\\s+stop|sufficient', continue_text):\n",
    "            should_continue = False\n",
    "    \n",
    "    print(f\"  Retrospective: {'Continue' if should_continue else 'Stop'}, {len(reprioritized)} questions\")\n",
    "    \n",
    "    return {\n",
    "        \"research_retrospective_notes\": [f\"### Sprint {current_sprint} Retrospective\\n\\n{response.content}\"],\n",
    "        \"current_research_sprint\": current_sprint + 1,\n",
    "        \"research_backlog\": reprioritized,\n",
    "        \"research_brief\": findings_summary if not should_continue or current_sprint >= max_sprints else \"\"\n",
    "    }\n",
    "\n",
    "\n",
    "def route_after_retrospective(state: CombinedTier1State) -> Literal[\"execute_research_sprint\", \"quality_gate_1\"]:\n",
    "    \"\"\"Decide whether to continue sprinting or proceed to Gate 1.\"\"\"\n",
    "    current_sprint = state.get(\"current_research_sprint\", 1)\n",
    "    max_sprints = state.get(\"max_research_sprints\", MAX_RESEARCH_SPRINTS)\n",
    "    backlog = state.get(\"research_backlog\", [])\n",
    "    \n",
    "    if current_sprint > max_sprints:\n",
    "        print(f\"\\nMax sprints ({max_sprints}) reached. Proceeding to Gate 1.\")\n",
    "        return \"quality_gate_1\"\n",
    "    if not backlog:\n",
    "        print(f\"\\nBacklog empty. Proceeding to Gate 1.\")\n",
    "        return \"quality_gate_1\"\n",
    "    \n",
    "    print(f\"\\nContinuing to sprint {current_sprint}. {len(backlog)} questions remaining.\")\n",
    "    return \"execute_research_sprint\"\n",
    "\n",
    "\n",
    "async def quality_gate_1(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Gate 1: Validate source sufficiency.\"\"\"\n",
    "    source_urls = state.get(\"research_source_urls\", [])\n",
    "    unique_urls = list(set(source_urls))\n",
    "    domains = list(set(extract_domain(url) for url in unique_urls))\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Quality Gate 1: Source Validation\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Unique sources: {len(unique_urls)} (min: {MIN_SOURCES_FOR_GATE1})\")\n",
    "    print(f\"  Unique domains: {len(domains)} (min: {MIN_DOMAINS_FOR_GATE1})\")\n",
    "    \n",
    "    passed = len(unique_urls) >= MIN_SOURCES_FOR_GATE1 and len(domains) >= MIN_DOMAINS_FOR_GATE1\n",
    "    \n",
    "    if passed:\n",
    "        print(f\"  Gate 1: PASSED\")\n",
    "    else:\n",
    "        print(f\"  Gate 1: FAILED\")\n",
    "    \n",
    "    # Compress findings if not already done\n",
    "    research_brief = state.get(\"research_brief\", \"\")\n",
    "    if not research_brief:\n",
    "        all_findings = \"\\n\\n\".join(state.get(\"sprint_findings\", []))\n",
    "        if len(all_findings) > 4000:\n",
    "            summary_prompt = COMPRESS_FINDINGS_PROMPT.format(all_findings=all_findings[:8000])\n",
    "            summary_response = await llm.ainvoke([HumanMessage(content=summary_prompt)])\n",
    "            research_brief = summary_response.content\n",
    "        else:\n",
    "            research_brief = all_findings\n",
    "    \n",
    "    return {\n",
    "        \"gate1_passed\": passed,\n",
    "        \"gate1_attempts\": state.get(\"gate1_attempts\", 0) + 1,\n",
    "        \"research_brief\": research_brief\n",
    "    }\n",
    "\n",
    "\n",
    "def route_after_gate1(state: CombinedTier1State) -> Literal[\"execute_research_sprint\", \"generate_skeleton\"]:\n",
    "    \"\"\"Route after Gate 1.\"\"\"\n",
    "    passed = state.get(\"gate1_passed\", False)\n",
    "    attempts = state.get(\"gate1_attempts\", 0)\n",
    "    \n",
    "    if passed:\n",
    "        return \"generate_skeleton\"\n",
    "    elif attempts < 2:\n",
    "        print(f\"  Gate 1 failed, attempting additional sprint...\")\n",
    "        return \"execute_research_sprint\"\n",
    "    else:\n",
    "        print(f\"  Gate 1 failed after {attempts} attempts, proceeding anyway...\")\n",
    "        return \"generate_skeleton\"\n",
    "\n",
    "print(\"Phase 1 node functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede8188c",
   "metadata": {},
   "source": [
    "## 10. Phase 2 Node Functions (Structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e810655",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "async def generate_skeleton(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Generate document skeleton from research brief.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    research_brief = state.get(\"research_brief\", \"\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 2a: Generating Document Skeleton\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    prompt = SKELETON_GENERATION_PROMPT.format(\n",
    "        question=question,\n",
    "        research_brief=research_brief[:MAX_CONTEXT_CHARS]\n",
    "    )\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(SkeletonGenerationOutput)\n",
    "    result = await structured_llm.ainvoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    # Build skeleton dict\n",
    "    skeleton = {\n",
    "        \"thesis\": result.thesis,\n",
    "        \"root_nodes\": [s.node_id for s in result.sections],\n",
    "        \"nodes\": {s.node_id: s.model_dump() for s in result.sections},\n",
    "        \"style_constraints\": \"Academic tone, comprehensive citations\"\n",
    "    }\n",
    "    \n",
    "    print(f\"  Generated skeleton with {len(result.sections)} sections\")\n",
    "    print(f\"  Thesis: {result.thesis[:80]}...\")\n",
    "    \n",
    "    return {\n",
    "        \"skeleton\": skeleton\n",
    "    }\n",
    "\n",
    "\n",
    "async def identify_claim_placeholders(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Identify claims that need to be researched (from V2).\"\"\"\n",
    "    skeleton = state.get(\"skeleton\", {})\n",
    "    research_brief = state.get(\"research_brief\", \"\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 2b: Identifying Claim Placeholders\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Build skeleton summary\n",
    "    skeleton_summary = []\n",
    "    for node_id in skeleton.get(\"root_nodes\", []):\n",
    "        node = skeleton.get(\"nodes\", {}).get(node_id, {})\n",
    "        skeleton_summary.append(f\"- {node_id}: {node.get('title', '')} - {node.get('intent', '')[:100]}\")\n",
    "    \n",
    "    prompt = CLAIM_PLACEHOLDER_PROMPT.format(\n",
    "        thesis=skeleton.get(\"thesis\", \"\"),\n",
    "        skeleton_summary=\"\\n\".join(skeleton_summary),\n",
    "        research_brief=research_brief[:MAX_CONTEXT_CHARS]\n",
    "    )\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(ClaimPlaceholderOutput)\n",
    "    result = await structured_llm.ainvoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    claims_registry = {claim.claim_id: claim.model_dump() for claim in result.claims}\n",
    "    \n",
    "    print(f\"  Identified {len(claims_registry)} placeholder claims\")\n",
    "    \n",
    "    return {\n",
    "        \"claims_registry\": claims_registry\n",
    "    }\n",
    "\n",
    "\n",
    "async def validate_skeleton(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Gate 1.5: Validate skeleton structure.\"\"\"\n",
    "    skeleton = state.get(\"skeleton\", {})\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 2c: Skeleton Validation (Gate 1.5)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    is_valid, issues = validate_skeleton(skeleton)\n",
    "    \n",
    "    if is_valid:\n",
    "        print(f\"  Skeleton validation: PASSED\")\n",
    "    else:\n",
    "        print(f\"  Skeleton validation: FAILED\")\n",
    "        for issue in issues:\n",
    "            print(f\"    - {issue}\")\n",
    "    \n",
    "    return {\n",
    "        \"skeleton_validated\": is_valid\n",
    "    }\n",
    "\n",
    "\n",
    "def route_after_skeleton_validation(state: CombinedTier1State) -> Literal[\"expand_all_nodes\", \"generate_skeleton\"]:\n",
    "    \"\"\"Route after skeleton validation.\"\"\"\n",
    "    validated = state.get(\"skeleton_validated\", False)\n",
    "    if validated:\n",
    "        return \"expand_all_nodes\"\n",
    "    else:\n",
    "        # Could retry skeleton generation, but for now just proceed\n",
    "        print(\"  Proceeding despite validation issues...\")\n",
    "        return \"expand_all_nodes\"\n",
    "\n",
    "print(\"Phase 2 node functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80369d9",
   "metadata": {},
   "source": [
    "## 11. Phase 3 Node Functions (Expansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18275094",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "async def expand_all_nodes(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Expand all skeleton nodes into prose.\"\"\"\n",
    "    skeleton = state.get(\"skeleton\", {})\n",
    "    question = state[\"question\"]\n",
    "    research_brief = state.get(\"research_brief\", \"\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 3a: Expanding Nodes\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    leaf_nodes = get_leaf_nodes(skeleton)\n",
    "    sorted_nodes = topological_sort_nodes(skeleton, leaf_nodes)\n",
    "    \n",
    "    prose_store = {}\n",
    "    nodes_expanded = []\n",
    "    \n",
    "    # Build skeleton summary\n",
    "    skeleton_summary = []\n",
    "    for node_id in skeleton.get(\"root_nodes\", []):\n",
    "        node = skeleton.get(\"nodes\", {}).get(node_id, {})\n",
    "        skeleton_summary.append(f\"- {node_id}: {node.get('title', '')}\")\n",
    "    \n",
    "    for node_id in sorted_nodes:\n",
    "        node = skeleton.get(\"nodes\", {}).get(node_id, {})\n",
    "        print(f\"  Expanding: {node_id} - {node.get('title', '')[:40]}...\")\n",
    "        \n",
    "        # Get previous bridge_out\n",
    "        prev_node, next_node = get_adjacent_nodes(skeleton, node_id)\n",
    "        previous_bridge_out = \"\"\n",
    "        if prev_node and prev_node in prose_store:\n",
    "            previous_bridge_out = prose_store[prev_node].get(\"bridge_out\", \"\")\n",
    "        \n",
    "        # Get dependency summaries\n",
    "        dependency_summaries = []\n",
    "        for dep_id in node.get(\"dependencies\", []):\n",
    "            if dep_id in prose_store:\n",
    "                dep_summary = prose_store[dep_id].get(\"summary\", \"\")\n",
    "                if dep_summary:\n",
    "                    dep_title = skeleton.get(\"nodes\", {}).get(dep_id, {}).get(\"title\", dep_id)\n",
    "                    dependency_summaries.append(f\"[{dep_title}]: {dep_summary}\")\n",
    "        \n",
    "        prompt = PROSE_GENERATION_PROMPT.format(\n",
    "            question=question,\n",
    "            thesis=skeleton.get(\"thesis\", \"\"),\n",
    "            node_id=node_id,\n",
    "            title=node.get(\"title\", \"\"),\n",
    "            intent=node.get(\"intent\", \"\"),\n",
    "            target_words=node.get(\"target_word_count\", TARGET_WORDS_PER_NODE),\n",
    "            skeleton_summary=\"\\n\".join(skeleton_summary),\n",
    "            previous_bridge_out=previous_bridge_out if previous_bridge_out else \"(First section - no previous content)\",\n",
    "            dependency_summaries=\"\\n\".join(dependency_summaries) if dependency_summaries else \"(No dependencies)\",\n",
    "            research_findings=research_brief[:MAX_CONTEXT_CHARS]\n",
    "        )\n",
    "        \n",
    "        structured_llm = llm.with_structured_output(ProseGenerationOutput)\n",
    "        result = await structured_llm.ainvoke([HumanMessage(content=prompt)])\n",
    "        \n",
    "        prose_entry = ProseEntry(\n",
    "            node_id=node_id,\n",
    "            main_content=result.main_content,\n",
    "            bridge_in=result.bridge_in,\n",
    "            bridge_out=result.bridge_out,\n",
    "            summary=result.summary\n",
    "        )\n",
    "        prose_store[node_id] = prose_entry.model_dump()\n",
    "        nodes_expanded.append(node_id)\n",
    "    \n",
    "    print(f\"  Expanded {len(nodes_expanded)} nodes\")\n",
    "    \n",
    "    return {\n",
    "        \"prose_store\": prose_store,\n",
    "        \"nodes_expanded\": nodes_expanded\n",
    "    }\n",
    "\n",
    "\n",
    "async def extract_inline_claims(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Extract claims from expanded prose.\"\"\"\n",
    "    prose_store = state.get(\"prose_store\", {})\n",
    "    existing_claims = state.get(\"claims_registry\", {})\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 3b: Extracting Claims\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    all_claims = dict(existing_claims)  # Start with placeholder claims\n",
    "    \n",
    "    for node_id, prose_entry in prose_store.items():\n",
    "        prose = f\"{prose_entry.get('bridge_in', '')}\\n{prose_entry.get('main_content', '')}\\n{prose_entry.get('bridge_out', '')}\"\n",
    "        \n",
    "        prompt = CLAIM_EXTRACTION_PROMPT.format(\n",
    "            node_id=node_id,\n",
    "            prose=prose\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            structured_llm = llm.with_structured_output(ClaimExtractionOutput)\n",
    "            result = await structured_llm.ainvoke([HumanMessage(content=prompt)])\n",
    "            \n",
    "            for claim in result.claims:\n",
    "                all_claims[claim.claim_id] = claim.model_dump()\n",
    "        except Exception as e:\n",
    "            print(f\"    Error extracting claims from {node_id}: {e}\")\n",
    "    \n",
    "    print(f\"  Total claims in registry: {len(all_claims)}\")\n",
    "    \n",
    "    return {\n",
    "        \"claims_registry\": all_claims\n",
    "    }\n",
    "\n",
    "\n",
    "async def assemble_draft(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Assemble prose into initial draft.\"\"\"\n",
    "    skeleton = state.get(\"skeleton\", {})\n",
    "    prose_store = state.get(\"prose_store\", {})\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 3c: Assembling Draft\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    leaf_nodes = get_leaf_nodes(skeleton)\n",
    "    \n",
    "    document_parts = []\n",
    "    document_parts.append(f\"# Research Report\\n\\n**Thesis:** {skeleton.get('thesis', '')}\\n\\n---\\n\")\n",
    "    \n",
    "    for node_id in leaf_nodes:\n",
    "        node = skeleton.get(\"nodes\", {}).get(node_id, {})\n",
    "        prose = prose_store.get(node_id, {})\n",
    "        \n",
    "        section = f\"## {node.get('title', node_id)}\\n\\n\"\n",
    "        if prose.get(\"bridge_in\"):\n",
    "            section += f\"{prose['bridge_in']}\\n\\n\"\n",
    "        section += f\"{prose.get('main_content', '')}\\n\\n\"\n",
    "        if prose.get(\"bridge_out\"):\n",
    "            section += f\"{prose['bridge_out']}\\n\"\n",
    "        \n",
    "        document_parts.append(section)\n",
    "    \n",
    "    assembled_draft = \"\\n---\\n\\n\".join(document_parts)\n",
    "    \n",
    "    print(f\"  Assembled draft: {len(assembled_draft)} characters\")\n",
    "    \n",
    "    return {\n",
    "        \"assembled_draft\": assembled_draft,\n",
    "        \"current_verification_iteration\": 0,\n",
    "        \"max_verification_iterations\": MAX_VERIFICATION_ITERATIONS\n",
    "    }\n",
    "\n",
    "print(\"Phase 3 node functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484cd4d",
   "metadata": {},
   "source": [
    "## 12. Phase 4 Node Functions (Verification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ceca97",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "async def structured_critique(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Perform structured critique of the document.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    skeleton = state.get(\"skeleton\", {})\n",
    "    claims_registry = state.get(\"claims_registry\", {})\n",
    "    assembled_draft = state.get(\"assembled_draft\", \"\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 4a: Structured Critique\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Build skeleton summary\n",
    "    skeleton_summary = []\n",
    "    for node_id in skeleton.get(\"root_nodes\", []):\n",
    "        node = skeleton.get(\"nodes\", {}).get(node_id, {})\n",
    "        skeleton_summary.append(f\"- {node_id}: {node.get('title', '')}\")\n",
    "    \n",
    "    # Build claims summary\n",
    "    claims_summary = []\n",
    "    for claim_id, claim in list(claims_registry.items())[:30]:\n",
    "        status = claim.get(\"verification_status\", \"unverified\")\n",
    "        text = claim.get(\"claim_text\", \"\")[:80]\n",
    "        claims_summary.append(f\"- [{status}] {claim_id}: {text}\")\n",
    "    \n",
    "    prompt = CRITIQUE_PROMPT.format(\n",
    "        question=question,\n",
    "        thesis=skeleton.get(\"thesis\", \"\"),\n",
    "        skeleton_summary=\"\\n\".join(skeleton_summary),\n",
    "        claims_summary=\"\\n\".join(claims_summary) if claims_summary else \"(No claims yet)\",\n",
    "        document_content=assembled_draft[:MAX_CONTEXT_CHARS]\n",
    "    )\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(CritiqueResult)\n",
    "    result = await structured_llm.ainvoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    noise_map = [issue.model_dump() for issue in result.issues]\n",
    "    nodes_to_patch = list(set(\n",
    "        node_id \n",
    "        for issue in result.issues \n",
    "        for node_id in issue.target_nodes\n",
    "        if issue.severity in [\"critical\", \"major\"]\n",
    "    ))\n",
    "    \n",
    "    print(f\"  Quality score: {result.overall_quality}/10\")\n",
    "    print(f\"  Issues found: {len(noise_map)}\")\n",
    "    print(f\"  Nodes to patch: {len(nodes_to_patch)}\")\n",
    "    \n",
    "    return {\n",
    "        \"noise_map\": noise_map,\n",
    "        \"nodes_to_patch\": nodes_to_patch,\n",
    "        \"quality_scores\": [{\"critique_score\": result.overall_quality}]\n",
    "    }\n",
    "\n",
    "\n",
    "async def quality_gate_2(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Gate 2: Multi-dimensional quality check (V1's prompt).\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    assembled_draft = state.get(\"assembled_draft\", \"\")\n",
    "    claims_registry = state.get(\"claims_registry\", {})\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 4b: Quality Gate 2 (Multi-dimensional)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Build claims summary\n",
    "    claims_summary = []\n",
    "    for claim_id, claim in list(claims_registry.items())[:30]:\n",
    "        status = claim.get(\"verification_status\", \"unverified\")\n",
    "        evidence = claim.get(\"supporting_evidence\", [])\n",
    "        text = claim.get(\"claim_text\", \"\")[:80]\n",
    "        claims_summary.append(f\"- [{status}, {len(evidence)} sources] {text}\")\n",
    "    \n",
    "    prompt = GATE2_CHECK_PROMPT.format(\n",
    "        question=question,\n",
    "        document_content=assembled_draft[:MAX_CONTEXT_CHARS],\n",
    "        claims_summary=\"\\n\".join(claims_summary) if claims_summary else \"(No claims)\"\n",
    "    )\n",
    "    \n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "    content = response.content\n",
    "    \n",
    "    # Parse scores\n",
    "    scores = {}\n",
    "    for dim in [\"QUESTION_COVERAGE\", \"EVIDENCE_QUALITY\", \"COHERENCE\", \"DEPTH\", \"OVERALL\"]:\n",
    "        match = re.search(rf'{dim}:\\s*(\\d+(?:\\.\\d+)?)', content, re.IGNORECASE)\n",
    "        if match:\n",
    "            scores[dim.lower()] = float(match.group(1))\n",
    "    \n",
    "    # Parse pass/fail\n",
    "    pass_match = re.search(r'PASS:\\s*(YES|NO)', content, re.IGNORECASE)\n",
    "    passed = pass_match and pass_match.group(1).upper() == \"YES\"\n",
    "    \n",
    "    # Parse weak claims\n",
    "    weak_claims = []\n",
    "    weak_match = re.search(r'WEAK_CLAIMS:\\s*(.+?)(?=\\n\\n|$)', content, re.DOTALL | re.IGNORECASE)\n",
    "    if weak_match:\n",
    "        weak_text = weak_match.group(1).strip()\n",
    "        if weak_text.lower() != \"none\":\n",
    "            weak_claims = [c.strip() for c in re.split(r'[-â€¢\\n,]', weak_text) if c.strip()]\n",
    "    \n",
    "    print(f\"  Scores: {scores}\")\n",
    "    print(f\"  Gate 2: {'PASSED' if passed else 'FAILED'}\")\n",
    "    if weak_claims:\n",
    "        print(f\"  Weak claims: {len(weak_claims)}\")\n",
    "    \n",
    "    return {\n",
    "        \"gate2_passed\": passed,\n",
    "        \"quality_scores\": [scores],\n",
    "        \"nodes_to_patch\": [c.get(\"source_node\") for c in claims_registry.values() \n",
    "                          if c.get(\"claim_text\", \"\") in weak_claims][:5]\n",
    "    }\n",
    "\n",
    "\n",
    "def route_after_gate2(state: CombinedTier1State) -> Literal[\"targeted_retrieval\", \"assemble_document\", \"mark_limitations\"]:\n",
    "    \"\"\"Route after Gate 2.\"\"\"\n",
    "    passed = state.get(\"gate2_passed\", False)\n",
    "    iteration = state.get(\"current_verification_iteration\", 0)\n",
    "    max_iterations = state.get(\"max_verification_iterations\", MAX_VERIFICATION_ITERATIONS)\n",
    "    \n",
    "    if passed:\n",
    "        return \"assemble_document\"\n",
    "    elif iteration < max_iterations:\n",
    "        return \"targeted_retrieval\"\n",
    "    else:\n",
    "        return \"mark_limitations\"\n",
    "\n",
    "\n",
    "async def targeted_retrieval(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Fetch evidence for weak claims using Knowledge Cache.\"\"\"\n",
    "    nodes_to_patch = state.get(\"nodes_to_patch\", [])\n",
    "    noise_map = state.get(\"noise_map\", [])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 4c: Targeted Retrieval\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    targeted_evidence = {}\n",
    "    all_decisions = []\n",
    "    \n",
    "    # Get search queries from noise map\n",
    "    search_queries = []\n",
    "    for issue in noise_map:\n",
    "        if issue.get(\"search_query\") and issue.get(\"severity\") in [\"critical\", \"major\"]:\n",
    "            for node_id in issue.get(\"target_nodes\", []):\n",
    "                search_queries.append((node_id, issue[\"search_query\"]))\n",
    "    \n",
    "    # Execute searches\n",
    "    for node_id, query in search_queries[:6]:  # Limit searches\n",
    "        print(f\"  Searching for {node_id}: {query[:40]}...\")\n",
    "        content, urls, decision = await cascaded_search(query, knowledge_base)\n",
    "        \n",
    "        if node_id not in targeted_evidence:\n",
    "            targeted_evidence[node_id] = []\n",
    "        targeted_evidence[node_id].append(content[:2000])\n",
    "        all_decisions.append(decision.model_dump())\n",
    "    \n",
    "    print(f\"  Retrieved evidence for {len(targeted_evidence)} nodes\")\n",
    "    \n",
    "    return {\n",
    "        \"targeted_evidence\": targeted_evidence,\n",
    "        \"cache_decisions\": all_decisions\n",
    "    }\n",
    "\n",
    "\n",
    "async def apply_patches(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Apply patches to nodes with issues.\"\"\"\n",
    "    skeleton = state.get(\"skeleton\", {})\n",
    "    prose_store = state.get(\"prose_store\", {})\n",
    "    nodes_to_patch = state.get(\"nodes_to_patch\", [])\n",
    "    noise_map = state.get(\"noise_map\", [])\n",
    "    targeted_evidence = state.get(\"targeted_evidence\", {})\n",
    "    iteration = state.get(\"current_verification_iteration\", 0)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 4d: Applying Patches (Iteration {iteration + 1})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    updated_prose_store = dict(prose_store)\n",
    "    \n",
    "    for node_id in nodes_to_patch[:5]:  # Limit patches per iteration\n",
    "        node = skeleton.get(\"nodes\", {}).get(node_id, {})\n",
    "        current_prose = prose_store.get(node_id, {})\n",
    "        \n",
    "        if not current_prose:\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Patching: {node_id}\")\n",
    "        \n",
    "        # Get issues for this node\n",
    "        node_issues = [issue for issue in noise_map if node_id in issue.get(\"target_nodes\", [])]\n",
    "        issues_text = \"\\n\".join([\n",
    "            f\"- [{i.get('severity')}] {i.get('issue_type')}: {i.get('description')}\\n  Suggestion: {i.get('suggestion')}\"\n",
    "            for i in node_issues[:3]\n",
    "        ])\n",
    "        \n",
    "        # Get evidence\n",
    "        evidence = targeted_evidence.get(node_id, [])\n",
    "        evidence_text = \"\\n\\n\".join(evidence) if evidence else \"(No new evidence)\"\n",
    "        \n",
    "        # Get adjacent prose\n",
    "        prev_node, next_node = get_adjacent_nodes(skeleton, node_id)\n",
    "        prev_bridge = prose_store.get(prev_node, {}).get(\"bridge_out\", \"\") if prev_node else \"\"\n",
    "        next_bridge = prose_store.get(next_node, {}).get(\"bridge_in\", \"\") if next_node else \"\"\n",
    "        \n",
    "        prompt = PATCH_PROMPT.format(\n",
    "            node_id=node_id,\n",
    "            title=node.get(\"title\", \"\"),\n",
    "            intent=node.get(\"intent\", \"\"),\n",
    "            current_bridge_in=current_prose.get(\"bridge_in\", \"\"),\n",
    "            current_main_content=current_prose.get(\"main_content\", \"\"),\n",
    "            current_bridge_out=current_prose.get(\"bridge_out\", \"\"),\n",
    "            issues_for_node=issues_text if issues_text else \"(No specific issues)\",\n",
    "            new_evidence=evidence_text[:MAX_CONTEXT_CHARS],\n",
    "            prev_bridge_out=prev_bridge if prev_bridge else \"(Start of document)\",\n",
    "            next_bridge_in=next_bridge if next_bridge else \"(End of document)\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            structured_llm = llm.with_structured_output(ProseGenerationOutput)\n",
    "            result = await structured_llm.ainvoke([HumanMessage(content=prompt)])\n",
    "            \n",
    "            updated_prose = {\n",
    "                \"node_id\": node_id,\n",
    "                \"main_content\": result.main_content,\n",
    "                \"bridge_in\": result.bridge_in,\n",
    "                \"bridge_out\": result.bridge_out,\n",
    "                \"summary\": result.summary,\n",
    "                \"revision_count\": current_prose.get(\"revision_count\", 0) + 1\n",
    "            }\n",
    "            updated_prose_store[node_id] = updated_prose\n",
    "        except Exception as e:\n",
    "            print(f\"    Error patching {node_id}: {e}\")\n",
    "    \n",
    "    print(f\"  Patched {min(len(nodes_to_patch), 5)} nodes\")\n",
    "    \n",
    "    # Reassemble draft\n",
    "    leaf_nodes = get_leaf_nodes(skeleton)\n",
    "    document_parts = [f\"# Research Report\\n\\n**Thesis:** {skeleton.get('thesis', '')}\\n\\n---\\n\"]\n",
    "    \n",
    "    for node_id in leaf_nodes:\n",
    "        node = skeleton.get(\"nodes\", {}).get(node_id, {})\n",
    "        prose = updated_prose_store.get(node_id, {})\n",
    "        \n",
    "        section = f\"## {node.get('title', node_id)}\\n\\n\"\n",
    "        if prose.get(\"bridge_in\"):\n",
    "            section += f\"{prose['bridge_in']}\\n\\n\"\n",
    "        section += f\"{prose.get('main_content', '')}\\n\\n\"\n",
    "        if prose.get(\"bridge_out\"):\n",
    "            section += f\"{prose['bridge_out']}\\n\"\n",
    "        \n",
    "        document_parts.append(section)\n",
    "    \n",
    "    assembled_draft = \"\\n---\\n\\n\".join(document_parts)\n",
    "    \n",
    "    return {\n",
    "        \"prose_store\": updated_prose_store,\n",
    "        \"assembled_draft\": assembled_draft,\n",
    "        \"current_verification_iteration\": iteration + 1,\n",
    "        \"nodes_to_patch\": []  # Clear for next iteration\n",
    "    }\n",
    "\n",
    "\n",
    "def route_after_patches(state: CombinedTier1State) -> Literal[\"structured_critique\", \"mark_limitations\"]:\n",
    "    \"\"\"Route after applying patches.\"\"\"\n",
    "    iteration = state.get(\"current_verification_iteration\", 0)\n",
    "    max_iterations = state.get(\"max_verification_iterations\", MAX_VERIFICATION_ITERATIONS)\n",
    "    \n",
    "    # Skip cascade on first iteration (optimization)\n",
    "    if SKIP_CASCADE_ON_FIRST_ITERATION and iteration == 1:\n",
    "        print(\"  Skipping cascade check on first iteration\")\n",
    "    \n",
    "    if iteration < max_iterations:\n",
    "        return \"structured_critique\"\n",
    "    else:\n",
    "        return \"mark_limitations\"\n",
    "\n",
    "\n",
    "async def mark_limitations(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Mark limitations for unresolved issues (graceful degradation from V2).\"\"\"\n",
    "    noise_map = state.get(\"noise_map\", [])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 4e: Marking Limitations\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Collect unresolved critical/major issues\n",
    "    limitations = []\n",
    "    for issue in noise_map:\n",
    "        if issue.get(\"severity\") in [\"critical\", \"major\"]:\n",
    "            limitations.append(f\"- {issue.get('description', 'Unresolved issue')}\")\n",
    "    \n",
    "    if limitations:\n",
    "        print(f\"  Noted {len(limitations)} limitations\")\n",
    "    else:\n",
    "        print(f\"  No significant limitations to note\")\n",
    "    \n",
    "    return {\n",
    "        \"limitations_noted\": limitations[:5],  # Keep top 5\n",
    "        \"gate2_passed\": True  # Proceed anyway after marking limitations\n",
    "    }\n",
    "\n",
    "print(\"Phase 4 node functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c7bb0a",
   "metadata": {},
   "source": [
    "## 13. Phase 5 Node Functions (Finalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb74ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def assemble_document(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Assemble final document with references.\"\"\"\n",
    "    skeleton = state.get(\"skeleton\", {})\n",
    "    prose_store = state.get(\"prose_store\", {})\n",
    "    source_urls = state.get(\"research_source_urls\", [])\n",
    "    limitations = state.get(\"limitations_noted\", [])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 5a: Assembling Final Document\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    leaf_nodes = get_leaf_nodes(skeleton)\n",
    "    \n",
    "    # Build document\n",
    "    document_parts = []\n",
    "    document_parts.append(f\"# Research Report\\n\\n**Thesis:** {skeleton.get('thesis', '')}\\n\")\n",
    "    \n",
    "    for node_id in leaf_nodes:\n",
    "        node = skeleton.get(\"nodes\", {}).get(node_id, {})\n",
    "        prose = prose_store.get(node_id, {})\n",
    "        \n",
    "        section = f\"\\n## {node.get('title', node_id)}\\n\\n\"\n",
    "        if prose.get(\"bridge_in\"):\n",
    "            section += f\"{prose['bridge_in']}\\n\\n\"\n",
    "        section += f\"{prose.get('main_content', '')}\\n\\n\"\n",
    "        if prose.get(\"bridge_out\"):\n",
    "            section += f\"{prose['bridge_out']}\\n\"\n",
    "        \n",
    "        document_parts.append(section)\n",
    "    \n",
    "    # Add limitations if any\n",
    "    if limitations:\n",
    "        limitations_section = \"\\n## Limitations and Caveats\\n\\n\"\n",
    "        limitations_section += \"The following aspects could not be fully addressed:\\n\\n\"\n",
    "        limitations_section += \"\\n\".join(limitations)\n",
    "        document_parts.append(limitations_section)\n",
    "    \n",
    "    # Add references\n",
    "    unique_urls = list(set(source_urls))[:30]\n",
    "    if unique_urls:\n",
    "        refs_section = \"\\n## References\\n\\n\"\n",
    "        for i, url in enumerate(unique_urls, 1):\n",
    "            refs_section += f\"{i}. {url}\\n\"\n",
    "        document_parts.append(refs_section)\n",
    "    \n",
    "    final_document = \"\\n\".join(document_parts)\n",
    "    \n",
    "    print(f\"  Document assembled: {len(final_document)} characters\")\n",
    "    \n",
    "    return {\n",
    "        \"assembled_draft\": final_document\n",
    "    }\n",
    "\n",
    "\n",
    "async def final_polish(state: CombinedTier1State) -> dict:\n",
    "    \"\"\"Apply final polish to document.\"\"\"\n",
    "    assembled_draft = state.get(\"assembled_draft\", \"\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 5b: Final Polish\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # For now, just pass through - could add executive summary generation\n",
    "    final_report = assembled_draft\n",
    "    \n",
    "    # Print cache statistics\n",
    "    print(f\"\\n{knowledge_base.get_stats_summary()}\")\n",
    "    \n",
    "    print(f\"\\n  Final report: {len(final_report)} characters\")\n",
    "    print(f\"  Word count: ~{len(final_report.split())}\")\n",
    "    \n",
    "    return {\n",
    "        \"final_report\": final_report\n",
    "    }\n",
    "\n",
    "print(\"Phase 5 node functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5835460d",
   "metadata": {},
   "source": [
    "## 14. Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Combined Tier 1 Agent graph\n",
    "builder = StateGraph(CombinedTier1State)\n",
    "\n",
    "# Phase 1: Research\n",
    "builder.add_node(\"decompose_question\", decompose_question)\n",
    "builder.add_node(\"execute_research_sprint\", execute_research_sprint)\n",
    "builder.add_node(\"research_retrospective\", research_retrospective)\n",
    "builder.add_node(\"quality_gate_1\", quality_gate_1)\n",
    "\n",
    "# Phase 2: Structure\n",
    "builder.add_node(\"generate_skeleton\", generate_skeleton)\n",
    "builder.add_node(\"identify_claim_placeholders\", identify_claim_placeholders)\n",
    "builder.add_node(\"validate_skeleton\", validate_skeleton)\n",
    "\n",
    "# Phase 3: Expansion\n",
    "builder.add_node(\"expand_all_nodes\", expand_all_nodes)\n",
    "builder.add_node(\"extract_inline_claims\", extract_inline_claims)\n",
    "builder.add_node(\"assemble_draft\", assemble_draft)\n",
    "\n",
    "# Phase 4: Verification\n",
    "builder.add_node(\"structured_critique\", structured_critique)\n",
    "builder.add_node(\"quality_gate_2\", quality_gate_2)\n",
    "builder.add_node(\"targeted_retrieval\", targeted_retrieval)\n",
    "builder.add_node(\"apply_patches\", apply_patches)\n",
    "builder.add_node(\"mark_limitations\", mark_limitations)\n",
    "\n",
    "# Phase 5: Finalization\n",
    "builder.add_node(\"assemble_document\", assemble_document)\n",
    "builder.add_node(\"final_polish\", final_polish)\n",
    "\n",
    "# Edges: Phase 1\n",
    "builder.add_edge(START, \"decompose_question\")\n",
    "builder.add_edge(\"decompose_question\", \"execute_research_sprint\")\n",
    "builder.add_edge(\"execute_research_sprint\", \"research_retrospective\")\n",
    "builder.add_conditional_edges(\"research_retrospective\", route_after_retrospective)\n",
    "builder.add_conditional_edges(\"quality_gate_1\", route_after_gate1)\n",
    "\n",
    "# Edges: Phase 2\n",
    "builder.add_edge(\"generate_skeleton\", \"identify_claim_placeholders\")\n",
    "builder.add_edge(\"identify_claim_placeholders\", \"validate_skeleton\")\n",
    "builder.add_conditional_edges(\"validate_skeleton\", route_after_skeleton_validation)\n",
    "\n",
    "# Edges: Phase 3\n",
    "builder.add_edge(\"expand_all_nodes\", \"extract_inline_claims\")\n",
    "builder.add_edge(\"extract_inline_claims\", \"assemble_draft\")\n",
    "builder.add_edge(\"assemble_draft\", \"structured_critique\")\n",
    "\n",
    "# Edges: Phase 4\n",
    "builder.add_edge(\"structured_critique\", \"quality_gate_2\")\n",
    "builder.add_conditional_edges(\"quality_gate_2\", route_after_gate2)\n",
    "builder.add_edge(\"targeted_retrieval\", \"apply_patches\")\n",
    "builder.add_conditional_edges(\"apply_patches\", route_after_patches)\n",
    "builder.add_edge(\"mark_limitations\", \"assemble_document\")\n",
    "\n",
    "# Edges: Phase 5\n",
    "builder.add_edge(\"assemble_document\", \"final_polish\")\n",
    "builder.add_edge(\"final_polish\", END)\n",
    "\n",
    "# Compile\n",
    "combined_tier1_graph = builder.compile()\n",
    "\n",
    "print(\"Combined Tier 1 Agent compiled successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e939ef",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Visualize the graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(combined_tier1_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197de03a",
   "metadata": {},
   "source": [
    "## 15. Agent Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def combined_tier1_agent_async(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Async version of the Combined Tier 1 research agent.\n",
    "    \"\"\"\n",
    "    question = inputs.get(\"question\", \"\")\n",
    "    \n",
    "    # Reset knowledge base for new session\n",
    "    global knowledge_base\n",
    "    knowledge_base = KnowledgeBase()\n",
    "    \n",
    "    result = await combined_tier1_graph.ainvoke(\n",
    "        {\"question\": question},\n",
    "        config={\"recursion_limit\": 100}\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"output\": result.get(\"final_report\", \"\"),\n",
    "        \"sprint_findings\": result.get(\"sprint_findings\", []),\n",
    "        \"source_urls\": list(set(result.get(\"research_source_urls\", []))),\n",
    "        \"quality_scores\": result.get(\"quality_scores\", []),\n",
    "        \"cache_stats\": knowledge_base.get_stats_summary(),\n",
    "        \"limitations\": result.get(\"limitations_noted\", [])\n",
    "    }\n",
    "\n",
    "\n",
    "def combined_tier1_agent(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Sync wrapper for Combined Tier 1 research agent.\n",
    "    Compatible with evaluation harness.\n",
    "    \"\"\"\n",
    "    question = inputs.get(\"question\", \"\")\n",
    "    \n",
    "    async def _execute():\n",
    "        global knowledge_base\n",
    "        knowledge_base = KnowledgeBase()\n",
    "        return await combined_tier1_graph.ainvoke(\n",
    "            {\"question\": question},\n",
    "            config={\"recursion_limit\": 100}\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        import concurrent.futures\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future = executor.submit(asyncio.run, _execute())\n",
    "            result = future.result()\n",
    "    except RuntimeError:\n",
    "        result = asyncio.run(_execute())\n",
    "    \n",
    "    return {\n",
    "        \"output\": result.get(\"final_report\", \"\"),\n",
    "        \"sprint_findings\": result.get(\"sprint_findings\", []),\n",
    "        \"source_urls\": list(set(result.get(\"research_source_urls\", []))),\n",
    "        \"quality_scores\": result.get(\"quality_scores\", []),\n",
    "        \"cache_stats\": knowledge_base.get_stats_summary(),\n",
    "        \"limitations\": result.get(\"limitations_noted\", [])\n",
    "    }\n",
    "\n",
    "print(\"Agent wrappers defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761e4709",
   "metadata": {},
   "source": [
    "## 16. Manual Test\n",
    "\n",
    "Run this cell to verify the agent works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f586e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "test_question = \"What are the key benefits and challenges of using large language models in enterprise applications?\"\n",
    "\n",
    "print(f\"Testing Combined Tier 1 Agent with question:\\n{test_question}\\n\")\n",
    "print(\"Running combined research agent (this may take several minutes)...\\n\")\n",
    "\n",
    "try:\n",
    "    result = await combined_tier1_agent_async({\"question\": test_question})\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"FINAL REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    print(result[\"output\"][:3000] + \"...\" if len(result[\"output\"]) > 3000 else result[\"output\"])\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Report length: {len(result['output'])} characters\")\n",
    "    print(f\"Sprint findings: {len(result.get('sprint_findings', []))}\")\n",
    "    print(f\"Unique sources: {len(result.get('source_urls', []))}\")\n",
    "    print(f\"Quality scores: {result.get('quality_scores', [])}\")\n",
    "    print(f\"\\n{result.get('cache_stats', '')}\")\n",
    "    if result.get(\"limitations\"):\n",
    "        print(f\"Limitations noted: {result['limitations']}\")\n",
    "    print(\"Agent test PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"Agent test FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
