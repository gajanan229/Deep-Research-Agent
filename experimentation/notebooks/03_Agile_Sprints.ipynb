{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paradigm 03: Agile Sprints Research Agent\n",
    "\n",
    "This notebook implements the **Agile ResearchOps** paradigm from the Research Paradigms document.\n",
    "\n",
    "## Core Concept\n",
    "\n",
    "The Agile approach transforms the agent from an open-loop system to a closed-loop control system with:\n",
    "- **Sprint-based execution**: Time-boxed research iterations\n",
    "- **Retrospective reflection**: Course correction after each sprint\n",
    "- **Backlog management**: Dynamic re-prioritization of research questions\n",
    "\n",
    "## Literature Validation\n",
    "\n",
    "> \"Search-o1, R1-Searcher, DeepResearcher, WebDancer... exemplify this paradigm through iterative cycles of explicit reasoning, action, and reflection, aligning with the ReAct framework.\" â€”[Survey-3]\n",
    "\n",
    "## Technology Stack\n",
    "\n",
    "- **LLM**: `gpt-5-mini-2025-08-07`\n",
    "- **Web Search**: Tavily API\n",
    "- **Tracing**: LangSmith\n",
    "- **Framework**: LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configured successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import operator\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from typing import List, Annotated, TypedDict, Literal\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from tavily import TavilyClient\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Load environment variables\n",
    "env_path = Path(\"../.env\")\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Configure LangSmith tracing\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"deep_research_new\"\n",
    "\n",
    "print(\"Environment configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-5-mini-2025-08-07\n",
      "Max sprints: 3\n",
      "Searches per sprint: 5\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM and Tavily client\n",
    "MODEL_NAME = \"gpt-5-mini-2025-08-07\"\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "tavily_client = TavilyClient()\n",
    "\n",
    "# Sprint configuration\n",
    "MAX_SPRINTS = 3\n",
    "SEARCHES_PER_SPRINT = 5\n",
    "MAX_TOKENS_PER_SPRINT = 15000  # ~50k total budget\n",
    "\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "print(f\"Max sprints: {MAX_SPRINTS}\")\n",
    "print(f\"Searches per sprint: {SEARCHES_PER_SPRINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. State Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchQuestion(BaseModel):\n",
    "    \"\"\"A research question in the backlog.\"\"\"\n",
    "    question: str = Field(description=\"The research question to investigate\")\n",
    "    priority: int = Field(default=1, description=\"Priority level (1=highest)\")\n",
    "    status: str = Field(default=\"pending\", description=\"pending, in_progress, or completed\")\n",
    "\n",
    "class SprintFinding(BaseModel):\n",
    "    \"\"\"A finding from a sprint.\"\"\"\n",
    "    question: str = Field(description=\"The question this finding addresses\")\n",
    "    finding: str = Field(description=\"The key finding or insight\")\n",
    "    sources: List[str] = Field(default_factory=list, description=\"Source URLs\")\n",
    "\n",
    "class RetrospectiveOutput(BaseModel):\n",
    "    \"\"\"Output from the retrospective analysis.\"\"\"\n",
    "    what_we_learned: str = Field(description=\"Key learnings from this sprint\")\n",
    "    what_is_still_unclear: str = Field(description=\"Gaps or uncertainties remaining\")\n",
    "    should_continue: bool = Field(description=\"Whether to continue with another sprint\")\n",
    "    updated_priorities: List[str] = Field(description=\"Re-prioritized questions for next sprint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgileResearchState(TypedDict):\n",
    "    \"\"\"State for the Agile Research Agent.\"\"\"\n",
    "    # Input\n",
    "    question: str\n",
    "\n",
    "    # Sprint management - NOTE: backlog is NOT an accumulator, it gets replaced each sprint\n",
    "    backlog: List[str]  # Research questions to investigate (replaced, not accumulated)\n",
    "    current_sprint: int\n",
    "    max_sprints: int\n",
    "\n",
    "    # Findings accumulator (these DO accumulate across sprints)\n",
    "    sprint_findings: Annotated[List[str], operator.add]  # Findings from all sprints\n",
    "    source_urls: Annotated[List[str], operator.add]  # Track all sources for citation\n",
    "\n",
    "    # Retrospective notes\n",
    "    retrospective_notes: Annotated[List[str], operator.add]\n",
    "    findings_summary: str  # Compressed summary to avoid token bloat\n",
    "    should_terminate_early: bool  # NEW: Flag from retrospective to stop early\n",
    "\n",
    "    # Output\n",
    "    final_report: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Node Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "PLAN_BACKLOG_PROMPT = \"\"\"You are a research planning expert. Given a research question, \n",
    "decompose it into 4-6 specific sub-questions that need to be investigated.\n",
    "\n",
    "Research Question: {question}\n",
    "\n",
    "Generate a prioritized list of specific, focused research questions that together will answer the main question.\n",
    "Each question should be independently searchable and cover a different aspect.\n",
    "\n",
    "Return your response as a numbered list (highest priority first):\n",
    "1. [Most critical sub-question]\n",
    "2. [Second priority sub-question]\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "SPRINT_RESEARCH_PROMPT = \"\"\"You are a research agent conducting Sprint {sprint_num} of {max_sprints}.\n",
    "\n",
    "Current research focus: {current_question}\n",
    "\n",
    "Based on the search results below, extract the key findings that address the research question.\n",
    "Be specific and cite sources with URLs where relevant.\n",
    "\n",
    "Search Results:\n",
    "{search_results}\n",
    "\n",
    "Provide a comprehensive summary of findings (400-600 words). Include specific facts, statistics, and source URLs.\n",
    "\"\"\"\n",
    "\n",
    "SUMMARIZE_FINDINGS_PROMPT = \"\"\"Summarize the key findings from the research sprints so far into a concise format.\n",
    "\n",
    "All Findings:\n",
    "{all_findings}\n",
    "\n",
    "Create a bullet-point summary (max 500 words) capturing:\n",
    "- Key facts and statistics discovered\n",
    "- Main themes and patterns\n",
    "- Important sources cited\n",
    "- Any contradictions or debates identified\n",
    "\n",
    "Be concise but preserve critical information.\n",
    "\"\"\"\n",
    "\n",
    "RETROSPECTIVE_PROMPT = \"\"\"You are conducting a sprint retrospective for a research project.\n",
    "\n",
    "Original Question: {original_question}\n",
    "\n",
    "Sprint {sprint_num} of {max_sprints} has completed.\n",
    "\n",
    "Summary of findings so far:\n",
    "{findings_summary}\n",
    "\n",
    "Current remaining questions in backlog:\n",
    "{remaining_backlog}\n",
    "\n",
    "Analyze the progress and provide a STRUCTURED response:\n",
    "\n",
    "## LEARNINGS\n",
    "What key insights did we gain this sprint?\n",
    "\n",
    "## GAPS\n",
    "What is still unclear or needs more investigation?\n",
    "\n",
    "## CONTINUE\n",
    "Should we continue with another sprint? Answer YES or NO with brief justification.\n",
    "\n",
    "## NEW_QUESTIONS\n",
    "List any NEW questions that emerged from our research (or \"None\" if none):\n",
    "- [New question 1]\n",
    "- [New question 2]\n",
    "\n",
    "## REPRIORITIZED_BACKLOG\n",
    "Reorder the remaining questions by priority, incorporating any new questions:\n",
    "1. [Highest priority question]\n",
    "2. [Next priority]\n",
    "...\n",
    "\n",
    "Be honest about gaps and uncertainties. Focus on what will most help answer the original question.\n",
    "\"\"\"\n",
    "\n",
    "FINAL_REPORT_PROMPT = \"\"\"You are a senior research analyst writing a final report.\n",
    "\n",
    "Original Question: {original_question}\n",
    "\n",
    "All Research Findings:\n",
    "{all_findings}\n",
    "\n",
    "Retrospective Insights:\n",
    "{retrospective_notes}\n",
    "\n",
    "Source URLs used:\n",
    "{source_urls}\n",
    "\n",
    "Write a comprehensive research report that:\n",
    "1. Directly answers the original question with evidence\n",
    "2. Synthesizes findings across all sprints into coherent analysis\n",
    "3. Acknowledges any remaining uncertainties or limitations\n",
    "4. Includes inline citations with source URLs\n",
    "\n",
    "The report should be well-structured with clear sections and approximately 1000-1500 words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web(query: str, max_results: int = 5) -> tuple[str, List[str]]:\n",
    "    \"\"\"Execute web search using Tavily. Returns (formatted_results, list_of_urls).\"\"\"\n",
    "    try:\n",
    "        # Truncate query if too long (Tavily limit)\n",
    "        if len(query) > 400:\n",
    "            query = query[:400]\n",
    "\n",
    "        response = tavily_client.search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            include_answer=True\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        urls = []\n",
    "\n",
    "        if response.get(\"answer\"):\n",
    "            results.append(f\"Summary: {response['answer']}\")\n",
    "\n",
    "        for r in response.get(\"results\", []):\n",
    "            url = r.get('url', 'N/A')\n",
    "            urls.append(url)\n",
    "            results.append(f\"- {r.get('title', 'No title')}: {r.get('content', '')[:500]}... (Source: {url})\")\n",
    "\n",
    "        return \"\\n\\n\".join(results), urls\n",
    "    except Exception as e:\n",
    "        return f\"Search error: {str(e)}\", []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def plan_backlog(state: AgileResearchState) -> dict:\n",
    "    \"\"\"Plan the initial research backlog by decomposing the question.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    prompt = PLAN_BACKLOG_PROMPT.format(question=question)\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    # Parse numbered list from response\n",
    "    lines = response.content.strip().split(\"\\n\")\n",
    "    backlog = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and (line[0].isdigit() or line.startswith(\"-\")):\n",
    "            # Remove numbering and clean up\n",
    "            clean = line.lstrip(\"0123456789.-) \").strip()\n",
    "            if clean:\n",
    "                backlog.append(clean)\n",
    "    \n",
    "    print(f\"Created backlog with {len(backlog)} research questions\")\n",
    "    \n",
    "    return {\n",
    "        \"backlog\": backlog,\n",
    "        \"current_sprint\": 1,\n",
    "        \"max_sprints\": MAX_SPRINTS\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_sprint(state: AgileResearchState) -> dict:\n",
    "    \"\"\"Execute a research sprint on the current backlog item.\"\"\"\n",
    "    backlog = state.get(\"backlog\", [])\n",
    "    current_sprint = state.get(\"current_sprint\", 1)\n",
    "    max_sprints = state.get(\"max_sprints\", MAX_SPRINTS)\n",
    "\n",
    "    if not backlog:\n",
    "        return {\"sprint_findings\": [\"No questions in backlog to research.\"]}\n",
    "\n",
    "    # Get current question (first in backlog)\n",
    "    current_question = backlog[0]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Sprint {current_sprint}/{max_sprints}: {current_question[:80]}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Execute searches\n",
    "    search_results = []\n",
    "    all_urls = []\n",
    "    queries = [current_question]  # Main query\n",
    "\n",
    "    # Generate additional search queries for depth\n",
    "    if SEARCHES_PER_SPRINT > 1:\n",
    "        query_prompt = f\"\"\"Generate {SEARCHES_PER_SPRINT - 1} specific web search queries to investigate this question from different angles:\n",
    "        Question: {current_question}\n",
    "\n",
    "        Return only the search queries, one per line.\"\"\"\n",
    "        query_response = await llm.ainvoke([HumanMessage(content=query_prompt)])\n",
    "        additional_queries = [q.strip() for q in query_response.content.split(\"\\n\") if q.strip()]\n",
    "        queries.extend(additional_queries[:SEARCHES_PER_SPRINT - 1])\n",
    "\n",
    "    # Execute all searches\n",
    "    for query in queries:\n",
    "        print(f\"  Searching: {query[:60]}...\")\n",
    "        result, urls = search_web(query)\n",
    "        search_results.append(result)\n",
    "        all_urls.extend(urls)\n",
    "\n",
    "    combined_results = \"\\n\\n---\\n\\n\".join(search_results)\n",
    "\n",
    "    # Synthesize findings\n",
    "    synthesis_prompt = SPRINT_RESEARCH_PROMPT.format(\n",
    "        sprint_num=current_sprint,\n",
    "        max_sprints=max_sprints,\n",
    "        current_question=current_question,\n",
    "        search_results=combined_results\n",
    "    )\n",
    "\n",
    "    synthesis = await llm.ainvoke([HumanMessage(content=synthesis_prompt)])\n",
    "\n",
    "    finding = f\"## Sprint {current_sprint} Findings: {current_question}\\n\\n{synthesis.content}\"\n",
    "    print(f\"  Synthesized {len(synthesis.content)} characters of findings\")\n",
    "    print(f\"  Collected {len(all_urls)} source URLs\")\n",
    "\n",
    "    # Remove processed question from backlog (backlog is NOT an accumulator, so this replaces it)\n",
    "    updated_backlog = backlog[1:] if len(backlog) > 1 else []\n",
    "\n",
    "    return {\n",
    "        \"sprint_findings\": [finding],\n",
    "        \"source_urls\": all_urls,\n",
    "        \"backlog\": updated_backlog  # This REPLACES the backlog since it's not an Annotated accumulator\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "async def summarize_findings(all_findings: str) -> str:\n",
    "    \"\"\"Summarize findings to manage token budget.\"\"\"\n",
    "    if len(all_findings) < 2000:\n",
    "        return all_findings  # No need to summarize if short\n",
    "\n",
    "    prompt = SUMMARIZE_FINDINGS_PROMPT.format(all_findings=all_findings[:8000])\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "\n",
    "def parse_reprioritized_backlog(response_content: str) -> List[str]:\n",
    "    \"\"\"Parse the reprioritized backlog from retrospective output.\"\"\"\n",
    "    # Look for the REPRIORITIZED_BACKLOG section\n",
    "    backlog_match = re.search(\n",
    "        r'## REPRIORITIZED_BACKLOG\\s*(.*?)(?=##|$)',\n",
    "        response_content,\n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    if not backlog_match:\n",
    "        # Fallback: look for numbered list at the end\n",
    "        backlog_match = re.search(r'(?:backlog|priority).*?:\\s*((?:\\d+\\..*?\\n?)+)', response_content, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    if backlog_match:\n",
    "        backlog_text = backlog_match.group(1)\n",
    "        # Parse numbered list\n",
    "        questions = []\n",
    "        for line in backlog_text.strip().split(\"\\n\"):\n",
    "            line = line.strip()\n",
    "            if line and (line[0].isdigit() or line.startswith(\"-\")):\n",
    "                clean = re.sub(r'^[\\d\\.\\-\\)\\s]+', '', line).strip()\n",
    "                if clean and len(clean) > 10:  # Filter out too-short items\n",
    "                    questions.append(clean)\n",
    "        return questions\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "def parse_should_continue(response_content: str) -> bool:\n",
    "    \"\"\"Parse whether the retrospective recommends continuing.\n",
    "    \n",
    "    Handles various negative phrasings:\n",
    "    - \"NO\" / \"No\" / \"no\"\n",
    "    - \"Not at this time\" / \"Not necessary\"\n",
    "    - \"Negative\"\n",
    "    - \"We should stop\"\n",
    "    - \"No need to continue\"\n",
    "    \"\"\"\n",
    "    # Look for CONTINUE section\n",
    "    continue_match = re.search(\n",
    "        r'## CONTINUE\\s*(.*?)(?=##|$)',\n",
    "        response_content,\n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    if continue_match:\n",
    "        continue_text = continue_match.group(1).strip().lower()\n",
    "        \n",
    "        # Check for negative patterns (more comprehensive)\n",
    "        negative_patterns = [\n",
    "            r'^no\\b',           # Starts with \"no\" (word boundary)\n",
    "            r'^not\\b',          # Starts with \"not\" (not necessary, not at this time)\n",
    "            r'^negative',       # Starts with \"negative\"\n",
    "            r'should\\s+stop',   # \"should stop\"\n",
    "            r'should\\s+not\\s+continue',  # \"should not continue\"\n",
    "            r'no\\s+need',       # \"no need to continue\"\n",
    "            r'sufficient',      # \"question is sufficiently answered\"\n",
    "            r'adequately\\s+answered',  # \"adequately answered\"\n",
    "            r'fully\\s+answered',  # \"fully answered\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in negative_patterns:\n",
    "            if re.search(pattern, continue_text):\n",
    "                return False  # Don't continue\n",
    "    \n",
    "    # Default to continuing if no clear negative signal\n",
    "    return True\n",
    "\n",
    "\n",
    "async def retrospective(state: AgileResearchState) -> dict:\n",
    "    \"\"\"Conduct retrospective analysis after sprint with dynamic backlog re-prioritization.\"\"\"\n",
    "    original_question = state[\"question\"]\n",
    "    current_sprint = state.get(\"current_sprint\", 1)\n",
    "    max_sprints = state.get(\"max_sprints\", MAX_SPRINTS)\n",
    "    all_findings = \"\\n\\n\".join(state.get(\"sprint_findings\", []))\n",
    "    remaining_backlog = state.get(\"backlog\", [])\n",
    "\n",
    "    # Summarize findings to manage token budget\n",
    "    findings_summary = await summarize_findings(all_findings)\n",
    "\n",
    "    prompt = RETROSPECTIVE_PROMPT.format(\n",
    "        original_question=original_question,\n",
    "        sprint_num=current_sprint,\n",
    "        max_sprints=max_sprints,\n",
    "        findings_summary=findings_summary,\n",
    "        remaining_backlog=\"\\n\".join(f\"- {q}\" for q in remaining_backlog) if remaining_backlog else \"None - all initial questions addressed\"\n",
    "    )\n",
    "\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # Parse the reprioritized backlog from the response\n",
    "    reprioritized_backlog = parse_reprioritized_backlog(response.content)\n",
    "    \n",
    "    # Parse whether to continue\n",
    "    should_continue = parse_should_continue(response.content)\n",
    "\n",
    "    # If parsing failed, fall back to existing backlog\n",
    "    if not reprioritized_backlog and remaining_backlog:\n",
    "        reprioritized_backlog = remaining_backlog\n",
    "        print(f\"  Retrospective: Using existing backlog (parsing failed)\")\n",
    "    else:\n",
    "        print(f\"  Retrospective: Reprioritized to {len(reprioritized_backlog)} questions\")\n",
    "    \n",
    "    if not should_continue:\n",
    "        print(f\"  Retrospective: Recommends STOPPING early (question sufficiently answered)\")\n",
    "\n",
    "    retro_note = f\"### Sprint {current_sprint} Retrospective\\n\\n{response.content}\"\n",
    "\n",
    "    return {\n",
    "        \"retrospective_notes\": [retro_note],\n",
    "        \"current_sprint\": current_sprint + 1,\n",
    "        \"backlog\": reprioritized_backlog,  # REPLACE with reprioritized backlog\n",
    "        \"findings_summary\": findings_summary,  # Store for next retrospective\n",
    "        \"should_terminate_early\": not should_continue  # Set termination flag\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue_sprinting(state: AgileResearchState) -> Literal[\"execute_sprint\", \"write_report\"]:\n",
    "    \"\"\"Decide whether to continue with another sprint or write the final report.\"\"\"\n",
    "    current_sprint = state.get(\"current_sprint\", 1)\n",
    "    max_sprints = state.get(\"max_sprints\", MAX_SPRINTS)\n",
    "    backlog = state.get(\"backlog\", [])\n",
    "    should_terminate_early = state.get(\"should_terminate_early\", False)\n",
    "    \n",
    "    # Stop conditions:\n",
    "    # 1. Reached max sprints\n",
    "    # 2. Backlog is empty\n",
    "    # 3. Retrospective recommends early termination (question answered)\n",
    "    if current_sprint > max_sprints:\n",
    "        print(f\"\\nMax sprints ({max_sprints}) reached. Moving to final report.\")\n",
    "        return \"write_report\"\n",
    "    \n",
    "    if not backlog:\n",
    "        print(f\"\\nBacklog empty after sprint {current_sprint - 1}. Moving to final report.\")\n",
    "        return \"write_report\"\n",
    "    \n",
    "    if should_terminate_early:\n",
    "        print(f\"\\nRetrospective recommends early termination. Moving to final report.\")\n",
    "        return \"write_report\"\n",
    "    \n",
    "    print(f\"\\nContinuing to sprint {current_sprint}. {len(backlog)} questions remaining.\")\n",
    "    return \"execute_sprint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def write_report(state: AgileResearchState) -> dict:\n",
    "    \"\"\"Write the final research report.\"\"\"\n",
    "    original_question = state[\"question\"]\n",
    "    all_findings = \"\\n\\n\".join(state.get(\"sprint_findings\", []))\n",
    "    findings_summary = state.get(\"findings_summary\", \"\")\n",
    "    retrospective_notes = \"\\n\\n\".join(state.get(\"retrospective_notes\", []))\n",
    "    source_urls = list(set(state.get(\"source_urls\", [])))  # Deduplicate\n",
    "\n",
    "    # Token efficiency: use summary if findings are too long\n",
    "    # Full findings can be 1800+ words with 3 sprints, which risks token limits\n",
    "    if len(all_findings) > 6000 and findings_summary:\n",
    "        print(f\"  Using findings summary ({len(findings_summary)} chars) instead of full findings ({len(all_findings)} chars)\")\n",
    "        findings_for_report = findings_summary\n",
    "    else:\n",
    "        findings_for_report = all_findings\n",
    "    \n",
    "    # Also truncate retrospective notes if too long\n",
    "    if len(retrospective_notes) > 3000:\n",
    "        retrospective_notes = retrospective_notes[:3000] + \"\\n\\n[... truncated for brevity ...]\"\n",
    "\n",
    "    prompt = FINAL_REPORT_PROMPT.format(\n",
    "        original_question=original_question,\n",
    "        all_findings=findings_for_report,\n",
    "        retrospective_notes=retrospective_notes,\n",
    "        source_urls=\"\\n\".join(source_urls[:30])  # Limit to top 30 sources\n",
    "    )\n",
    "\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    print(f\"\\nFinal report generated: {len(response.content)} characters\")\n",
    "    print(f\"Sources used: {len(source_urls)} unique URLs\")\n",
    "\n",
    "    return {\n",
    "        \"final_report\": response.content\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agile Research Agent compiled successfully\n"
     ]
    }
   ],
   "source": [
    "# Build the Agile Research Agent graph\n",
    "agile_builder = StateGraph(AgileResearchState)\n",
    "\n",
    "# Add nodes\n",
    "agile_builder.add_node(\"plan_backlog\", plan_backlog)\n",
    "agile_builder.add_node(\"execute_sprint\", execute_sprint)\n",
    "agile_builder.add_node(\"retrospective\", retrospective)\n",
    "agile_builder.add_node(\"write_report\", write_report)\n",
    "\n",
    "# Add edges\n",
    "agile_builder.add_edge(START, \"plan_backlog\")\n",
    "agile_builder.add_edge(\"plan_backlog\", \"execute_sprint\")\n",
    "agile_builder.add_edge(\"execute_sprint\", \"retrospective\")\n",
    "\n",
    "# Conditional edge: continue sprinting or write report\n",
    "agile_builder.add_conditional_edges(\n",
    "    \"retrospective\",\n",
    "    should_continue_sprinting,\n",
    "    {\n",
    "        \"execute_sprint\": \"execute_sprint\",\n",
    "        \"write_report\": \"write_report\"\n",
    "    }\n",
    ")\n",
    "\n",
    "agile_builder.add_edge(\"write_report\", END)\n",
    "\n",
    "# Compile\n",
    "agile_graph = agile_builder.compile()\n",
    "\n",
    "print(\"Agile Research Agent compiled successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(agile_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Agent Wrapper for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def agile_sprints_agent_async(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Async version of the Agile Sprints research agent.\n",
    "    Use this version when calling from Jupyter notebooks.\n",
    "    \"\"\"\n",
    "    question = inputs.get(\"question\", \"\")\n",
    "\n",
    "    result = await agile_graph.ainvoke(\n",
    "        {\"question\": question},\n",
    "        config={\"recursion_limit\": 50}\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"output\": result.get(\"final_report\", \"\"),\n",
    "        \"sprint_findings\": result.get(\"sprint_findings\", []),\n",
    "        \"retrospective_notes\": result.get(\"retrospective_notes\", []),\n",
    "        \"source_urls\": list(set(result.get(\"source_urls\", []))),  # Deduplicated URLs\n",
    "        \"num_sprints\": result.get(\"current_sprint\", 1) - 1  # Actual sprints completed\n",
    "    }\n",
    "\n",
    "\n",
    "def agile_sprints_agent(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Sync wrapper function for Agile Sprints research agent.\n",
    "\n",
    "    Compatible with evaluation harness.\n",
    "\n",
    "    Args:\n",
    "        inputs: Dictionary with 'question' key\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with 'output' key containing final report\n",
    "    \"\"\"\n",
    "    question = inputs.get(\"question\", \"\")\n",
    "\n",
    "    async def _execute():\n",
    "        return await agile_graph.ainvoke(\n",
    "            {\"question\": question},\n",
    "            config={\"recursion_limit\": 50}\n",
    "        )\n",
    "\n",
    "    # Check if we're already in an async context (e.g., Jupyter notebook)\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        # We're in an event loop - need to run in a separate thread\n",
    "        import concurrent.futures\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future = executor.submit(asyncio.run, _execute())\n",
    "            result = future.result()\n",
    "    except RuntimeError:\n",
    "        # No event loop running, safe to use asyncio.run\n",
    "        result = asyncio.run(_execute())\n",
    "\n",
    "    return {\n",
    "        \"output\": result.get(\"final_report\", \"\"),\n",
    "        \"sprint_findings\": result.get(\"sprint_findings\", []),\n",
    "        \"retrospective_notes\": result.get(\"retrospective_notes\", []),\n",
    "        \"source_urls\": list(set(result.get(\"source_urls\", []))),  # Deduplicated URLs\n",
    "        \"num_sprints\": result.get(\"current_sprint\", 1) - 1  # Actual sprints completed\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Manual Test\n",
    "\n",
    "Run this cell to verify the agent works correctly with a simple test question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Agile Sprints Agent with question:\n",
      "What are the key benefits and challenges of using large language models in enterprise applications?\n",
      "\n",
      "Running sprint-based research (this may take several minutes)...\n",
      "\n",
      "Created backlog with 6 research questions\n",
      "\n",
      "============================================================\n",
      "Sprint 1/3: What measurable business benefits (productivity gains, cost savings, revenue imp...\n",
      "============================================================\n",
      "  Searching: What measurable business benefits (productivity gains, cost ...\n",
      "  Searching: enterprise ROI of large language models quantified \"producti...\n",
      "  Searching: LLM deployment case study customer support \"handle time\" \"fi...\n",
      "  Searching: methodology to measure ROI of LLMs \"KPIs\" \"productivity metr...\n",
      "  Searching: developer productivity large language models \"GitHub Copilot...\n",
      "  Synthesized 4839 characters of findings\n",
      "  Collected 25 source URLs\n",
      "  Retrospective: Reprioritized to 20 questions\n",
      "\n",
      "Continuing to sprint 2. 20 questions remaining.\n",
      "\n",
      "============================================================\n",
      "Sprint 2/3: What are the principal data privacy, security, and compliance risks (including d...\n",
      "============================================================\n",
      "  Searching: What are the principal data privacy, security, and complianc...\n",
      "  Searching: LLM enterprise data leakage risks \"prompt injection\" \"model ...\n",
      "  Searching: LLM regulatory compliance for enterprises GDPR HIPAA CCPA da...\n",
      "  Searching: Thirdâ€‘party LLM API vendor risk enterprise contracts SOC 2 I...\n",
      "  Searching: Secure LLM deployment best practices NIST AI RMF ENISA guida...\n",
      "  Synthesized 5655 characters of findings\n",
      "  Collected 25 source URLs\n",
      "  Retrospective: Reprioritized to 27 questions\n",
      "\n",
      "Continuing to sprint 3. 27 questions remaining.\n",
      "\n",
      "============================================================\n",
      "Sprint 3/3: What are the measured incident rates and average remediation costs for real-worl...\n",
      "============================================================\n",
      "  Searching: What are the measured incident rates and average remediation...\n",
      "  Searching: measured incident rate and remediation cost of LLM hallucina...\n",
      "  Searching: measured frequency and remediation cost of LLM data leakage ...\n",
      "  Searching: measured incident rates and average remediation costs for se...\n",
      "  Searching: industry-by-industry measured costs and incident rates of LL...\n",
      "  Synthesized 5296 characters of findings\n",
      "  Collected 22 source URLs\n",
      "  Retrospective: Reprioritized to 22 questions\n",
      "\n",
      "Max sprints (3) reached. Moving to final report.\n",
      "  Using findings summary (3477 chars) instead of full findings (16499 chars)\n",
      "\n",
      "Final report generated: 12064 characters\n",
      "Sources used: 67 unique URLs\n",
      "================================================================================\n",
      "FINAL REPORT\n",
      "================================================================================\n",
      "Executive summary\n",
      "Large language models (LLMs) can deliver substantial, measurable benefits in enterprise settings â€” particularly for repeatable, high-volume tasks such as content/document workflows, customer operations, software engineering, and certain R&D activities â€” but they also introduce material security, privacy, reliability, and operational risks that can erode or reverse net value unless actively managed. Empirical and vendor/analyst reports show productivity and time-to-market gains often in the tens of percent, but real-world outcomes vary and depend on use-case scoping, measurement discipline, and controls. Key unresolved gaps include systematic quantification of security incident likelihood/impact, total cost of ownership (TCO) under realistic workloads, and production failure rates for hallucinations and remediation effort [https://www.mckinsey.com/capabilities/tech-and-ai/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier; https://www.worklytics.co/resources/calculating-roi-generative-ai-tools-worklytics-framework; https://www.paradisosolutions.com/blog/measure-ai-productivity-gains-metrics/].\n",
      "\n",
      "1) Key benefits (evidence-based)\n",
      "- Large, repeatable time savings and productivity gains. Multiple industry reports and case studies show sizeable speed-ups for narrowly scoped tasks:\n",
      "  - Content & document workflows: reported reductions in content-creation time and faster document processing in vendor/industry accounts (typical headline improvements repeatedly cited are tens of percent) [https://www.paradisosolutions.com/blog/measure-ai-productivity-gains-metrics/].\n",
      "  - Software engineering: developer-assistants (e.g., GitHub Copilot and similar tools) have been associated with productivity gains commonly reported in the ~30â€“55% range, with broad adoption among engineers [https://www.aboutchromebooks.com/github-copilot-statistics/; https://www.mckinsey.com/capabilities/tech-and-ai/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier].\n",
      "  - Economic concentration: McKinsey estimates roughly 75% of generative-AI economic potential is concentrated in customer operations, marketing & sales, software engineering, and R&D â€” indicating where enterprises should prioritize pilots for near-term value [https://www.mckinsey.com/capabilities/tech-and-ai/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier].\n",
      "\n",
      "- Faster time-to-market and scaling of content-driven programs. For use cases with high volume and repeatability (tickets, summaries, templated copy, first drafts), LLMs reduce time-per-transaction and enable greater throughput at similar headcount, producing measurable campaign or service capacity lift when integrated with measurement frameworks [https://www.worklytics.co/resources/calculating-roi-generative-ai-tools-worklytics-framework; https://www.kumohq.co/blog/ai-roi-measurement-framework].\n",
      "\n",
      "- New capabilities and automation paths. LLMs enable novel...\n",
      "\n",
      "================================================================================\n",
      "Report length: 12064 characters\n",
      "Sprints completed: 3\n",
      "Sprint findings collected: 3\n",
      "Retrospectives conducted: 3\n",
      "Unique sources: 67\n",
      "Agent test PASSED\n"
     ]
    }
   ],
   "source": [
    "# Simple test\n",
    "test_question = \"What are the key benefits and challenges of using large language models in enterprise applications?\"\n",
    "\n",
    "print(f\"Testing Agile Sprints Agent with question:\\n{test_question}\\n\")\n",
    "print(\"Running sprint-based research (this may take several minutes)...\\n\")\n",
    "\n",
    "try:\n",
    "    # Use the async version in notebooks to avoid event loop conflicts\n",
    "    result = await agile_sprints_agent_async({\"question\": test_question})\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"FINAL REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    print(result[\"output\"][:3000] + \"...\" if len(result[\"output\"]) > 3000 else result[\"output\"])\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Report length: {len(result['output'])} characters\")\n",
    "    print(f\"Sprints completed: {result.get('num_sprints', 'N/A')}\")\n",
    "    print(f\"Sprint findings collected: {len(result.get('sprint_findings', []))}\")\n",
    "    print(f\"Retrospectives conducted: {len(result.get('retrospective_notes', []))}\")\n",
    "    print(f\"Unique sources: {len(result.get('source_urls', []))}\")\n",
    "    print(\"Agent test PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"Agent test FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Harness Integration\n",
    "\n",
    "Once the manual test passes, uncomment and run the cells below for full evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation harness and metrics\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from evaluation import (\n",
    "    ExperimentHarness, \n",
    "    fact_recall, \n",
    "    citation_precision,\n",
    "    coherence_judge, \n",
    "    depth_judge, \n",
    "    relevance_judge,\n",
    "    minimum_sources_check\n",
    ")\n",
    "\n",
    "# Initialize harness with the golden test dataset\n",
    "harness = ExperimentHarness(\n",
    "    dataset_path=\"../data/deep_research_agent_test_dataset.yaml\",\n",
    "    langsmith_dataset_name=\"deep-research-golden-v2\"\n",
    ")\n",
    "\n",
    "print(\"Evaluation harness initialized successfully!\")\n",
    "print(f\"Dataset: {harness.dataset_path}\")\n",
    "print(f\"LangSmith dataset name: {harness.langsmith_dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Full Evaluation\n",
    "\n",
    "This runs the complete evaluation on all 20 questions.\n",
    "\n",
    "**âš ï¸ WARNING:** This is expensive and time-consuming!\n",
    "- **Expected runtime:** 1-2 hours\n",
    "- Single run to reduce cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Evaluation on All 20 Questions\n",
    "# âš ï¸ EXPENSIVE - Only uncomment when ready for full evaluation\n",
    "# Uncomment to run:\n",
    "\n",
    "# # Define comprehensive evaluator suite\n",
    "# evaluators = [\n",
    "#     fact_recall,              # Required facts coverage\n",
    "#     citation_precision,       # Citation URL validity\n",
    "#     minimum_sources_check,    # Minimum source count\n",
    "#     coherence_judge,          # Logical structure\n",
    "#     depth_judge,              # Analysis depth\n",
    "#     relevance_judge,          # Addresses question\n",
    "# ]\n",
    "# \n",
    "# # Run full evaluation\n",
    "# print(\"Starting FULL evaluation on all 20 questions...\")\n",
    "# print(\"Agile Sprints Agent - this will take 1-2 hours.\")\n",
    "# print(\"=\" * 80 + \"\\n\")\n",
    "# \n",
    "# results = harness.run_evaluation(\n",
    "#     agent_fn=agile_sprints_agent,\n",
    "#     evaluators=evaluators,\n",
    "#     experiment_name=\"agile_sprints_v1\",\n",
    "#     monte_carlo_runs=1,  # Single run to reduce cost\n",
    "#     max_concurrency=2,   # Lower concurrency for stability\n",
    "#     description=\"Agile Sprints paradigm evaluation on all difficulty tiers\"\n",
    "# )\n",
    "# \n",
    "# # Display comprehensive results\n",
    "# print(\"\\n\" + \"=\" * 80)\n",
    "# print(\"FULL EVALUATION RESULTS\")\n",
    "# print(\"=\" * 80)\n",
    "# print(f\"Experiment: {results.experiment_name}\")\n",
    "# print(f\"Questions evaluated: {results.num_questions}\")\n",
    "# print(f\"Runs per question: {results.num_runs}\")\n",
    "# \n",
    "# print(f\"\\n{'Metric':<30} {'Mean':<10}\")\n",
    "# print(\"-\" * 40)\n",
    "# for metric_name in sorted(results.metrics.keys()):\n",
    "#     if not metric_name.endswith('_std'):\n",
    "#         value = results.metrics.get(metric_name, 0)\n",
    "#         print(f\"{metric_name:<30} {value:<10.3f}\")\n",
    "# \n",
    "# # Save results to file\n",
    "# import json\n",
    "# from datetime import datetime\n",
    "# \n",
    "# results_file = Path(\"../results\") / f\"agile_sprints_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "# results_file.parent.mkdir(exist_ok=True)\n",
    "# \n",
    "# with open(results_file, 'w') as f:\n",
    "#     json.dump({\n",
    "#         \"experiment_name\": results.experiment_name,\n",
    "#         \"num_questions\": results.num_questions,\n",
    "#         \"num_runs\": results.num_runs,\n",
    "#         \"metrics\": results.metrics,\n",
    "#         \"per_question\": results.per_question_results\n",
    "#     }, f, indent=2)\n",
    "# \n",
    "# print(f\"\\nResults saved to: {results_file}\")\n",
    "\n",
    "print(\"Full evaluation cell ready. Uncomment to run when ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Viewing Results in LangSmith\n",
    "\n",
    "After running an evaluation, you can view detailed results in the LangSmith UI:\n",
    "\n",
    "1. Go to https://smith.langchain.com\n",
    "2. Navigate to your project (`deep_research_new`)\n",
    "3. Click on \"Datasets\" to see your test dataset\n",
    "4. Click on \"Experiments\" to see evaluation runs\n",
    "5. Compare Agile Sprints vs Baseline results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
