{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paradigm 03: Agile Sprints Research Agent\n",
    "\n",
    "This notebook implements the **Agile ResearchOps** paradigm from the Research Paradigms document.\n",
    "\n",
    "## Core Concept\n",
    "\n",
    "The Agile approach transforms the agent from an open-loop system to a closed-loop control system with:\n",
    "- **Sprint-based execution**: Time-boxed research iterations\n",
    "- **Retrospective reflection**: Course correction after each sprint\n",
    "- **Backlog management**: Dynamic re-prioritization of research questions\n",
    "\n",
    "## Literature Validation\n",
    "\n",
    "> \"Search-o1, R1-Searcher, DeepResearcher, WebDancer... exemplify this paradigm through iterative cycles of explicit reasoning, action, and reflection, aligning with the ReAct framework.\" —[Survey-3]\n",
    "\n",
    "## Technology Stack\n",
    "\n",
    "- **LLM**: `gpt-5-mini-2025-08-07`\n",
    "- **Web Search**: Tavily API\n",
    "- **Tracing**: LangSmith\n",
    "- **Framework**: LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from typing import List, Annotated, TypedDict, Literal\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from tavily import TavilyClient\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Load environment variables\n",
    "env_path = Path(\"../.env\")\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Configure LangSmith tracing\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"deep_research_new\"\n",
    "\n",
    "print(\"Environment configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM and Tavily client\n",
    "MODEL_NAME = \"gpt-5-mini-2025-08-07\"\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "tavily_client = TavilyClient()\n",
    "\n",
    "# Sprint configuration\n",
    "MAX_SPRINTS = 3\n",
    "SEARCHES_PER_SPRINT = 5\n",
    "MAX_TOKENS_PER_SPRINT = 15000  # ~50k total budget\n",
    "\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "print(f\"Max sprints: {MAX_SPRINTS}\")\n",
    "print(f\"Searches per sprint: {SEARCHES_PER_SPRINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. State Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchQuestion(BaseModel):\n",
    "    \"\"\"A research question in the backlog.\"\"\"\n",
    "    question: str = Field(description=\"The research question to investigate\")\n",
    "    priority: int = Field(default=1, description=\"Priority level (1=highest)\")\n",
    "    status: str = Field(default=\"pending\", description=\"pending, in_progress, or completed\")\n",
    "\n",
    "class SprintFinding(BaseModel):\n",
    "    \"\"\"A finding from a sprint.\"\"\"\n",
    "    question: str = Field(description=\"The question this finding addresses\")\n",
    "    finding: str = Field(description=\"The key finding or insight\")\n",
    "    sources: List[str] = Field(default_factory=list, description=\"Source URLs\")\n",
    "\n",
    "class RetrospectiveOutput(BaseModel):\n",
    "    \"\"\"Output from the retrospective analysis.\"\"\"\n",
    "    what_we_learned: str = Field(description=\"Key learnings from this sprint\")\n",
    "    what_is_still_unclear: str = Field(description=\"Gaps or uncertainties remaining\")\n",
    "    should_continue: bool = Field(description=\"Whether to continue with another sprint\")\n",
    "    updated_priorities: List[str] = Field(description=\"Re-prioritized questions for next sprint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgileResearchState(TypedDict):\n",
    "    \"\"\"State for the Agile Research Agent.\"\"\"\n",
    "    # Input\n",
    "    question: str\n",
    "    \n",
    "    # Sprint management\n",
    "    backlog: Annotated[List[str], operator.add]  # Research questions to investigate\n",
    "    current_sprint: int\n",
    "    max_sprints: int\n",
    "    \n",
    "    # Findings accumulator\n",
    "    sprint_findings: Annotated[List[str], operator.add]  # Findings from all sprints\n",
    "    \n",
    "    # Retrospective notes\n",
    "    retrospective_notes: Annotated[List[str], operator.add]\n",
    "    \n",
    "    # Output\n",
    "    final_report: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Node Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "PLAN_BACKLOG_PROMPT = \"\"\"You are a research planning expert. Given a research question, \n",
    "decompose it into 3-5 specific sub-questions that need to be investigated.\n",
    "\n",
    "Research Question: {question}\n",
    "\n",
    "Generate a list of specific, focused research questions that together will answer the main question.\n",
    "Each question should be independently searchable.\n",
    "\n",
    "Return your response as a numbered list:\n",
    "1. [First sub-question]\n",
    "2. [Second sub-question]\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "SPRINT_RESEARCH_PROMPT = \"\"\"You are a research agent conducting Sprint {sprint_num} of {max_sprints}.\n",
    "\n",
    "Current research focus: {current_question}\n",
    "\n",
    "Based on the search results below, extract the key findings that address the research question.\n",
    "Be specific and cite sources where relevant.\n",
    "\n",
    "Search Results:\n",
    "{search_results}\n",
    "\n",
    "Provide a comprehensive summary of findings (400-600 words).\n",
    "\"\"\"\n",
    "\n",
    "RETROSPECTIVE_PROMPT = \"\"\"You are conducting a sprint retrospective for a research project.\n",
    "\n",
    "Original Question: {original_question}\n",
    "\n",
    "Sprint {sprint_num} of {max_sprints} has completed.\n",
    "\n",
    "Findings so far:\n",
    "{all_findings}\n",
    "\n",
    "Remaining questions in backlog:\n",
    "{remaining_backlog}\n",
    "\n",
    "Analyze the progress and provide:\n",
    "1. What did we learn this sprint?\n",
    "2. What is still unclear or needs more investigation?\n",
    "3. Should we continue with another sprint? (consider if we have enough to answer the question)\n",
    "4. If continuing, what should be the priority for the next sprint?\n",
    "\n",
    "Be honest about gaps and uncertainties.\n",
    "\"\"\"\n",
    "\n",
    "FINAL_REPORT_PROMPT = \"\"\"You are a senior research analyst writing a final report.\n",
    "\n",
    "Original Question: {original_question}\n",
    "\n",
    "All Research Findings:\n",
    "{all_findings}\n",
    "\n",
    "Retrospective Notes:\n",
    "{retrospective_notes}\n",
    "\n",
    "Write a comprehensive research report that:\n",
    "1. Directly answers the original question\n",
    "2. Synthesizes findings across all sprints\n",
    "3. Acknowledges any remaining uncertainties\n",
    "4. Includes citations to sources\n",
    "\n",
    "The report should be well-structured with sections and approximately 1000-1500 words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web(query: str, max_results: int = 5) -> str:\n",
    "    \"\"\"Execute web search using Tavily.\"\"\"\n",
    "    try:\n",
    "        # Truncate query if too long (Tavily limit)\n",
    "        if len(query) > 400:\n",
    "            query = query[:400]\n",
    "        \n",
    "        response = tavily_client.search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            include_answer=True\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        if response.get(\"answer\"):\n",
    "            results.append(f\"Summary: {response['answer']}\")\n",
    "        \n",
    "        for r in response.get(\"results\", []):\n",
    "            results.append(f\"- {r.get('title', 'No title')}: {r.get('content', '')[:500]}... (Source: {r.get('url', 'N/A')})\")\n",
    "        \n",
    "        return \"\\n\\n\".join(results)\n",
    "    except Exception as e:\n",
    "        return f\"Search error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def plan_backlog(state: AgileResearchState) -> dict:\n",
    "    \"\"\"Plan the initial research backlog by decomposing the question.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    prompt = PLAN_BACKLOG_PROMPT.format(question=question)\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    # Parse numbered list from response\n",
    "    lines = response.content.strip().split(\"\\n\")\n",
    "    backlog = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and (line[0].isdigit() or line.startswith(\"-\")):\n",
    "            # Remove numbering and clean up\n",
    "            clean = line.lstrip(\"0123456789.-) \").strip()\n",
    "            if clean:\n",
    "                backlog.append(clean)\n",
    "    \n",
    "    print(f\"Created backlog with {len(backlog)} research questions\")\n",
    "    \n",
    "    return {\n",
    "        \"backlog\": backlog,\n",
    "        \"current_sprint\": 1,\n",
    "        \"max_sprints\": MAX_SPRINTS\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_sprint(state: AgileResearchState) -> dict:\n",
    "    \"\"\"Execute a research sprint on the current backlog item.\"\"\"\n",
    "    backlog = state.get(\"backlog\", [])\n",
    "    current_sprint = state.get(\"current_sprint\", 1)\n",
    "    max_sprints = state.get(\"max_sprints\", MAX_SPRINTS)\n",
    "    \n",
    "    if not backlog:\n",
    "        return {\"sprint_findings\": [\"No questions in backlog to research.\"]}\n",
    "    \n",
    "    # Get current question (first in backlog)\n",
    "    current_question = backlog[0]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Sprint {current_sprint}/{max_sprints}: {current_question[:80]}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Execute searches\n",
    "    search_results = []\n",
    "    queries = [current_question]  # Main query\n",
    "    \n",
    "    # Generate additional search queries for depth\n",
    "    if SEARCHES_PER_SPRINT > 1:\n",
    "        query_prompt = f\"\"\"Generate {SEARCHES_PER_SPRINT - 1} specific web search queries to investigate this question from different angles:\n",
    "        Question: {current_question}\n",
    "        \n",
    "        Return only the search queries, one per line.\"\"\"\n",
    "        query_response = await llm.ainvoke([HumanMessage(content=query_prompt)])\n",
    "        additional_queries = [q.strip() for q in query_response.content.split(\"\\n\") if q.strip()]\n",
    "        queries.extend(additional_queries[:SEARCHES_PER_SPRINT - 1])\n",
    "    \n",
    "    # Execute all searches\n",
    "    for query in queries:\n",
    "        print(f\"  Searching: {query[:60]}...\")\n",
    "        result = search_web(query)\n",
    "        search_results.append(result)\n",
    "    \n",
    "    combined_results = \"\\n\\n---\\n\\n\".join(search_results)\n",
    "    \n",
    "    # Synthesize findings\n",
    "    synthesis_prompt = SPRINT_RESEARCH_PROMPT.format(\n",
    "        sprint_num=current_sprint,\n",
    "        max_sprints=max_sprints,\n",
    "        current_question=current_question,\n",
    "        search_results=combined_results\n",
    "    )\n",
    "    \n",
    "    synthesis = await llm.ainvoke([HumanMessage(content=synthesis_prompt)])\n",
    "    \n",
    "    finding = f\"## Sprint {current_sprint} Findings: {current_question}\\n\\n{synthesis.content}\"\n",
    "    print(f\"  Synthesized {len(synthesis.content)} characters of findings\")\n",
    "    \n",
    "    # Remove processed question from backlog\n",
    "    updated_backlog = backlog[1:] if len(backlog) > 1 else []\n",
    "    \n",
    "    return {\n",
    "        \"sprint_findings\": [finding],\n",
    "        \"backlog\": updated_backlog\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrospective(state: AgileResearchState) -> dict:\n",
    "    \"\"\"Conduct retrospective analysis after sprint.\"\"\"\n",
    "    original_question = state[\"question\"]\n",
    "    current_sprint = state.get(\"current_sprint\", 1)\n",
    "    max_sprints = state.get(\"max_sprints\", MAX_SPRINTS)\n",
    "    all_findings = \"\\n\\n\".join(state.get(\"sprint_findings\", []))\n",
    "    remaining_backlog = state.get(\"backlog\", [])\n",
    "    \n",
    "    prompt = RETROSPECTIVE_PROMPT.format(\n",
    "        original_question=original_question,\n",
    "        sprint_num=current_sprint,\n",
    "        max_sprints=max_sprints,\n",
    "        all_findings=all_findings,\n",
    "        remaining_backlog=\"\\n\".join(remaining_backlog) if remaining_backlog else \"None\"\n",
    "    )\n",
    "    \n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    retro_note = f\"### Sprint {current_sprint} Retrospective\\n\\n{response.content}\"\n",
    "    print(f\"  Retrospective complete\")\n",
    "    \n",
    "    return {\n",
    "        \"retrospective_notes\": [retro_note],\n",
    "        \"current_sprint\": current_sprint + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue_sprinting(state: AgileResearchState) -> Literal[\"execute_sprint\", \"write_report\"]:\n",
    "    \"\"\"Decide whether to continue with another sprint or write the final report.\"\"\"\n",
    "    current_sprint = state.get(\"current_sprint\", 1)\n",
    "    max_sprints = state.get(\"max_sprints\", MAX_SPRINTS)\n",
    "    backlog = state.get(\"backlog\", [])\n",
    "    \n",
    "    # Stop conditions:\n",
    "    # 1. Reached max sprints\n",
    "    # 2. Backlog is empty\n",
    "    if current_sprint > max_sprints:\n",
    "        print(f\"\\nMax sprints ({max_sprints}) reached. Moving to final report.\")\n",
    "        return \"write_report\"\n",
    "    \n",
    "    if not backlog:\n",
    "        print(f\"\\nBacklog empty after sprint {current_sprint - 1}. Moving to final report.\")\n",
    "        return \"write_report\"\n",
    "    \n",
    "    print(f\"\\nContinuing to sprint {current_sprint}. {len(backlog)} questions remaining.\")\n",
    "    return \"execute_sprint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def write_report(state: AgileResearchState) -> dict:\n",
    "    \"\"\"Write the final research report.\"\"\"\n",
    "    original_question = state[\"question\"]\n",
    "    all_findings = \"\\n\\n\".join(state.get(\"sprint_findings\", []))\n",
    "    retrospective_notes = \"\\n\\n\".join(state.get(\"retrospective_notes\", []))\n",
    "    \n",
    "    prompt = FINAL_REPORT_PROMPT.format(\n",
    "        original_question=original_question,\n",
    "        all_findings=all_findings,\n",
    "        retrospective_notes=retrospective_notes\n",
    "    )\n",
    "    \n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    print(f\"\\nFinal report generated: {len(response.content)} characters\")\n",
    "    \n",
    "    return {\n",
    "        \"final_report\": response.content\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Agile Research Agent graph\n",
    "agile_builder = StateGraph(AgileResearchState)\n",
    "\n",
    "# Add nodes\n",
    "agile_builder.add_node(\"plan_backlog\", plan_backlog)\n",
    "agile_builder.add_node(\"execute_sprint\", execute_sprint)\n",
    "agile_builder.add_node(\"retrospective\", retrospective)\n",
    "agile_builder.add_node(\"write_report\", write_report)\n",
    "\n",
    "# Add edges\n",
    "agile_builder.add_edge(START, \"plan_backlog\")\n",
    "agile_builder.add_edge(\"plan_backlog\", \"execute_sprint\")\n",
    "agile_builder.add_edge(\"execute_sprint\", \"retrospective\")\n",
    "\n",
    "# Conditional edge: continue sprinting or write report\n",
    "agile_builder.add_conditional_edges(\n",
    "    \"retrospective\",\n",
    "    should_continue_sprinting,\n",
    "    {\n",
    "        \"execute_sprint\": \"execute_sprint\",\n",
    "        \"write_report\": \"write_report\"\n",
    "    }\n",
    ")\n",
    "\n",
    "agile_builder.add_edge(\"write_report\", END)\n",
    "\n",
    "# Compile\n",
    "agile_graph = agile_builder.compile()\n",
    "\n",
    "print(\"Agile Research Agent compiled successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(agile_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Agent Wrapper for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agile_sprints_agent(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Wrapper function for Agile Sprints research agent.\n",
    "    \n",
    "    Compatible with evaluation harness.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Dictionary with 'question' key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with 'output' key containing final report\n",
    "    \"\"\"\n",
    "    question = inputs.get(\"question\", \"\")\n",
    "    \n",
    "    # Run with recursion limit\n",
    "    result = asyncio.run(\n",
    "        agile_graph.ainvoke(\n",
    "            {\"question\": question},\n",
    "            config={\"recursion_limit\": 50}\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"output\": result.get(\"final_report\", \"\"),\n",
    "        \"sprint_findings\": result.get(\"sprint_findings\", []),\n",
    "        \"retrospective_notes\": result.get(\"retrospective_notes\", [])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Manual Test\n",
    "\n",
    "Run this cell to verify the agent works correctly with a simple test question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test\n",
    "test_question = \"What are the key benefits and challenges of using large language models in enterprise applications?\"\n",
    "\n",
    "print(f\"Testing Agile Sprints Agent with question:\\n{test_question}\\n\")\n",
    "print(\"Running sprint-based research (this may take several minutes)...\\n\")\n",
    "\n",
    "try:\n",
    "    result = agile_sprints_agent({\"question\": test_question})\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"FINAL REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    print(result[\"output\"][:3000] + \"...\" if len(result[\"output\"]) > 3000 else result[\"output\"])\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Report length: {len(result['output'])} characters\")\n",
    "    print(f\"Number of sprint findings: {len(result.get('sprint_findings', []))}\")\n",
    "    print(f\"Number of retrospectives: {len(result.get('retrospective_notes', []))}\")\n",
    "    print(\"Agent test PASSED ✓\")\n",
    "except Exception as e:\n",
    "    print(f\"Agent test FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Harness Integration\n",
    "\n",
    "Once the manual test passes, uncomment and run the cells below for full evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation harness and metrics\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from evaluation import (\n",
    "    ExperimentHarness, \n",
    "    fact_recall, \n",
    "    citation_precision,\n",
    "    coherence_judge, \n",
    "    depth_judge, \n",
    "    relevance_judge,\n",
    "    minimum_sources_check\n",
    ")\n",
    "\n",
    "# Initialize harness with the golden test dataset\n",
    "harness = ExperimentHarness(\n",
    "    dataset_path=\"../data/deep_research_agent_test_dataset.yaml\",\n",
    "    langsmith_dataset_name=\"deep-research-golden-v2\"\n",
    ")\n",
    "\n",
    "print(\"Evaluation harness initialized successfully!\")\n",
    "print(f\"Dataset: {harness.dataset_path}\")\n",
    "print(f\"LangSmith dataset name: {harness.langsmith_dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Full Evaluation\n",
    "\n",
    "This runs the complete evaluation on all 20 questions.\n",
    "\n",
    "**⚠️ WARNING:** This is expensive and time-consuming!\n",
    "- **Expected runtime:** 1-2 hours\n",
    "- Single run to reduce cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Evaluation on All 20 Questions\n",
    "# ⚠️ EXPENSIVE - Only uncomment when ready for full evaluation\n",
    "# Uncomment to run:\n",
    "\n",
    "# # Define comprehensive evaluator suite\n",
    "# evaluators = [\n",
    "#     fact_recall,              # Required facts coverage\n",
    "#     citation_precision,       # Citation URL validity\n",
    "#     minimum_sources_check,    # Minimum source count\n",
    "#     coherence_judge,          # Logical structure\n",
    "#     depth_judge,              # Analysis depth\n",
    "#     relevance_judge,          # Addresses question\n",
    "# ]\n",
    "# \n",
    "# # Run full evaluation\n",
    "# print(\"Starting FULL evaluation on all 20 questions...\")\n",
    "# print(\"Agile Sprints Agent - this will take 1-2 hours.\")\n",
    "# print(\"=\" * 80 + \"\\n\")\n",
    "# \n",
    "# results = harness.run_evaluation(\n",
    "#     agent_fn=agile_sprints_agent,\n",
    "#     evaluators=evaluators,\n",
    "#     experiment_name=\"agile_sprints_v1\",\n",
    "#     monte_carlo_runs=1,  # Single run to reduce cost\n",
    "#     max_concurrency=2,   # Lower concurrency for stability\n",
    "#     description=\"Agile Sprints paradigm evaluation on all difficulty tiers\"\n",
    "# )\n",
    "# \n",
    "# # Display comprehensive results\n",
    "# print(\"\\n\" + \"=\" * 80)\n",
    "# print(\"FULL EVALUATION RESULTS\")\n",
    "# print(\"=\" * 80)\n",
    "# print(f\"Experiment: {results.experiment_name}\")\n",
    "# print(f\"Questions evaluated: {results.num_questions}\")\n",
    "# print(f\"Runs per question: {results.num_runs}\")\n",
    "# \n",
    "# print(f\"\\n{'Metric':<30} {'Mean':<10}\")\n",
    "# print(\"-\" * 40)\n",
    "# for metric_name in sorted(results.metrics.keys()):\n",
    "#     if not metric_name.endswith('_std'):\n",
    "#         value = results.metrics.get(metric_name, 0)\n",
    "#         print(f\"{metric_name:<30} {value:<10.3f}\")\n",
    "# \n",
    "# # Save results to file\n",
    "# import json\n",
    "# from datetime import datetime\n",
    "# \n",
    "# results_file = Path(\"../results\") / f\"agile_sprints_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "# results_file.parent.mkdir(exist_ok=True)\n",
    "# \n",
    "# with open(results_file, 'w') as f:\n",
    "#     json.dump({\n",
    "#         \"experiment_name\": results.experiment_name,\n",
    "#         \"num_questions\": results.num_questions,\n",
    "#         \"num_runs\": results.num_runs,\n",
    "#         \"metrics\": results.metrics,\n",
    "#         \"per_question\": results.per_question_results\n",
    "#     }, f, indent=2)\n",
    "# \n",
    "# print(f\"\\nResults saved to: {results_file}\")\n",
    "\n",
    "print(\"Full evaluation cell ready. Uncomment to run when ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Viewing Results in LangSmith\n",
    "\n",
    "After running an evaluation, you can view detailed results in the LangSmith UI:\n",
    "\n",
    "1. Go to https://smith.langchain.com\n",
    "2. Navigate to your project (`deep_research_new`)\n",
    "3. Click on \"Datasets\" to see your test dataset\n",
    "4. Click on \"Experiments\" to see evaluation runs\n",
    "5. Compare Agile Sprints vs Baseline results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
