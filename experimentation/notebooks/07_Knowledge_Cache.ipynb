{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paradigm 07: Cascading Knowledge Cache Research Agent\n",
    "\n",
    "This notebook implements the **Cascading Knowledge Cache (CKC)** paradigm.\n",
    "\n",
    "## Core Concept\n",
    "\n",
    "The CKC architecture introduces an intelligent intermediary layer between search intent and search execution. Rather than treating each search as isolated, accumulated results become a growing internal knowledge base consulted before incurring the cost of external retrieval.\n",
    "\n",
    "**Fundamental Principle**: Never fetch what you already know, and use what you know to fetch smarter.\n",
    "\n",
    "## Key Principles\n",
    "\n",
    "1. **Fail Fast, Fail Cheap**: Cheapest operations execute first\n",
    "2. **Graduated Confidence**: HIGH/MEDIUM/LOW classifications\n",
    "3. **Specificity Awareness**: Query precision modulates thresholds\n",
    "4. **Temporal Intelligence**: Detect time-sensitive queries\n",
    "\n",
    "## Architecture\n",
    "\n",
    "- **Layer 1**: Deterministic Deduplication (exact matching)\n",
    "- **Layer 2**: Semantic Similarity Retrieval (vector search + confidence)\n",
    "- **Layer 3**: LLM-Augmented Judgment (gap analysis, query refinement)\n",
    "\n",
    "## Technology Stack\n",
    "\n",
    "- **LLM**: gpt-5-mini-2025-08-07\n",
    "- **Web Search**: Tavily API\n",
    "- **Embeddings**: OpenAI text-embedding-3-small\n",
    "- **Tracing**: LangSmith\n",
    "- **Framework**: LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configured successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import operator\n",
    "import asyncio\n",
    "import hashlib\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Annotated, TypedDict, Literal, Optional, Any\n",
    "from urllib.parse import urlparse\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from tavily import TavilyClient\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables\n",
    "env_path = Path(\"../.env\")\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Configure LangSmith tracing\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"deep_research_new\"\n",
    "\n",
    "print(\"Environment configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-5-mini-2025-08-07\n",
      "Embedding model: text-embedding-3-small\n",
      "Confidence thresholds: HIGH >= 0.75, LOW < 0.4\n",
      "Verification sprints: max 6, 6 claims/sprint\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM, Tavily, and Embeddings\n",
    "MODEL_NAME = \"gpt-5-mini-2025-08-07\"\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0, max_retries=10)\n",
    "tavily_client = TavilyClient()\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Cache Configuration\n",
    "CHUNK_SIZE = 500  # characters\n",
    "CHUNK_OVERLAP = 100  # characters\n",
    "HIGH_CONFIDENCE_THRESHOLD = 0.75\n",
    "LOW_CONFIDENCE_THRESHOLD = 0.40\n",
    "SPECIFICITY_ADJUSTMENT_FACTOR = 0.2  # How much specificity raises thresholds\n",
    "TOP_K_RETRIEVAL = 5\n",
    "\n",
    "# Verification Sprint Configuration (similar to Agile Sprints)\n",
    "MAX_VERIFICATION_SPRINTS = 6  # Maximum verification iterations\n",
    "CLAIMS_PER_SPRINT = 6  # Claims to verify per sprint\n",
    "MIN_CACHE_HIT_RATE_TO_STOP = 0.6  # Stop if cache hit rate exceeds this\n",
    "\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "print(f\"Embedding model: text-embedding-3-small\")\n",
    "print(f\"Confidence thresholds: HIGH >= {HIGH_CONFIDENCE_THRESHOLD}, LOW < {LOW_CONFIDENCE_THRESHOLD}\")\n",
    "print(f\"Verification sprints: max {MAX_VERIFICATION_SPRINTS}, {CLAIMS_PER_SPRINT} claims/sprint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedDocument(BaseModel):\n",
    "    \"\"\"A cached web document.\"\"\"\n",
    "    url: str = Field(description=\"Original URL\")\n",
    "    normalized_url: str = Field(description=\"Normalized URL for lookup\")\n",
    "    content: str = Field(description=\"Full text content\")\n",
    "    content_hash: str = Field(description=\"SHA-256 hash of content\")\n",
    "    title: str = Field(default=\"\", description=\"Page title\")\n",
    "    retrieval_timestamp: str = Field(description=\"When this was retrieved\")\n",
    "    source_query: str = Field(default=\"\", description=\"Query that led to this content\")\n",
    "\n",
    "\n",
    "class CachedChunk(BaseModel):\n",
    "    \"\"\"A chunk of content with embedding.\"\"\"\n",
    "    chunk_id: str = Field(description=\"Unique identifier\")\n",
    "    text: str = Field(description=\"Chunk text content\")\n",
    "    embedding: List[float] = Field(description=\"Vector embedding\")\n",
    "    source_url: str = Field(description=\"Source document URL\")\n",
    "    position: int = Field(description=\"Position within source document\")\n",
    "\n",
    "\n",
    "class QueryCacheEntry(BaseModel):\n",
    "    \"\"\"A cached query and its results.\"\"\"\n",
    "    original_query: str\n",
    "    light_normalized: str\n",
    "    aggressive_normalized: str\n",
    "    timestamp: str\n",
    "    result_urls: List[str]\n",
    "    result_summary: str\n",
    "\n",
    "\n",
    "class CacheDecision(BaseModel):\n",
    "    \"\"\"Record of a cache decision for observability.\"\"\"\n",
    "    query: str\n",
    "    layer_reached: Literal[\"L1\", \"L2\", \"L3\"]\n",
    "    decision: Literal[\"HIT\", \"HIGH_CONF\", \"MEDIUM_CONF\", \"LOW_CONF\",\n",
    "                      \"SUFFICIENT\", \"PARTIAL\", \"INSUFFICIENT\"]\n",
    "    confidence_score: float = 0.0\n",
    "    action_taken: Literal[\"USE_CACHE\", \"SEARCH\", \"TARGETED_SEARCH\"]\n",
    "    reasoning: str = \"\"\n",
    "    timestamp: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryAnalysis(BaseModel):\n",
    "    \"\"\"Analysis of a query's characteristics.\"\"\"\n",
    "    original_query: str\n",
    "    specificity_score: float = Field(description=\"0.0 (general) to 1.0 (very specific)\")\n",
    "    temporal_intent_score: float = Field(description=\"0.0 (no temporal) to 1.0 (time-sensitive)\")\n",
    "    adjusted_high_threshold: float\n",
    "    adjusted_low_threshold: float\n",
    "    extracted_entities: List[str] = Field(default_factory=list)\n",
    "    extracted_dates: List[str] = Field(default_factory=list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Knowledge Base Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base initialized\n"
     ]
    }
   ],
   "source": [
    "class KnowledgeBase:\n",
    "    \"\"\"Session-scoped knowledge base with cascading cache capabilities.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.url_registry: Dict[str, CachedDocument] = {}\n",
    "        self.query_cache: Dict[str, QueryCacheEntry] = {}\n",
    "        self.chunks: List[CachedChunk] = []\n",
    "        self.chunk_embeddings: Optional[np.ndarray] = None  # For fast similarity\n",
    "\n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            \"total_queries\": 0,\n",
    "            \"l1_hits\": 0,\n",
    "            \"l2_high\": 0,\n",
    "            \"l2_medium\": 0,\n",
    "            \"l2_low\": 0,\n",
    "            \"l3_sufficient\": 0,\n",
    "            \"l3_partial\": 0,\n",
    "            \"l3_insufficient\": 0,\n",
    "            \"web_searches_executed\": 0,\n",
    "            \"web_searches_avoided\": 0\n",
    "        }\n",
    "\n",
    "    # === URL Normalization ===\n",
    "    def normalize_url(self, url: str) -> str:\n",
    "        \"\"\"Normalize URL for consistent lookup.\"\"\"\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            # Lowercase host, remove www prefix\n",
    "            host = parsed.netloc.lower()\n",
    "            if host.startswith(\"www.\"):\n",
    "                host = host[4:]\n",
    "            # Remove trailing slash from path\n",
    "            path = parsed.path.rstrip(\"/\")\n",
    "            # Sort query parameters\n",
    "            query_params = sorted(parsed.query.split(\"&\")) if parsed.query else []\n",
    "            # Remove tracking params\n",
    "            tracking_params = {\"utm_source\", \"utm_medium\", \"utm_campaign\", \"ref\", \"fbclid\"}\n",
    "            query_params = [p for p in query_params if p.split(\"=\")[0] not in tracking_params]\n",
    "            query = \"&\".join(query_params)\n",
    "            # Reconstruct\n",
    "            normalized = f\"https://{host}{path}\"\n",
    "            if query:\n",
    "                normalized += f\"?{query}\"\n",
    "            return normalized\n",
    "        except:\n",
    "            return url.lower()\n",
    "\n",
    "    # === Query Normalization ===\n",
    "    def normalize_query_light(self, query: str) -> str:\n",
    "        \"\"\"Light normalization: lowercase, collapse whitespace.\"\"\"\n",
    "        return \" \".join(query.lower().split())\n",
    "\n",
    "    def normalize_query_aggressive(self, query: str) -> str:\n",
    "        \"\"\"Aggressive normalization: remove stop words, sort terms.\"\"\"\n",
    "        stop_words = {\"the\", \"a\", \"an\", \"is\", \"are\", \"of\", \"in\", \"to\", \"for\", \"and\", \"or\", \"what\", \"how\", \"why\", \"when\", \"where\"}\n",
    "        light = self.normalize_query_light(query)\n",
    "        terms = [t for t in light.split() if t not in stop_words and len(t) > 1]\n",
    "        return \" \".join(sorted(terms))\n",
    "\n",
    "    # === Content Hashing ===\n",
    "    def compute_content_hash(self, content: str) -> str:\n",
    "        \"\"\"Compute SHA-256 hash of content.\"\"\"\n",
    "        return hashlib.sha256(content.encode()).hexdigest()\n",
    "\n",
    "    # === Document Storage ===\n",
    "    def add_document(self, url: str, content: str, title: str = \"\", source_query: str = \"\"):\n",
    "        \"\"\"Add a document to the knowledge base.\"\"\"\n",
    "        normalized_url = self.normalize_url(url)\n",
    "\n",
    "        doc = CachedDocument(\n",
    "            url=url,\n",
    "            normalized_url=normalized_url,\n",
    "            content=content,\n",
    "            content_hash=self.compute_content_hash(content),\n",
    "            title=title,\n",
    "            retrieval_timestamp=datetime.now().isoformat(),\n",
    "            source_query=source_query\n",
    "        )\n",
    "\n",
    "        self.url_registry[normalized_url] = doc\n",
    "\n",
    "        # Chunk and embed the content\n",
    "        self._chunk_and_embed(doc)\n",
    "\n",
    "        return doc\n",
    "\n",
    "    def _chunk_and_embed(self, doc: CachedDocument):\n",
    "        \"\"\"Chunk document and compute embeddings.\"\"\"\n",
    "        content = doc.content\n",
    "        chunks_text = []\n",
    "\n",
    "        # Simple chunking with overlap\n",
    "        for i in range(0, len(content), CHUNK_SIZE - CHUNK_OVERLAP):\n",
    "            chunk_text = content[i:i + CHUNK_SIZE]\n",
    "            if len(chunk_text) > 50:  # Minimum chunk size\n",
    "                chunks_text.append(chunk_text)\n",
    "\n",
    "        if not chunks_text:\n",
    "            return\n",
    "\n",
    "        # Compute embeddings (batch)\n",
    "        embeddings = embeddings_model.embed_documents(chunks_text)\n",
    "\n",
    "        # Create chunk objects\n",
    "        for i, (text, embedding) in enumerate(zip(chunks_text, embeddings)):\n",
    "            chunk = CachedChunk(\n",
    "                chunk_id=f\"{doc.content_hash[:8]}_{i}\",\n",
    "                text=text,\n",
    "                embedding=embedding,\n",
    "                source_url=doc.url,\n",
    "                position=i\n",
    "            )\n",
    "            self.chunks.append(chunk)\n",
    "\n",
    "        # Update embedding matrix for fast similarity\n",
    "        self._update_embedding_matrix()\n",
    "\n",
    "    def _update_embedding_matrix(self):\n",
    "        \"\"\"Update the numpy matrix of embeddings for fast search.\"\"\"\n",
    "        if self.chunks:\n",
    "            self.chunk_embeddings = np.array([c.embedding for c in self.chunks])\n",
    "\n",
    "    # === Query Cache ===\n",
    "    def add_query(self, query: str, result_urls: List[str], result_summary: str):\n",
    "        \"\"\"Add a query to the cache.\"\"\"\n",
    "        entry = QueryCacheEntry(\n",
    "            original_query=query,\n",
    "            light_normalized=self.normalize_query_light(query),\n",
    "            aggressive_normalized=self.normalize_query_aggressive(query),\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            result_urls=result_urls,\n",
    "            result_summary=result_summary\n",
    "        )\n",
    "\n",
    "        # Store under both normalizations\n",
    "        self.query_cache[entry.light_normalized] = entry\n",
    "        self.query_cache[entry.aggressive_normalized] = entry\n",
    "\n",
    "        return entry\n",
    "\n",
    "    # === Lookups ===\n",
    "    def lookup_url(self, url: str) -> Optional[CachedDocument]:\n",
    "        \"\"\"Check if URL is already cached.\"\"\"\n",
    "        normalized = self.normalize_url(url)\n",
    "        return self.url_registry.get(normalized)\n",
    "\n",
    "    def lookup_query_exact(self, query: str) -> Optional[QueryCacheEntry]:\n",
    "        \"\"\"Check for exact query match (light normalization).\"\"\"\n",
    "        light = self.normalize_query_light(query)\n",
    "        return self.query_cache.get(light)\n",
    "\n",
    "    def lookup_query_aggressive(self, query: str) -> Optional[QueryCacheEntry]:\n",
    "        \"\"\"Check for bag-of-words query match (aggressive normalization).\"\"\"\n",
    "        aggressive = self.normalize_query_aggressive(query)\n",
    "        return self.query_cache.get(aggressive)\n",
    "\n",
    "    # === Semantic Search ===\n",
    "    def semantic_search(self, query: str, top_k: int = TOP_K_RETRIEVAL) -> List[Tuple[CachedChunk, float]]:\n",
    "        \"\"\"Find semantically similar chunks.\"\"\"\n",
    "        if not self.chunks or self.chunk_embeddings is None:\n",
    "            return []\n",
    "\n",
    "        # Embed query\n",
    "        query_embedding = np.array(embeddings_model.embed_query(query))\n",
    "\n",
    "        # Cosine similarity\n",
    "        similarities = np.dot(self.chunk_embeddings, query_embedding) / (\n",
    "            np.linalg.norm(self.chunk_embeddings, axis=1) * np.linalg.norm(query_embedding) + 1e-8\n",
    "        )\n",
    "\n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append((self.chunks[idx], float(similarities[idx])))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_stats_summary(self) -> str:\n",
    "        \"\"\"Get human-readable stats summary.\"\"\"\n",
    "        total = self.stats[\"total_queries\"]\n",
    "        if total == 0:\n",
    "            return \"No queries processed yet.\"\n",
    "\n",
    "        avoided = self.stats[\"web_searches_avoided\"]\n",
    "        executed = self.stats[\"web_searches_executed\"]\n",
    "        hit_rate = avoided / total * 100 if total > 0 else 0\n",
    "\n",
    "        return f\"\"\"\n",
    "Cache Statistics:\n",
    "- Total queries: {total}\n",
    "- Web searches avoided: {avoided} ({hit_rate:.1f}% hit rate)\n",
    "- Web searches executed: {executed}\n",
    "- Layer 1 hits: {self.stats['l1_hits']}\n",
    "- Layer 2 HIGH confidence: {self.stats['l2_high']}\n",
    "- Layer 2 MEDIUM confidence: {self.stats['l2_medium']}\n",
    "- Layer 2 LOW confidence: {self.stats['l2_low']}\n",
    "- Layer 3 SUFFICIENT: {self.stats['l3_sufficient']}\n",
    "- Layer 3 PARTIAL: {self.stats['l3_partial']}\n",
    "- Layer 3 INSUFFICIENT: {self.stats['l3_insufficient']}\n",
    "- Documents cached: {len(self.url_registry)}\n",
    "- Chunks indexed: {len(self.chunks)}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Initialize global knowledge base (session-scoped)\n",
    "knowledge_base = KnowledgeBase()\n",
    "print(\"Knowledge base initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_specificity(query: str) -> float:\n",
    "    \"\"\"Compute query specificity score (0.0 to 1.0).\"\"\"\n",
    "    score = 0.0\n",
    "\n",
    "    # Numerical precision: dates, percentages, specific numbers\n",
    "    if re.search(r'\\b\\d{4}\\b', query):  # Years\n",
    "        score += 0.2\n",
    "    if re.search(r'\\d+%|\\$\\d+|\\d+\\s*(billion|million|thousand)', query, re.IGNORECASE):\n",
    "        score += 0.2\n",
    "    if re.search(r'Q[1-4]\\s*\\d{4}|FY\\d{4}', query, re.IGNORECASE):  # Quarters, fiscal years\n",
    "        score += 0.2\n",
    "\n",
    "    # Proper nouns (capitalized words not at sentence start)\n",
    "    proper_nouns = re.findall(r'(?<!^)(?<!\\. )[A-Z][a-z]+', query)\n",
    "    if len(proper_nouns) > 1:\n",
    "        score += 0.15\n",
    "\n",
    "    # Quoted phrases\n",
    "    if '\"' in query or \"'\" in query:\n",
    "        score += 0.15\n",
    "\n",
    "    # Specific question words\n",
    "    if re.search(r'\\b(exact|precise|specific|exactly|how many|what is the)\\b', query, re.IGNORECASE):\n",
    "        score += 0.1\n",
    "\n",
    "    return min(score, 1.0)\n",
    "\n",
    "\n",
    "def compute_temporal_intent(query: str) -> float:\n",
    "    \"\"\"Compute temporal intent score (0.0 to 1.0).\"\"\"\n",
    "    score = 0.0\n",
    "\n",
    "    # Explicit currency terms\n",
    "    if re.search(r'\\b(current|latest|now|today|recent|present|this week|this month|this year)\\b', query, re.IGNORECASE):\n",
    "        score += 0.4\n",
    "\n",
    "    # Role/status queries\n",
    "    if re.search(r'\\b(who is the|what is the current|is .+ still)\\b', query, re.IGNORECASE):\n",
    "        score += 0.3\n",
    "\n",
    "    # Comparative present\n",
    "    if re.search(r'\\b(how has .+ changed|compared to|versus last)\\b', query, re.IGNORECASE):\n",
    "        score += 0.2\n",
    "\n",
    "    # Event-driven topics\n",
    "    if re.search(r'\\b(stock price|election|weather|score|breaking)\\b', query, re.IGNORECASE):\n",
    "        score += 0.3\n",
    "\n",
    "    return min(score, 1.0)\n",
    "\n",
    "\n",
    "def analyze_query(query: str) -> QueryAnalysis:\n",
    "    \"\"\"Perform full query analysis.\"\"\"\n",
    "    specificity = compute_specificity(query)\n",
    "    temporal_intent = compute_temporal_intent(query)\n",
    "\n",
    "    # Adjust thresholds based on specificity\n",
    "    # Higher specificity = higher threshold required\n",
    "    high_adjustment = specificity * SPECIFICITY_ADJUSTMENT_FACTOR\n",
    "    adjusted_high = min(HIGH_CONFIDENCE_THRESHOLD + high_adjustment, 0.95)\n",
    "    adjusted_low = LOW_CONFIDENCE_THRESHOLD  # Low threshold stays the same\n",
    "\n",
    "    # Extract entities and dates\n",
    "    entities = re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b', query)\n",
    "    dates = re.findall(r'\\b\\d{4}\\b|\\bQ[1-4]\\s*\\d{4}\\b|\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{4}\\b', query, re.IGNORECASE)\n",
    "\n",
    "    return QueryAnalysis(\n",
    "        original_query=query,\n",
    "        specificity_score=specificity,\n",
    "        temporal_intent_score=temporal_intent,\n",
    "        adjusted_high_threshold=adjusted_high,\n",
    "        adjusted_low_threshold=adjusted_low,\n",
    "        extracted_entities=entities[:10],\n",
    "        extracted_dates=dates[:5]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Layer 1: Deterministic Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_layer1(query: str, kb: KnowledgeBase) -> Tuple[Literal[\"HIT\", \"MISS\"], Optional[str], Optional[List[str]]]:\n",
    "    \"\"\"\n",
    "    Layer 1: Deterministic deduplication.\n",
    "\n",
    "    Returns: (decision, cached_summary, cached_urls)\n",
    "    \"\"\"\n",
    "    # Check exact query match (light normalization)\n",
    "    exact_match = kb.lookup_query_exact(query)\n",
    "    if exact_match:\n",
    "        return \"HIT\", exact_match.result_summary, exact_match.result_urls\n",
    "\n",
    "    # Check bag-of-words match (aggressive normalization)\n",
    "    aggressive_match = kb.lookup_query_aggressive(query)\n",
    "    if aggressive_match:\n",
    "        return \"HIT\", aggressive_match.result_summary, aggressive_match.result_urls\n",
    "\n",
    "    return \"MISS\", None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Layer 2: Semantic Retrieval and Confidence Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confidence(\n",
    "    top_results: List[Tuple[CachedChunk, float]],\n",
    "    query: str,\n",
    "    analysis: QueryAnalysis\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Compute multi-signal confidence score.\n",
    "\n",
    "    Returns: (confidence_score, signal_breakdown)\n",
    "    \"\"\"\n",
    "    if not top_results:\n",
    "        return 0.0, {}\n",
    "\n",
    "    # Signal 1: Top score magnitude\n",
    "    top_score = top_results[0][1]\n",
    "\n",
    "    # Signal 2: Score gap (discrimination)\n",
    "    if len(top_results) > 1:\n",
    "        score_gap = top_results[0][1] - top_results[1][1]\n",
    "    else:\n",
    "        score_gap = top_score\n",
    "\n",
    "    # Signal 3: Term overlap (Jaccard similarity)\n",
    "    query_terms = set(query.lower().split())\n",
    "    top_chunk_terms = set(top_results[0][0].text.lower().split())\n",
    "    term_overlap = len(query_terms & top_chunk_terms) / len(query_terms | top_chunk_terms) if query_terms | top_chunk_terms else 0\n",
    "\n",
    "    # Weighted combination\n",
    "    weights = {\n",
    "        \"top_score\": 0.5,\n",
    "        \"score_gap\": 0.25,\n",
    "        \"term_overlap\": 0.25\n",
    "    }\n",
    "\n",
    "    raw_confidence = (\n",
    "        weights[\"top_score\"] * top_score +\n",
    "        weights[\"score_gap\"] * min(score_gap * 2, 1.0) +  # Scale gap\n",
    "        weights[\"term_overlap\"] * term_overlap\n",
    "    )\n",
    "\n",
    "    # Apply temporal penalty if needed\n",
    "    temporal_penalty = 0.0  # Within-session, all content is recent\n",
    "\n",
    "    confidence = max(0.0, min(raw_confidence - temporal_penalty, 1.0))\n",
    "\n",
    "    signals = {\n",
    "        \"top_score\": top_score,\n",
    "        \"score_gap\": score_gap,\n",
    "        \"term_overlap\": term_overlap,\n",
    "        \"temporal_penalty\": temporal_penalty,\n",
    "        \"raw_confidence\": raw_confidence,\n",
    "        \"final_confidence\": confidence\n",
    "    }\n",
    "\n",
    "    return confidence, signals\n",
    "\n",
    "\n",
    "def check_layer2(\n",
    "    query: str,\n",
    "    kb: KnowledgeBase,\n",
    "    analysis: QueryAnalysis\n",
    ") -> Tuple[Literal[\"HIGH\", \"MEDIUM\", \"LOW\"], float, List[Tuple[CachedChunk, float]], Dict]:\n",
    "    \"\"\"\n",
    "    Layer 2: Semantic retrieval with confidence scoring.\n",
    "\n",
    "    Returns: (decision, confidence, retrieved_chunks, signals)\n",
    "    \"\"\"\n",
    "    # Perform semantic search\n",
    "    results = kb.semantic_search(query, top_k=TOP_K_RETRIEVAL)\n",
    "\n",
    "    if not results:\n",
    "        return \"LOW\", 0.0, [], {}\n",
    "\n",
    "    # Compute confidence\n",
    "    confidence, signals = compute_confidence(results, query, analysis)\n",
    "\n",
    "    # Classify based on adjusted thresholds\n",
    "    if confidence >= analysis.adjusted_high_threshold:\n",
    "        decision = \"HIGH\"\n",
    "    elif confidence >= analysis.adjusted_low_threshold:\n",
    "        decision = \"MEDIUM\"\n",
    "    else:\n",
    "        decision = \"LOW\"\n",
    "\n",
    "    return decision, confidence, results, signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Layer 3: LLM-Augmented Judgment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER3_JUDGMENT_PROMPT = \"\"\"You are evaluating whether cached content answers a research query.\n",
    "\n",
    "QUERY: {query}\n",
    "\n",
    "CACHED CONTENT (top {num_chunks} relevant chunks):\n",
    "{chunks_text}\n",
    "\n",
    "Analyze on these dimensions:\n",
    "1. TOPICAL RELEVANCE: Is the cached content about the same subject?\n",
    "2. SPECIFICITY MATCH: Does it address the specific aspect asked about?\n",
    "3. COMPLETENESS: Does it provide a complete or only partial answer?\n",
    "4. FACTUAL DENSITY: Does it contain concrete facts that answer the query?\n",
    "\n",
    "Provide your assessment in this exact format:\n",
    "VERDICT: [SUFFICIENT|PARTIAL|INSUFFICIENT]\n",
    "RELEVANCE: [0.0-1.0]\n",
    "COMPLETENESS: [0.0-1.0]\n",
    "REASONING: [Your explanation in 2-3 sentences]\n",
    "GAPS: [List specific missing information, or \"None\" if sufficient]\n",
    "\"\"\"\n",
    "\n",
    "GAP_ANALYSIS_PROMPT = \"\"\"Based on your analysis, the cached content only partially answers the query.\n",
    "\n",
    "QUERY: {query}\n",
    "IDENTIFIED GAPS: {gaps}\n",
    "\n",
    "Generate 1-2 highly targeted search queries that would fill these specific gaps.\n",
    "The refined queries should:\n",
    "- Target ONLY the missing information (not repeat what we already have)\n",
    "- Be specific and searchable\n",
    "- Different from the original query\n",
    "\n",
    "Return queries one per line, no numbering or bullets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer3Output(BaseModel):\n",
    "    \"\"\"Structured output from Layer 3 judgment.\"\"\"\n",
    "    verdict: Literal[\"SUFFICIENT\", \"PARTIAL\", \"INSUFFICIENT\"]\n",
    "    relevance: float\n",
    "    completeness: float\n",
    "    reasoning: str\n",
    "    gaps: List[str]\n",
    "\n",
    "\n",
    "async def check_layer3(\n",
    "    query: str,\n",
    "    chunks: List[Tuple[CachedChunk, float]],\n",
    "    kb: KnowledgeBase\n",
    ") -> Tuple[Literal[\"SUFFICIENT\", \"PARTIAL\", \"INSUFFICIENT\"], List[str], Optional[List[str]]]:\n",
    "    \"\"\"\n",
    "    Layer 3: LLM-augmented judgment with gap analysis.\n",
    "\n",
    "    Returns: (verdict, gaps, refined_queries)\n",
    "    \"\"\"\n",
    "    # Format chunks for LLM\n",
    "    chunks_text = \"\\n\\n---\\n\\n\".join([\n",
    "        f\"[Chunk {i+1}, similarity: {score:.3f}]\\n{chunk.text}\"\n",
    "        for i, (chunk, score) in enumerate(chunks[:5])\n",
    "    ])\n",
    "\n",
    "    prompt = LAYER3_JUDGMENT_PROMPT.format(\n",
    "        query=query,\n",
    "        num_chunks=min(len(chunks), 5),\n",
    "        chunks_text=chunks_text\n",
    "    )\n",
    "\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # Parse response\n",
    "    content = response.content\n",
    "\n",
    "    # Extract verdict\n",
    "    verdict_match = re.search(r'VERDICT:\\s*(SUFFICIENT|PARTIAL|INSUFFICIENT)', content, re.IGNORECASE)\n",
    "    verdict = verdict_match.group(1).upper() if verdict_match else \"INSUFFICIENT\"\n",
    "\n",
    "    # Extract gaps\n",
    "    gaps = []\n",
    "    gaps_match = re.search(r'GAPS:\\s*(.+?)(?=\\n\\n|$)', content, re.DOTALL | re.IGNORECASE)\n",
    "    if gaps_match:\n",
    "        gaps_text = gaps_match.group(1).strip()\n",
    "        if gaps_text.lower() != \"none\":\n",
    "            gaps = [g.strip() for g in re.split(r'[-\\u2022\\n]', gaps_text) if g.strip()]\n",
    "\n",
    "    # Generate refined queries if PARTIAL\n",
    "    refined_queries = None\n",
    "    if verdict == \"PARTIAL\" and gaps:\n",
    "        refine_prompt = GAP_ANALYSIS_PROMPT.format(\n",
    "            query=query,\n",
    "            gaps=\"\\n\".join(f\"- {g}\" for g in gaps)\n",
    "        )\n",
    "        refine_response = await llm.ainvoke([HumanMessage(content=refine_prompt)])\n",
    "        refined_queries = [q.strip() for q in refine_response.content.split(\"\\n\") if q.strip()][:2]\n",
    "\n",
    "    return verdict, gaps, refined_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cascaded Search Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web(query: str, max_results: int = 8) -> Tuple[str, List[str], List[str]]:\n",
    "    \"\"\"Execute web search using Tavily. Returns (summary, results, urls).\"\"\"\n",
    "    try:\n",
    "        if len(query) > 400:\n",
    "            query = query[:400]\n",
    "\n",
    "        response = tavily_client.search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            include_answer=True\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        urls = []\n",
    "        summary = response.get(\"answer\", \"\")\n",
    "\n",
    "        for r in response.get(\"results\", []):\n",
    "            url = r.get('url', '')\n",
    "            urls.append(url)\n",
    "            content = r.get('content', '')[:500]\n",
    "            title = r.get('title', 'No title')\n",
    "            results.append(f\"[{title}] {content}... (Source: {url})\")\n",
    "\n",
    "        return summary, results, urls\n",
    "    except Exception as e:\n",
    "        return f\"Search error: {str(e)}\", [], []\n",
    "\n",
    "\n",
    "async def cascaded_search(\n",
    "    query: str,\n",
    "    kb: KnowledgeBase\n",
    ") -> Tuple[str, List[str], CacheDecision]:\n",
    "    \"\"\"\n",
    "    Execute the full cascading cache check and search if needed.\n",
    "\n",
    "    Returns: (content, urls, decision_record)\n",
    "    \"\"\"\n",
    "    kb.stats[\"total_queries\"] += 1\n",
    "    timestamp = datetime.now().isoformat()\n",
    "\n",
    "    # === Query Analysis ===\n",
    "    analysis = analyze_query(query)\n",
    "\n",
    "    # === Layer 1: Deterministic Deduplication ===\n",
    "    l1_decision, l1_summary, l1_urls = check_layer1(query, kb)\n",
    "\n",
    "    if l1_decision == \"HIT\":\n",
    "        kb.stats[\"l1_hits\"] += 1\n",
    "        kb.stats[\"web_searches_avoided\"] += 1\n",
    "        decision = CacheDecision(\n",
    "            query=query,\n",
    "            layer_reached=\"L1\",\n",
    "            decision=\"HIT\",\n",
    "            confidence_score=1.0,\n",
    "            action_taken=\"USE_CACHE\",\n",
    "            reasoning=\"Exact query match found in cache\",\n",
    "            timestamp=timestamp\n",
    "        )\n",
    "        return l1_summary, l1_urls, decision\n",
    "\n",
    "    # === Layer 2: Semantic Retrieval ===\n",
    "    l2_decision, confidence, chunks, signals = check_layer2(query, kb, analysis)\n",
    "\n",
    "    if l2_decision == \"HIGH\":\n",
    "        kb.stats[\"l2_high\"] += 1\n",
    "        kb.stats[\"web_searches_avoided\"] += 1\n",
    "\n",
    "        # Compile content from chunks\n",
    "        content = \"\\n\\n\".join([f\"[From: {c.source_url}]\\n{c.text}\" for c, _ in chunks[:3]])\n",
    "        urls = list(set([c.source_url for c, _ in chunks]))\n",
    "\n",
    "        decision = CacheDecision(\n",
    "            query=query,\n",
    "            layer_reached=\"L2\",\n",
    "            decision=\"HIGH_CONF\",\n",
    "            confidence_score=confidence,\n",
    "            action_taken=\"USE_CACHE\",\n",
    "            reasoning=f\"High semantic similarity (conf={confidence:.3f})\",\n",
    "            timestamp=timestamp\n",
    "        )\n",
    "        return content, urls, decision\n",
    "\n",
    "    elif l2_decision == \"LOW\":\n",
    "        kb.stats[\"l2_low\"] += 1\n",
    "        # Skip Layer 3, go directly to search\n",
    "        summary, results, urls = search_web(query)\n",
    "\n",
    "        kb.stats[\"web_searches_executed\"] += 1\n",
    "\n",
    "        # Cache the results - use synthetic URL based on query hash\n",
    "        # Note: Tavily returns summaries, not full page content per URL\n",
    "        query_content = f\"Query: {query}\\n\\nAnswer: {summary}\\n\\nResults:\\n\" + \"\\n\\n\".join(results)\n",
    "        synthetic_url = f\"search://{kb.compute_content_hash(query)[:16]}\"\n",
    "        kb.add_document(synthetic_url, query_content, title=f\"Search: {query[:50]}\", source_query=query)\n",
    "        kb.add_query(query, urls, summary)\n",
    "\n",
    "        decision = CacheDecision(\n",
    "            query=query,\n",
    "            layer_reached=\"L2\",\n",
    "            decision=\"LOW_CONF\",\n",
    "            confidence_score=confidence,\n",
    "            action_taken=\"SEARCH\",\n",
    "            reasoning=f\"Low confidence ({confidence:.3f}), executed web search\",\n",
    "            timestamp=timestamp\n",
    "        )\n",
    "        return query_content, urls, decision\n",
    "\n",
    "    # === Layer 3: LLM Judgment (MEDIUM confidence) ===\n",
    "    kb.stats[\"l2_medium\"] += 1\n",
    "\n",
    "    verdict, gaps, refined_queries = await check_layer3(query, chunks, kb)\n",
    "\n",
    "    if verdict == \"SUFFICIENT\":\n",
    "        kb.stats[\"l3_sufficient\"] += 1\n",
    "        kb.stats[\"web_searches_avoided\"] += 1\n",
    "\n",
    "        content = \"\\n\\n\".join([f\"[From: {c.source_url}]\\n{c.text}\" for c, _ in chunks[:3]])\n",
    "        urls = list(set([c.source_url for c, _ in chunks]))\n",
    "\n",
    "        decision = CacheDecision(\n",
    "            query=query,\n",
    "            layer_reached=\"L3\",\n",
    "            decision=\"SUFFICIENT\",\n",
    "            confidence_score=confidence,\n",
    "            action_taken=\"USE_CACHE\",\n",
    "            reasoning=\"LLM judged cached content sufficient\",\n",
    "            timestamp=timestamp\n",
    "        )\n",
    "        return content, urls, decision\n",
    "\n",
    "    elif verdict == \"PARTIAL\" and refined_queries:\n",
    "        kb.stats[\"l3_partial\"] += 1\n",
    "\n",
    "        # Use cached content + targeted search for gaps\n",
    "        cached_content = \"\\n\\n\".join([f\"[Cached: {c.source_url}]\\n{c.text}\" for c, _ in chunks[:2]])\n",
    "        cached_urls = [c.source_url for c, _ in chunks[:2]]\n",
    "\n",
    "        # Execute refined searches\n",
    "        for refined_query in refined_queries:\n",
    "            summary, results, new_urls = search_web(refined_query, max_results=4)\n",
    "            kb.stats[\"web_searches_executed\"] += 1\n",
    "\n",
    "            # Cache new results - use synthetic URL based on query hash\n",
    "            gap_content = f\"Query: {refined_query}\\n\\nAnswer: {summary}\\n\\nResults:\\n\" + \"\\n\\n\".join(results)\n",
    "            synthetic_url = f\"search://{kb.compute_content_hash(refined_query)[:16]}\"\n",
    "            kb.add_document(synthetic_url, gap_content, title=f\"Search: {refined_query[:50]}\", source_query=refined_query)\n",
    "            kb.add_query(refined_query, new_urls, summary)\n",
    "\n",
    "            cached_content += f\"\\n\\n[Gap-fill search: {refined_query}]\\n{gap_content}\"\n",
    "            cached_urls.extend(new_urls)\n",
    "\n",
    "        decision = CacheDecision(\n",
    "            query=query,\n",
    "            layer_reached=\"L3\",\n",
    "            decision=\"PARTIAL\",\n",
    "            confidence_score=confidence,\n",
    "            action_taken=\"TARGETED_SEARCH\",\n",
    "            reasoning=f\"LLM identified gaps: {gaps[:2]}. Executed {len(refined_queries)} targeted searches.\",\n",
    "            timestamp=timestamp\n",
    "        )\n",
    "        return cached_content, list(set(cached_urls)), decision\n",
    "\n",
    "    else:\n",
    "        kb.stats[\"l3_insufficient\"] += 1\n",
    "\n",
    "        # Full search needed\n",
    "        summary, results, urls = search_web(query)\n",
    "        kb.stats[\"web_searches_executed\"] += 1\n",
    "\n",
    "        # Cache the results - use synthetic URL based on query hash\n",
    "        query_content = f\"Query: {query}\\n\\nAnswer: {summary}\\n\\nResults:\\n\" + \"\\n\\n\".join(results)\n",
    "        synthetic_url = f\"search://{kb.compute_content_hash(query)[:16]}\"\n",
    "        kb.add_document(synthetic_url, query_content, title=f\"Search: {query[:50]}\", source_query=query)\n",
    "        kb.add_query(query, urls, summary)\n",
    "\n",
    "        decision = CacheDecision(\n",
    "            query=query,\n",
    "            layer_reached=\"L3\",\n",
    "            decision=\"INSUFFICIENT\",\n",
    "            confidence_score=confidence,\n",
    "            action_taken=\"SEARCH\",\n",
    "            reasoning=\"LLM judged cached content insufficient\",\n",
    "            timestamp=timestamp\n",
    "        )\n",
    "        return query_content, urls, decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. LangGraph State and Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeCacheState(TypedDict):\n",
    "    \"\"\"State for the Knowledge Cache Research Agent with multi-sprint verification.\"\"\"\n",
    "    # Input\n",
    "    question: str\n",
    "\n",
    "    # Research planning\n",
    "    search_queries: List[str]\n",
    "\n",
    "    # Accumulated content\n",
    "    accumulated_content: Annotated[List[str], operator.add]\n",
    "    source_urls: Annotated[List[str], operator.add]\n",
    "\n",
    "    # Cache decisions (for observability)\n",
    "    cache_decisions: Annotated[List[dict], operator.add]\n",
    "\n",
    "    # Draft phase\n",
    "    draft_report: str\n",
    "\n",
    "    # Verification sprint management (similar to Agile Sprints backlog)\n",
    "    current_verification_sprint: int\n",
    "    max_verification_sprints: int\n",
    "    claims_backlog: List[Dict[str, Any]]  # Claims yet to verify (replaced each sprint)\n",
    "    verified_claims: Annotated[List[Dict[str, Any]], operator.add]  # Accumulates across sprints\n",
    "\n",
    "    # Verification metrics per sprint\n",
    "    verification_results: Annotated[List[str], operator.add]\n",
    "    sprint_cache_hits: int  # Hits in current sprint\n",
    "    sprint_total_queries: int  # Queries in current sprint\n",
    "    total_verification_cache_hits: int  # Cumulative hits\n",
    "    total_verification_queries: int  # Cumulative queries\n",
    "\n",
    "    # Retrospective control\n",
    "    should_stop_verification: bool\n",
    "    verification_notes: Annotated[List[str], operator.add]\n",
    "\n",
    "    # Output\n",
    "    final_report: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECOMPOSE_PROMPT = \"\"\"You are a research planning expert. Decompose this research question into\n",
    "5-7 specific search queries that together will comprehensively answer the question.\n",
    "\n",
    "Research Question: {question}\n",
    "\n",
    "Requirements:\n",
    "- Each query should be independently searchable\n",
    "- Cover different aspects of the question\n",
    "- Include both broad and specific queries\n",
    "- Later queries may intentionally overlap with earlier ones (to test caching)\n",
    "\n",
    "Return ONLY the search queries, one per line.\n",
    "\"\"\"\n",
    "\n",
    "DRAFT_SYNTHESIS_PROMPT = \"\"\"You are a senior research analyst writing a comprehensive research draft.\n",
    "\n",
    "Research Question: {question}\n",
    "\n",
    "Accumulated Research Content:\n",
    "{content}\n",
    "\n",
    "Source URLs:\n",
    "{sources}\n",
    "\n",
    "Write a comprehensive research draft (1000-1500 words) that:\n",
    "1. Directly answers the research question\n",
    "2. Synthesizes information from multiple sources\n",
    "3. Includes inline citations using [Source: URL] format\n",
    "4. Makes specific factual claims that can be verified\n",
    "\n",
    "Structure the report with clear sections appropriate to the topic.\n",
    "Note: This is a draft that will be verified and revised.\n",
    "\"\"\"\n",
    "\n",
    "CLAIM_EXTRACTION_PROMPT = \"\"\"Extract {num_claims} important factual claims from this content that should be verified.\n",
    "\n",
    "Content to analyze:\n",
    "{content}\n",
    "\n",
    "Already verified claims (DO NOT repeat these):\n",
    "{already_verified}\n",
    "\n",
    "For each NEW claim, provide:\n",
    "- claim_text: The specific factual assertion (be precise)\n",
    "- verification_query: A search query to verify this claim\n",
    "- importance: HIGH, MEDIUM, or LOW\n",
    "\n",
    "Focus on claims that are:\n",
    "1. NOT already in the verified list above\n",
    "2. Specific and verifiable (not opinions)\n",
    "3. Central to the research question\n",
    "4. Quantitative when possible (statistics, dates, numbers)\n",
    "\n",
    "Return as a JSON array with objects containing \"claim_text\", \"verification_query\", and \"importance\" fields.\n",
    "Return ONLY the JSON array, no other text.\n",
    "\"\"\"\n",
    "\n",
    "VERIFICATION_RETROSPECTIVE_PROMPT = \"\"\"You are conducting a verification sprint retrospective.\n",
    "\n",
    "Original Question: {question}\n",
    "\n",
    "Verification Sprint {sprint_num} of {max_sprints} has completed.\n",
    "\n",
    "Sprint Performance:\n",
    "- Claims verified this sprint: {claims_verified}\n",
    "- Cache hits this sprint: {cache_hits} ({hit_rate:.1f}%)\n",
    "- Total queries so far: {total_queries}\n",
    "- Total cache hits so far: {total_cache_hits}\n",
    "\n",
    "Verified Claims Summary:\n",
    "{verified_summary}\n",
    "\n",
    "Remaining Claims in Backlog:\n",
    "{remaining_backlog}\n",
    "\n",
    "Analyze and provide a STRUCTURED response:\n",
    "\n",
    "## LEARNINGS\n",
    "What did verification reveal? Any claims confirmed or contradicted?\n",
    "\n",
    "## GAPS\n",
    "What important aspects still need verification?\n",
    "\n",
    "## CONTINUE\n",
    "Should we continue with another verification sprint? Answer YES or NO.\n",
    "Consider:\n",
    "- If cache hit rate is high (>{min_hit_rate:.0%}), we have good coverage - may stop\n",
    "- If key claims remain unverified, continue\n",
    "- If we've reached diminishing returns, stop\n",
    "\n",
    "## NEW_CLAIMS\n",
    "List 2-4 NEW claims that emerged from verification (or \"None\"):\n",
    "- claim_text: [New factual claim]\n",
    "- verification_query: [Query to verify it]\n",
    "\"\"\"\n",
    "\n",
    "REVISION_PROMPT = \"\"\"You are a senior research analyst finalizing a research report.\n",
    "\n",
    "Research Question: {question}\n",
    "\n",
    "Original Draft:\n",
    "{draft}\n",
    "\n",
    "Verification Results ({num_sprints} verification sprints):\n",
    "{verification_results}\n",
    "\n",
    "Verification Notes:\n",
    "{verification_notes}\n",
    "\n",
    "Original Sources:\n",
    "{sources}\n",
    "\n",
    "Revise the draft into a final report (1000-1500 words) that:\n",
    "1. Incorporates verification findings from all sprints\n",
    "2. Corrects any inaccuracies found during verification\n",
    "3. Strengthens claims that were confirmed\n",
    "4. Notes any claims that could not be verified\n",
    "5. Maintains inline citations using [Source: URL] format\n",
    "\n",
    "Structure the report with clear sections appropriate to the topic.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def plan_research(state: KnowledgeCacheState) -> dict:\n",
    "    \"\"\"Decompose the research question into search queries.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Phase 1: Planning Research\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    prompt = DECOMPOSE_PROMPT.format(question=question)\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    queries = [q.strip() for q in response.content.split(\"\\n\") if q.strip()][:7]\n",
    "\n",
    "    print(f\"  Generated {len(queries)} search queries\")\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        print(f\"    {i}. {q[:60]}...\")\n",
    "\n",
    "    return {\"search_queries\": queries}\n",
    "\n",
    "\n",
    "async def execute_searches(state: KnowledgeCacheState) -> dict:\n",
    "    \"\"\"Execute all searches through the cascading cache.\"\"\"\n",
    "    queries = state.get(\"search_queries\", [])\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Phase 2: Executing Cascaded Searches\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    all_content = []\n",
    "    all_urls = []\n",
    "    all_decisions = []\n",
    "\n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\n  [{i}/{len(queries)}] Query: {query[:50]}...\")\n",
    "\n",
    "        content, urls, decision = await cascaded_search(query, knowledge_base)\n",
    "\n",
    "        all_content.append(f\"### Query: {query}\\n\\n{content}\")\n",
    "        all_urls.extend(urls)\n",
    "        all_decisions.append(decision.model_dump())\n",
    "\n",
    "        # Log decision\n",
    "        action_symbol = {\n",
    "            \"USE_CACHE\": \"CACHE HIT\",\n",
    "            \"SEARCH\": \"WEB SEARCH\",\n",
    "            \"TARGETED_SEARCH\": \"TARGETED\"\n",
    "        }\n",
    "        print(f\"      {action_symbol.get(decision.action_taken, '?')} | \"\n",
    "              f\"Layer: {decision.layer_reached} | \"\n",
    "              f\"Conf: {decision.confidence_score:.2f}\")\n",
    "\n",
    "    print(f\"\\n  --- Research Phase Complete ---\")\n",
    "    print(f\"  Web searches executed: {knowledge_base.stats['web_searches_executed']}\")\n",
    "\n",
    "    return {\n",
    "        \"accumulated_content\": all_content,\n",
    "        \"source_urls\": all_urls,\n",
    "        \"cache_decisions\": all_decisions\n",
    "    }\n",
    "\n",
    "\n",
    "async def synthesize_draft(state: KnowledgeCacheState) -> dict:\n",
    "    \"\"\"Synthesize initial draft report from accumulated content.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    content = state.get(\"accumulated_content\", [])\n",
    "    urls = list(set(state.get(\"source_urls\", [])))\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Phase 3: Synthesizing Draft Report\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    combined_content = \"\\n\\n---\\n\\n\".join(content)\n",
    "\n",
    "    # Token efficiency: limit content\n",
    "    if len(combined_content) > 15000:\n",
    "        combined_content = combined_content[:15000] + \"\\n\\n[... truncated ...]\"\n",
    "\n",
    "    prompt = DRAFT_SYNTHESIS_PROMPT.format(\n",
    "        question=question,\n",
    "        content=combined_content,\n",
    "        sources=\"\\n\".join(urls[:20])\n",
    "    )\n",
    "\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    print(f\"  Draft generated: {len(response.content)} characters\")\n",
    "\n",
    "    # Initialize verification sprint state\n",
    "    return {\n",
    "        \"draft_report\": response.content,\n",
    "        \"current_verification_sprint\": 1,\n",
    "        \"max_verification_sprints\": MAX_VERIFICATION_SPRINTS,\n",
    "        \"total_verification_cache_hits\": 0,\n",
    "        \"total_verification_queries\": 0,\n",
    "        \"should_stop_verification\": False\n",
    "    }\n",
    "\n",
    "\n",
    "async def extract_claims(state: KnowledgeCacheState) -> dict:\n",
    "    \"\"\"Extract claims for the current verification sprint.\"\"\"\n",
    "    draft = state.get(\"draft_report\", \"\")\n",
    "    current_sprint = state.get(\"current_verification_sprint\", 1)\n",
    "    verified_claims = state.get(\"verified_claims\", [])\n",
    "    verification_results = state.get(\"verification_results\", [])\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Verification Sprint {current_sprint}: Extracting Claims\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # For first sprint, use draft. For later sprints, include verification results too\n",
    "    if current_sprint == 1:\n",
    "        content_to_analyze = draft\n",
    "    else:\n",
    "        # Include recent verification findings for new claim extraction\n",
    "        recent_results = \"\\n\\n\".join(verification_results[-CLAIMS_PER_SPRINT:])\n",
    "        content_to_analyze = f\"{draft}\\n\\n### Recent Verification Findings:\\n{recent_results}\"\n",
    "\n",
    "    # Format already verified claims\n",
    "    already_verified = \"\\n\".join([\n",
    "        f\"- {c.get('claim_text', '')}\" for c in verified_claims\n",
    "    ]) if verified_claims else \"None yet\"\n",
    "\n",
    "    prompt = CLAIM_EXTRACTION_PROMPT.format(\n",
    "        num_claims=CLAIMS_PER_SPRINT,\n",
    "        content=content_to_analyze[:8000],  # Token limit\n",
    "        already_verified=already_verified\n",
    "    )\n",
    "\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # Parse JSON response\n",
    "    try:\n",
    "        content = response.content\n",
    "        start = content.find('[')\n",
    "        end = content.rfind(']') + 1\n",
    "        if start >= 0 and end > start:\n",
    "            claims = json.loads(content[start:end])\n",
    "        else:\n",
    "            claims = []\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"  Warning: Could not parse claims JSON, using empty list\")\n",
    "        claims = []\n",
    "\n",
    "    # Limit claims per sprint\n",
    "    claims = claims[:CLAIMS_PER_SPRINT]\n",
    "\n",
    "    print(f\"  Extracted {len(claims)} new claims for verification\")\n",
    "    for i, claim in enumerate(claims, 1):\n",
    "        claim_text = claim.get(\"claim_text\", \"\")[:55]\n",
    "        importance = claim.get(\"importance\", \"MEDIUM\")\n",
    "        print(f\"    {i}. [{importance}] {claim_text}...\")\n",
    "\n",
    "    return {\"claims_backlog\": claims}\n",
    "\n",
    "\n",
    "async def verify_claims(state: KnowledgeCacheState) -> dict:\n",
    "    \"\"\"Execute verification searches for current sprint's claims.\"\"\"\n",
    "    claims = state.get(\"claims_backlog\", [])\n",
    "    current_sprint = state.get(\"current_verification_sprint\", 1)\n",
    "    total_hits = state.get(\"total_verification_cache_hits\", 0)\n",
    "    total_queries = state.get(\"total_verification_queries\", 0)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Verification Sprint {current_sprint}: Verifying {len(claims)} Claims\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    verification_results = []\n",
    "    verified_claims = []\n",
    "    sprint_cache_hits = 0\n",
    "    sprint_queries = 0\n",
    "\n",
    "    for i, claim in enumerate(claims, 1):\n",
    "        query = claim.get(\"verification_query\", \"\")\n",
    "        claim_text = claim.get(\"claim_text\", \"\")\n",
    "\n",
    "        if not query:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n  [{i}/{len(claims)}] {claim_text[:50]}...\")\n",
    "\n",
    "        content, urls, decision = await cascaded_search(query, knowledge_base)\n",
    "        sprint_queries += 1\n",
    "\n",
    "        if decision.action_taken == \"USE_CACHE\":\n",
    "            sprint_cache_hits += 1\n",
    "            print(f\"      CACHE HIT | Layer: {decision.layer_reached} | Conf: {decision.confidence_score:.2f}\")\n",
    "        else:\n",
    "            print(f\"      WEB SEARCH | Layer: {decision.layer_reached} | Conf: {decision.confidence_score:.2f}\")\n",
    "\n",
    "        # Record verification result\n",
    "        truncated_content = content[:600] if len(content) > 600 else content\n",
    "        verification_results.append(\n",
    "            f\"### Claim: {claim_text}\\n\"\n",
    "            f\"Query: {query}\\n\"\n",
    "            f\"Evidence: {truncated_content}\\n\"\n",
    "            f\"Sources: {', '.join(urls[:3])}\"\n",
    "        )\n",
    "\n",
    "        # Mark claim as verified\n",
    "        verified_claims.append({\n",
    "            \"claim_text\": claim_text,\n",
    "            \"verification_query\": query,\n",
    "            \"evidence\": truncated_content,\n",
    "            \"cache_hit\": decision.action_taken == \"USE_CACHE\",\n",
    "            \"confidence\": decision.confidence_score\n",
    "        })\n",
    "\n",
    "    # Update cumulative stats\n",
    "    new_total_hits = total_hits + sprint_cache_hits\n",
    "    new_total_queries = total_queries + sprint_queries\n",
    "\n",
    "    sprint_hit_rate = sprint_cache_hits / max(sprint_queries, 1) * 100\n",
    "    cumulative_hit_rate = new_total_hits / max(new_total_queries, 1) * 100\n",
    "\n",
    "    print(f\"\\n  --- Sprint {current_sprint} Verification Complete ---\")\n",
    "    print(f\"  Sprint: {sprint_cache_hits}/{sprint_queries} cache hits ({sprint_hit_rate:.1f}%)\")\n",
    "    print(f\"  Cumulative: {new_total_hits}/{new_total_queries} cache hits ({cumulative_hit_rate:.1f}%)\")\n",
    "\n",
    "    return {\n",
    "        \"verification_results\": verification_results,\n",
    "        \"verified_claims\": verified_claims,\n",
    "        \"sprint_cache_hits\": sprint_cache_hits,\n",
    "        \"sprint_total_queries\": sprint_queries,\n",
    "        \"total_verification_cache_hits\": new_total_hits,\n",
    "        \"total_verification_queries\": new_total_queries\n",
    "    }\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_should_continue_verification(response_content: str) -> bool:\n",
    "    \"\"\"Parse whether retrospective recommends continuing verification.\"\"\"\n",
    "    continue_match = re.search(\n",
    "        r'## CONTINUE\\s*(.*?)(?=##|$)',\n",
    "        response_content,\n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    if continue_match:\n",
    "        continue_text = continue_match.group(1).strip().lower()\n",
    "        # Check for negative patterns\n",
    "        negative_patterns = [\n",
    "            r'^no\\b', r'^not\\b', r'should\\s+stop', r'should\\s+not\\s+continue',\n",
    "            r'no\\s+need', r'sufficient', r'adequately', r'fully\\s+verified',\n",
    "            r'diminishing\\s+returns', r'good\\s+coverage'\n",
    "        ]\n",
    "        for pattern in negative_patterns:\n",
    "            if re.search(pattern, continue_text):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def parse_new_claims(response_content: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Parse new claims from retrospective output.\"\"\"\n",
    "    claims_match = re.search(\n",
    "        r'## NEW_CLAIMS\\s*(.*?)(?=##|$)',\n",
    "        response_content,\n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    new_claims = []\n",
    "    if claims_match:\n",
    "        claims_text = claims_match.group(1).strip()\n",
    "        if claims_text.lower() == \"none\" or not claims_text:\n",
    "            return []\n",
    "\n",
    "        # Parse claim entries\n",
    "        claim_pattern = r'claim_text:\\s*(.+?)(?:\\n|$).*?verification_query:\\s*(.+?)(?:\\n|$)'\n",
    "        matches = re.findall(claim_pattern, claims_text, re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "        for claim_text, query in matches:\n",
    "            claim_text = claim_text.strip().strip('[]\"\\'')\n",
    "            query = query.strip().strip('[]\"\\'')\n",
    "            if claim_text and query and len(claim_text) > 10:\n",
    "                new_claims.append({\n",
    "                    \"claim_text\": claim_text,\n",
    "                    \"verification_query\": query,\n",
    "                    \"importance\": \"MEDIUM\"\n",
    "                })\n",
    "\n",
    "    return new_claims[:4]  # Limit to 4 new claims\n",
    "\n",
    "\n",
    "async def verification_retrospective(state: KnowledgeCacheState) -> dict:\n",
    "    \"\"\"Conduct retrospective after verification sprint - decide whether to continue.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    current_sprint = state.get(\"current_verification_sprint\", 1)\n",
    "    max_sprints = state.get(\"max_verification_sprints\", MAX_VERIFICATION_SPRINTS)\n",
    "    verified_claims = state.get(\"verified_claims\", [])\n",
    "    claims_backlog = state.get(\"claims_backlog\", [])\n",
    "\n",
    "    sprint_hits = state.get(\"sprint_cache_hits\", 0)\n",
    "    sprint_queries = state.get(\"sprint_total_queries\", 0)\n",
    "    total_hits = state.get(\"total_verification_cache_hits\", 0)\n",
    "    total_queries = state.get(\"total_verification_queries\", 0)\n",
    "\n",
    "    hit_rate = sprint_hits / max(sprint_queries, 1) * 100\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Verification Sprint {current_sprint}: Retrospective\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Format verified claims summary\n",
    "    verified_summary = \"\\n\".join([\n",
    "        f\"- {c.get('claim_text', '')[:60]}... [{'CACHE' if c.get('cache_hit') else 'WEB'}]\"\n",
    "        for c in verified_claims[-8:]  # Last 8 verified\n",
    "    ]) if verified_claims else \"None yet\"\n",
    "\n",
    "    # Format remaining backlog (claims not yet processed)\n",
    "    remaining = \"\\n\".join([\n",
    "        f\"- {c.get('claim_text', '')}\" for c in claims_backlog\n",
    "    ]) if claims_backlog else \"None\"\n",
    "\n",
    "    prompt = VERIFICATION_RETROSPECTIVE_PROMPT.format(\n",
    "        question=question,\n",
    "        sprint_num=current_sprint,\n",
    "        max_sprints=max_sprints,\n",
    "        claims_verified=sprint_queries,\n",
    "        cache_hits=sprint_hits,\n",
    "        hit_rate=hit_rate,\n",
    "        total_queries=total_queries,\n",
    "        total_cache_hits=total_hits,\n",
    "        verified_summary=verified_summary,\n",
    "        remaining_backlog=remaining,\n",
    "        min_hit_rate=MIN_CACHE_HIT_RATE_TO_STOP\n",
    "    )\n",
    "\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # Parse response\n",
    "    should_continue = parse_should_continue_verification(response.content)\n",
    "    new_claims = parse_new_claims(response.content)\n",
    "\n",
    "    # Add new claims to backlog for next sprint\n",
    "    updated_backlog = new_claims if new_claims else []\n",
    "\n",
    "    retro_note = f\"### Verification Sprint {current_sprint} Retrospective\\n\\n{response.content}\"\n",
    "\n",
    "    if should_continue and current_sprint < max_sprints:\n",
    "        print(f\"  Retrospective: CONTINUE to sprint {current_sprint + 1}\")\n",
    "        print(f\"  New claims added to backlog: {len(new_claims)}\")\n",
    "    else:\n",
    "        if current_sprint >= max_sprints:\n",
    "            print(f\"  Retrospective: STOP (max sprints reached)\")\n",
    "        else:\n",
    "            print(f\"  Retrospective: STOP (sufficient coverage)\")\n",
    "\n",
    "    return {\n",
    "        \"verification_notes\": [retro_note],\n",
    "        \"current_verification_sprint\": current_sprint + 1,\n",
    "        \"claims_backlog\": updated_backlog,\n",
    "        \"should_stop_verification\": not should_continue\n",
    "    }\n",
    "\n",
    "\n",
    "def should_continue_verification(state: KnowledgeCacheState) -> Literal[\"extract_claims\", \"revise_report\"]:\n",
    "    \"\"\"Decide whether to continue verification or write final report.\"\"\"\n",
    "    current_sprint = state.get(\"current_verification_sprint\", 1)\n",
    "    max_sprints = state.get(\"max_verification_sprints\", MAX_VERIFICATION_SPRINTS)\n",
    "    should_stop = state.get(\"should_stop_verification\", False)\n",
    "    claims_backlog = state.get(\"claims_backlog\", [])\n",
    "\n",
    "    # Stop conditions\n",
    "    if current_sprint > max_sprints:\n",
    "        print(f\"\\n  Max verification sprints ({max_sprints}) reached. Moving to revision.\")\n",
    "        return \"revise_report\"\n",
    "\n",
    "    if should_stop:\n",
    "        print(f\"\\n  Retrospective recommends stopping. Moving to revision.\")\n",
    "        return \"revise_report\"\n",
    "\n",
    "    if not claims_backlog:\n",
    "        print(f\"\\n  No more claims to verify. Moving to revision.\")\n",
    "        return \"revise_report\"\n",
    "\n",
    "    print(f\"\\n  Continuing to verification sprint {current_sprint}. {len(claims_backlog)} claims in backlog.\")\n",
    "    return \"extract_claims\"\n",
    "\n",
    "\n",
    "async def revise_report(state: KnowledgeCacheState) -> dict:\n",
    "    \"\"\"Incorporate all verification findings into final report.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    draft = state.get(\"draft_report\", \"\")\n",
    "    verification_results = state.get(\"verification_results\", [])\n",
    "    verification_notes = state.get(\"verification_notes\", [])\n",
    "    urls = list(set(state.get(\"source_urls\", [])))\n",
    "    num_sprints = state.get(\"current_verification_sprint\", 1) - 1\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 4: Revising Report ({num_sprints} verification sprints)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    combined_verification = \"\\n\\n---\\n\\n\".join(verification_results)\n",
    "    combined_notes = \"\\n\\n\".join(verification_notes)\n",
    "\n",
    "    # Token management\n",
    "    if len(combined_verification) > 6000:\n",
    "        combined_verification = combined_verification[:6000] + \"\\n\\n[... truncated ...]\"\n",
    "\n",
    "    prompt = REVISION_PROMPT.format(\n",
    "        question=question,\n",
    "        draft=draft,\n",
    "        num_sprints=num_sprints,\n",
    "        verification_results=combined_verification,\n",
    "        verification_notes=combined_notes[:2000],\n",
    "        sources=\"\\n\".join(urls[:20])\n",
    "    )\n",
    "\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    print(f\"  Final report generated: {len(response.content)} characters\")\n",
    "    print(f\"  Sources cited: {len(urls)}\")\n",
    "\n",
    "    # Print final cache stats\n",
    "    total_hits = state.get(\"total_verification_cache_hits\", 0)\n",
    "    total_queries = state.get(\"total_verification_queries\", 0)\n",
    "    research_queries = knowledge_base.stats[\"total_queries\"] - total_queries\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FINAL CACHE PERFORMANCE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nResearch Phase:\")\n",
    "    print(f\"  Queries: {research_queries}\")\n",
    "    print(f\"\\nVerification Phase ({num_sprints} sprints):\")\n",
    "    print(f\"  Total queries: {total_queries}\")\n",
    "    print(f\"  Cache hits: {total_hits} ({total_hits/max(total_queries,1)*100:.1f}%)\")\n",
    "    print(f\"\\nOverall:\")\n",
    "    print(knowledge_base.get_stats_summary())\n",
    "\n",
    "    return {\"final_report\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Cache Research Agent compiled successfully\n",
      "\n",
      "Architecture Flow:\n",
      "  plan_research  execute_searches  synthesize_draft \n",
      "  [VERIFICATION SPRINT LOOP]\n",
      "    extract_claims  verify_claims  verification_retrospective \n",
      "    (continue loop OR exit)\n",
      "  [END LOOP]\n",
      "   revise_report  END\n",
      "\n",
      "Key features:\n",
      "  - Multi-sprint verification (like Agile Sprints pattern)\n",
      "  - Retrospective decides whether to continue verification\n",
      "  - New claims emerge from verification findings\n",
      "  - Cache hits expected to increase in later sprints\n"
     ]
    }
   ],
   "source": [
    "# Build the Knowledge Cache Research Agent graph with Multi-Sprint Verification Loop\n",
    "kc_builder = StateGraph(KnowledgeCacheState)\n",
    "\n",
    "# Add nodes\n",
    "kc_builder.add_node(\"plan_research\", plan_research)\n",
    "kc_builder.add_node(\"execute_searches\", execute_searches)\n",
    "kc_builder.add_node(\"synthesize_draft\", synthesize_draft)\n",
    "kc_builder.add_node(\"extract_claims\", extract_claims)\n",
    "kc_builder.add_node(\"verify_claims\", verify_claims)\n",
    "kc_builder.add_node(\"verification_retrospective\", verification_retrospective)\n",
    "kc_builder.add_node(\"revise_report\", revise_report)\n",
    "\n",
    "# Add edges - research phase (linear)\n",
    "kc_builder.add_edge(START, \"plan_research\")\n",
    "kc_builder.add_edge(\"plan_research\", \"execute_searches\")\n",
    "kc_builder.add_edge(\"execute_searches\", \"synthesize_draft\")\n",
    "kc_builder.add_edge(\"synthesize_draft\", \"extract_claims\")\n",
    "\n",
    "# Verification sprint loop\n",
    "kc_builder.add_edge(\"extract_claims\", \"verify_claims\")\n",
    "kc_builder.add_edge(\"verify_claims\", \"verification_retrospective\")\n",
    "\n",
    "# Conditional: continue verification loop or move to revision\n",
    "kc_builder.add_conditional_edges(\n",
    "    \"verification_retrospective\",\n",
    "    should_continue_verification,\n",
    "    {\n",
    "        \"extract_claims\": \"extract_claims\",  # Continue loop\n",
    "        \"revise_report\": \"revise_report\"      # Exit loop\n",
    "    }\n",
    ")\n",
    "\n",
    "kc_builder.add_edge(\"revise_report\", END)\n",
    "\n",
    "# Compile\n",
    "knowledge_cache_graph = kc_builder.compile()\n",
    "\n",
    "print(\"Knowledge Cache Research Agent compiled successfully\")\n",
    "print(\"\\nArchitecture Flow:\")\n",
    "print(\"  plan_research  execute_searches  synthesize_draft \")\n",
    "print(\"  [VERIFICATION SPRINT LOOP]\")\n",
    "print(\"    extract_claims  verify_claims  verification_retrospective \")\n",
    "print(\"    (continue loop OR exit)\")\n",
    "print(\"  [END LOOP]\")\n",
    "print(\"   revise_report  END\")\n",
    "print(\"\\nKey features:\")\n",
    "print(\"  - Multi-sprint verification (like Agile Sprints pattern)\")\n",
    "print(\"  - Retrospective decides whether to continue verification\")\n",
    "print(\"  - New claims emerge from verification findings\")\n",
    "print(\"  - Cache hits expected to increase in later sprints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(knowledge_cache_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Agent Wrapper for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def knowledge_cache_agent_async(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Async version of the Knowledge Cache research agent.\n",
    "    Use this version when calling from Jupyter notebooks.\n",
    "    \"\"\"\n",
    "    # Reset knowledge base for fresh session\n",
    "    global knowledge_base\n",
    "    knowledge_base = KnowledgeBase()\n",
    "\n",
    "    question = inputs.get(\"question\", \"\")\n",
    "\n",
    "    result = await knowledge_cache_graph.ainvoke(\n",
    "        {\"question\": question},\n",
    "        config={\"recursion_limit\": 100}  # Higher limit for multi-sprint\n",
    "    )\n",
    "\n",
    "    num_sprints = result.get(\"current_verification_sprint\", 1) - 1\n",
    "\n",
    "    return {\n",
    "        \"output\": result.get(\"final_report\", \"\"),\n",
    "        \"source_urls\": list(set(result.get(\"source_urls\", []))),\n",
    "        \"cache_decisions\": result.get(\"cache_decisions\", []),\n",
    "        \"cache_stats\": knowledge_base.stats.copy(),\n",
    "        \"verification_sprints\": num_sprints,\n",
    "        \"total_verification_queries\": result.get(\"total_verification_queries\", 0),\n",
    "        \"total_verification_cache_hits\": result.get(\"total_verification_cache_hits\", 0),\n",
    "        \"verified_claims\": result.get(\"verified_claims\", []),\n",
    "        \"verification_notes\": result.get(\"verification_notes\", [])\n",
    "    }\n",
    "\n",
    "\n",
    "def knowledge_cache_agent(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Sync wrapper function for Knowledge Cache research agent.\n",
    "\n",
    "    Compatible with evaluation harness.\n",
    "    \"\"\"\n",
    "    question = inputs.get(\"question\", \"\")\n",
    "\n",
    "    async def _execute():\n",
    "        global knowledge_base\n",
    "        knowledge_base = KnowledgeBase()\n",
    "\n",
    "        return await knowledge_cache_graph.ainvoke(\n",
    "            {\"question\": question},\n",
    "            config={\"recursion_limit\": 100}\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        import concurrent.futures\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future = executor.submit(asyncio.run, _execute())\n",
    "            result = future.result()\n",
    "    except RuntimeError:\n",
    "        result = asyncio.run(_execute())\n",
    "\n",
    "    num_sprints = result.get(\"current_verification_sprint\", 1) - 1\n",
    "\n",
    "    return {\n",
    "        \"output\": result.get(\"final_report\", \"\"),\n",
    "        \"source_urls\": list(set(result.get(\"source_urls\", []))),\n",
    "        \"cache_decisions\": result.get(\"cache_decisions\", []),\n",
    "        \"cache_stats\": knowledge_base.stats.copy(),\n",
    "        \"verification_sprints\": num_sprints,\n",
    "        \"total_verification_queries\": result.get(\"total_verification_queries\", 0),\n",
    "        \"total_verification_cache_hits\": result.get(\"total_verification_cache_hits\", 0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Manual Test\n",
    "\n",
    "Run this cell to verify the agent works correctly with a simple test question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Knowledge Cache Agent with question:\n",
      "What are the key benefits and challenges of using large language models in enterprise applications?\n",
      "\n",
      "Running cascaded cache research with multi-sprint verification...\n",
      "(This may take several minutes due to multiple verification sprints)\n",
      "\n",
      "\n",
      "============================================================\n",
      "Phase 1: Planning Research\n",
      "============================================================\n",
      "  Generated 7 search queries\n",
      "    1. Benefits and challenges of using large language models (LLMs...\n",
      "    2. Technical challenges of deploying large language models in e...\n",
      "    3. Security, privacy, and compliance risks of LLMs in enterpris...\n",
      "    4. Cost, ROI, and total cost of ownership for enterprise adopti...\n",
      "    5. Governance, ethics, and explainability issues with enterpris...\n",
      "    6. Business use cases and measurable benefits of large language...\n",
      "    7. Architecture and integration best practices for enterprise L...\n",
      "\n",
      "============================================================\n",
      "Phase 2: Executing Cascaded Searches\n",
      "============================================================\n",
      "\n",
      "  [1/7] Query: Benefits and challenges of using large language mo...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.00\n",
      "\n",
      "  [2/7] Query: Technical challenges of deploying large language m...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.39\n",
      "\n",
      "  [3/7] Query: Security, privacy, and compliance risks of LLMs in...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.28\n",
      "\n",
      "  [4/7] Query: Cost, ROI, and total cost of ownership for enterpr...\n",
      "      TARGETED | Layer: L3 | Conf: 0.40\n",
      "\n",
      "  [5/7] Query: Governance, ethics, and explainability issues with...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.35\n",
      "\n",
      "  [6/7] Query: Business use cases and measurable benefits of larg...\n",
      "      TARGETED | Layer: L3 | Conf: 0.45\n",
      "\n",
      "  [7/7] Query: Architecture and integration best practices for en...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.32\n",
      "\n",
      "  --- Research Phase Complete ---\n",
      "  Web searches executed: 9\n",
      "\n",
      "============================================================\n",
      "Phase 3: Synthesizing Draft Report\n",
      "============================================================\n",
      "  Draft generated: 12851 characters\n",
      "\n",
      "============================================================\n",
      "Verification Sprint 1: Extracting Claims\n",
      "============================================================\n",
      "  Extracted 6 new claims for verification\n",
      "    1. [HIGH] Enterprises report gains in productivity, faster docume...\n",
      "    2. [HIGH] Combining retrieval-augmented generation (RAG) with LLM...\n",
      "    3. [HIGH] Inference for large LLMs typically requires GPUs or oth...\n",
      "    4. [HIGH] There are documented incidents and research demonstrati...\n",
      "    5. [HIGH] Regulatory bodies have issued guidance expecting data m...\n",
      "    6. [MEDIUM] Total cost-of-ownership analyses commonly recommend clo...\n",
      "\n",
      "============================================================\n",
      "Verification Sprint 1: Verifying 6 Claims\n",
      "============================================================\n",
      "\n",
      "  [1/6] Enterprises report gains in productivity, faster d...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.37\n",
      "\n",
      "  [2/6] Combining retrieval-augmented generation (RAG) wit...\n",
      "      WEB SEARCH | Layer: L3 | Conf: 0.44\n",
      "\n",
      "  [3/6] Inference for large LLMs typically requires GPUs o...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.37\n",
      "\n",
      "  [4/6] There are documented incidents and research demons...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.32\n",
      "\n",
      "  [5/6] Regulatory bodies have issued guidance expecting d...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.33\n",
      "\n",
      "  [6/6] Total cost-of-ownership analyses commonly recommen...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.39\n",
      "\n",
      "  --- Sprint 1 Verification Complete ---\n",
      "  Sprint: 0/6 cache hits (0.0%)\n",
      "  Cumulative: 0/6 cache hits (0.0%)\n",
      "\n",
      "============================================================\n",
      "Verification Sprint 1: Retrospective\n",
      "============================================================\n",
      "  Retrospective: CONTINUE to sprint 2\n",
      "  New claims added to backlog: 3\n",
      "\n",
      "  Continuing to verification sprint 2. 3 claims in backlog.\n",
      "\n",
      "============================================================\n",
      "Verification Sprint 2: Extracting Claims\n",
      "============================================================\n",
      "  Extracted 6 new claims for verification\n",
      "    1. [HIGH] LLMOps is emerging as a distinct operational discipline...\n",
      "    2. [HIGH] Tooling and standards for prompt and prompt-context ver...\n",
      "    3. [HIGH] Typical production LLM deployments are multi-component ...\n",
      "    4. [HIGH] Prompt-injection attacks have been demonstrated in rese...\n",
      "    5. [MEDIUM] Operational costs beyond raw computespecifically data ...\n",
      "    6. [MEDIUM] Model distillation or deploying smaller specialist mode...\n",
      "\n",
      "============================================================\n",
      "Verification Sprint 2: Verifying 6 Claims\n",
      "============================================================\n",
      "\n",
      "  [1/6] LLMOps is emerging as a distinct operational disci...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.36\n",
      "\n",
      "  [2/6] Tooling and standards for prompt and prompt-contex...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.25\n",
      "\n",
      "  [3/6] Typical production LLM deployments are multi-compo...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.35\n",
      "\n",
      "  [4/6] Prompt-injection attacks have been demonstrated in...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.23\n",
      "\n",
      "  [5/6] Operational costs beyond raw computespecifically ...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.34\n",
      "\n",
      "  [6/6] Model distillation or deploying smaller specialist...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.30\n",
      "\n",
      "  --- Sprint 2 Verification Complete ---\n",
      "  Sprint: 0/6 cache hits (0.0%)\n",
      "  Cumulative: 0/12 cache hits (0.0%)\n",
      "\n",
      "============================================================\n",
      "Verification Sprint 2: Retrospective\n",
      "============================================================\n",
      "  Retrospective: CONTINUE to sprint 3\n",
      "  New claims added to backlog: 4\n",
      "\n",
      "  Continuing to verification sprint 3. 4 claims in backlog.\n",
      "\n",
      "============================================================\n",
      "Verification Sprint 3: Extracting Claims\n",
      "============================================================\n",
      "  Extracted 6 new claims for verification\n",
      "    1. [HIGH] LLMs are being used in enterprise settings to generate ...\n",
      "    2. [HIGH] Enterprises are adopting LLMs for legal and contract re...\n",
      "    3. [HIGH] LLMs' in-context learning and fewshot prompting enable...\n",
      "    4. [MEDIUM] Common latency mitigations for interactive LLM applicat...\n",
      "    5. [HIGH] Finetuning LLMs for enterprise use requires preparing ...\n",
      "    6. [HIGH] Retrievalaugmented generation (RAG) configurations can...\n",
      "\n",
      "============================================================\n",
      "Verification Sprint 3: Verifying 6 Claims\n",
      "============================================================\n",
      "\n",
      "  [1/6] LLMs are being used in enterprise settings to gene...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.36\n",
      "\n",
      "  [2/6] Enterprises are adopting LLMs for legal and contra...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.32\n",
      "\n",
      "  [3/6] LLMs' in-context learning and fewshot prompting e...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.26\n",
      "\n",
      "  [4/6] Common latency mitigations for interactive LLM app...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.32\n",
      "\n",
      "  [5/6] Finetuning LLMs for enterprise use requires prepa...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.32\n",
      "\n",
      "  [6/6] Retrievalaugmented generation (RAG) configuration...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.30\n",
      "\n",
      "  --- Sprint 3 Verification Complete ---\n",
      "  Sprint: 0/6 cache hits (0.0%)\n",
      "  Cumulative: 0/18 cache hits (0.0%)\n",
      "\n",
      "============================================================\n",
      "Verification Sprint 3: Retrospective\n",
      "============================================================\n",
      "  Retrospective: CONTINUE to sprint 4\n",
      "  New claims added to backlog: 3\n",
      "\n",
      "  Continuing to verification sprint 4. 3 claims in backlog.\n",
      "\n",
      "============================================================\n",
      "Verification Sprint 4: Extracting Claims\n",
      "============================================================\n",
      "  Extracted 6 new claims for verification\n",
      "    1. [HIGH] Enterprises are replacing rule-based systems with LLMs ...\n",
      "    2. [HIGH] Batching and multi-model routing are commonly used oper...\n",
      "    3. [HIGH] Adding a retrieval step (RAG) to an LLM inference pipel...\n",
      "    4. [HIGH] Prompt/version drift and data drift, together with freq...\n",
      "    5. [MEDIUM] Enterprises use LLMs to generate marketing collateral, ...\n",
      "    6. [HIGH] Real-time or interactive enterprise applications (e.g.,...\n",
      "\n",
      "============================================================\n",
      "Verification Sprint 4: Verifying 6 Claims\n",
      "============================================================\n",
      "\n",
      "  [1/6] Enterprises are replacing rule-based systems with ...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.34\n",
      "\n",
      "  [2/6] Batching and multi-model routing are commonly used...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.36\n",
      "\n",
      "  [3/6] Adding a retrieval step (RAG) to an LLM inference ...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.36\n",
      "\n",
      "  [4/6] Prompt/version drift and data drift, together with...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.29\n",
      "\n",
      "  [5/6] Enterprises use LLMs to generate marketing collate...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.33\n",
      "\n",
      "  [6/6] Real-time or interactive enterprise applications (...\n",
      "      WEB SEARCH | Layer: L2 | Conf: 0.40\n",
      "\n",
      "  --- Sprint 4 Verification Complete ---\n",
      "  Sprint: 0/6 cache hits (0.0%)\n",
      "  Cumulative: 0/24 cache hits (0.0%)\n",
      "\n",
      "============================================================\n",
      "Verification Sprint 4: Retrospective\n",
      "============================================================\n",
      "  Retrospective: STOP (sufficient coverage)\n",
      "\n",
      "  Retrospective recommends stopping. Moving to revision.\n",
      "\n",
      "============================================================\n",
      "Phase 4: Revising Report (4 verification sprints)\n",
      "============================================================\n",
      "  Final report generated: 15107 characters\n",
      "  Sources cited: 55\n",
      "\n",
      "============================================================\n",
      "FINAL CACHE PERFORMANCE\n",
      "============================================================\n",
      "\n",
      "Research Phase:\n",
      "  Queries: 7\n",
      "\n",
      "Verification Phase (4 sprints):\n",
      "  Total queries: 24\n",
      "  Cache hits: 0 (0.0%)\n",
      "\n",
      "Overall:\n",
      "\n",
      "Cache Statistics:\n",
      "- Total queries: 31\n",
      "- Web searches avoided: 0 (0.0% hit rate)\n",
      "- Web searches executed: 34\n",
      "- Layer 1 hits: 0\n",
      "- Layer 2 HIGH confidence: 0\n",
      "- Layer 2 MEDIUM confidence: 3\n",
      "- Layer 2 LOW confidence: 28\n",
      "- Layer 3 SUFFICIENT: 0\n",
      "- Layer 3 PARTIAL: 3\n",
      "- Layer 3 INSUFFICIENT: 0\n",
      "- Documents cached: 34\n",
      "- Chunks indexed: 347\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FINAL REPORT\n",
      "================================================================================\n",
      "Title: Benefits and Challenges of Using Large Language Models (LLMs) in Enterprise Applications  Final Report\n",
      "\n",
      "Executive summary\n",
      "Large language models (LLMs) are being adopted across enterprises to automate knowledge work, provide natural-language interfaces, and extract structured insights from unstructured text. Practitioner reports and early academic studies indicate measurable productivity improvements, faster document processing, and improved customer experiences when LLMs are applied appropriately [Source: https://www.microsoft.com/en-us/research/publication/early-llm-based-tools-for-enterprise-information-workers-likely-provide-meaningful-boosts-to-productivity/][Source: https://www.v7labs.com/blog/impact-of-large-language-models-on-enterprise][Source: https://www.kellton.com/kellton-tech-blog/large-language-models-challenges-benefits]. However, enterprise adoption brings distinct operational, security, compliance, and cost challenges (inference cost and latency, governance/model/version management, data leakage, and the need for LLM-specific operations) that require dedicated mitigations and early governance [Source: https://appinventiv.com/blog/scaling-language-models-with-llmops/][Source: https://dl.acm.org/doi/full/10.1145/3747346]. This report summarizes verified benefits, enumerates major challenges, documents mitigations and best practices, and highlights evidence gaps requiring further quantification.\n",
      "\n",
      "1. Key enterprise benefits (verified)\n",
      "- Automation of knowledge work and content generation: LLMs reduce manual effort for email drafts, summaries, report drafts, marketing copy, and code suggestions and enable AI copilot workflows for knowledge workers. Multiple enterprise case studies and early empirical work show meaningful productivity improvements in information-worker tasks when LLM-based tools are introduced [Source: https://www.microsoft.com/en-us/research/publication/early-llm-based-tools-for-enterprise-information-workers-likely-provide-meaningful-boosts-to-productivity/][Source: https://www.v7labs.com/blog/impact-of-large-language-models-on-enterprise][Source: https://www.kellton.com/kellton-tech-blog/large-language-models-challenges-benefits].\n",
      "- Improved retrieval and grounded summarization via RAG: Retrieval-augmented generation (RAG) architectures that combine a retrieval layer (vector DBs or search) with generation produce more relevant and better-grounded answers than standalone generative prompting in many practical deployments, reducing hallucinations when retrieval quality and grounding controls are well-implemented [Source: https://k2view.com/blog/enterprise-llm][Source: https://kairntech.com/blog/articles/rag-production-the-complete-guide-to-building-and-deploying-retrieval-augmented-generation-applications/][Source: https://pmc.ncbi.nlm.nih.gov/articles/PMC12649634/].\n",
      "- Expanded NLP capabilities across functions: LLMs supply flexible intent detection, entity extraction, summarization, contract review aids, an...\n",
      "\n",
      "================================================================================\n",
      "CACHE PERFORMANCE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "=== Research Phase ===\n",
      "  Queries executed: 7\n",
      "\n",
      "=== Verification Phase (4 sprints) ===\n",
      "  Total queries: 24\n",
      "  Cache hits: 0 (0.0%)\n",
      "  Web searches: 24\n",
      "\n",
      "=== Overall Performance ===\n",
      "  Total queries: 31\n",
      "  Web searches avoided: 0 (0.0%)\n",
      "  Web searches executed: 34\n",
      "\n",
      "=== Layer Distribution ===\n",
      "  Layer 1 hits (exact): 0\n",
      "  Layer 2 HIGH conf: 0\n",
      "  Layer 2 MEDIUM conf: 3\n",
      "  Layer 2 LOW conf: 28\n",
      "  Layer 3 SUFFICIENT: 0\n",
      "  Layer 3 PARTIAL: 3\n",
      "  Layer 3 INSUFFICIENT: 0\n",
      "\n",
      "=== Verified Claims ===\n",
      "  Total claims verified: 24\n",
      "  Claims from cache: 0 (0.0%)\n",
      "\n",
      "================================================================================\n",
      "CACHE DECISION TRACE (first 20)\n",
      "================================================================================\n",
      " 1. [L2] SEARCH          | Conf: 0.00 | Low confidence (0.000), executed web search...\n",
      " 2. [L2] SEARCH          | Conf: 0.39 | Low confidence (0.385), executed web search...\n",
      " 3. [L2] SEARCH          | Conf: 0.28 | Low confidence (0.279), executed web search...\n",
      " 4. [L3] TARGETED_SEARCH | Conf: 0.40 | LLM identified gaps: ['Quantitative cost figu...\n",
      " 5. [L2] SEARCH          | Conf: 0.35 | Low confidence (0.355), executed web search...\n",
      " 6. [L3] TARGETED_SEARCH | Conf: 0.45 | LLM identified gaps: ['Customer support: miss...\n",
      " 7. [L2] SEARCH          | Conf: 0.32 | Low confidence (0.320), executed web search...\n",
      "\n",
      "Agent test PASSED\n"
     ]
    }
   ],
   "source": [
    "# Test with a question that naturally involves overlapping queries\n",
    "test_question = \"What are the key benefits and challenges of using large language models in enterprise applications?\"\n",
    "\n",
    "print(f\"Testing Knowledge Cache Agent with question:\\n{test_question}\\n\")\n",
    "print(\"Running cascaded cache research with multi-sprint verification...\")\n",
    "print(\"(This may take several minutes due to multiple verification sprints)\\n\")\n",
    "\n",
    "try:\n",
    "    result = await knowledge_cache_agent_async({\"question\": test_question})\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FINAL REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    print(result[\"output\"][:3000] + \"...\" if len(result[\"output\"]) > 3000 else result[\"output\"])\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CACHE PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    stats = result.get(\"cache_stats\", {})\n",
    "    total_queries = stats.get('total_queries', 0)\n",
    "    avoided = stats.get('web_searches_avoided', 0)\n",
    "    executed = stats.get('web_searches_executed', 0)\n",
    "\n",
    "    # Verification stats\n",
    "    num_sprints = result.get(\"verification_sprints\", 0)\n",
    "    verify_queries = result.get(\"total_verification_queries\", 0)\n",
    "    verify_hits = result.get(\"total_verification_cache_hits\", 0)\n",
    "    research_queries = total_queries - verify_queries\n",
    "\n",
    "    print(f\"\\n=== Research Phase ===\")\n",
    "    print(f\"  Queries executed: {research_queries}\")\n",
    "\n",
    "    print(f\"\\n=== Verification Phase ({num_sprints} sprints) ===\")\n",
    "    print(f\"  Total queries: {verify_queries}\")\n",
    "    print(f\"  Cache hits: {verify_hits} ({verify_hits/max(verify_queries,1)*100:.1f}%)\")\n",
    "    print(f\"  Web searches: {verify_queries - verify_hits}\")\n",
    "\n",
    "    print(f\"\\n=== Overall Performance ===\")\n",
    "    print(f\"  Total queries: {total_queries}\")\n",
    "    print(f\"  Web searches avoided: {avoided} ({avoided/max(total_queries,1)*100:.1f}%)\")\n",
    "    print(f\"  Web searches executed: {executed}\")\n",
    "\n",
    "    print(f\"\\n=== Layer Distribution ===\")\n",
    "    print(f\"  Layer 1 hits (exact): {stats.get('l1_hits', 0)}\")\n",
    "    print(f\"  Layer 2 HIGH conf: {stats.get('l2_high', 0)}\")\n",
    "    print(f\"  Layer 2 MEDIUM conf: {stats.get('l2_medium', 0)}\")\n",
    "    print(f\"  Layer 2 LOW conf: {stats.get('l2_low', 0)}\")\n",
    "    print(f\"  Layer 3 SUFFICIENT: {stats.get('l3_sufficient', 0)}\")\n",
    "    print(f\"  Layer 3 PARTIAL: {stats.get('l3_partial', 0)}\")\n",
    "    print(f\"  Layer 3 INSUFFICIENT: {stats.get('l3_insufficient', 0)}\")\n",
    "\n",
    "    # Verified claims summary\n",
    "    verified_claims = result.get(\"verified_claims\", [])\n",
    "    if verified_claims:\n",
    "        cache_hit_claims = sum(1 for c in verified_claims if c.get(\"cache_hit\"))\n",
    "        print(f\"\\n=== Verified Claims ===\")\n",
    "        print(f\"  Total claims verified: {len(verified_claims)}\")\n",
    "        print(f\"  Claims from cache: {cache_hit_claims} ({cache_hit_claims/len(verified_claims)*100:.1f}%)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CACHE DECISION TRACE (first 20)\")\n",
    "    print(\"=\" * 80)\n",
    "    for i, decision in enumerate(result.get(\"cache_decisions\", [])[:20], 1):\n",
    "        action = decision.get('action_taken', '?')\n",
    "        layer = decision.get('layer_reached', '?')\n",
    "        conf = decision.get('confidence_score', 0)\n",
    "        reasoning = decision.get('reasoning', '')[:45]\n",
    "        print(f\"{i:2}. [{layer}] {action:<15} | Conf: {conf:.2f} | {reasoning}...\")\n",
    "\n",
    "    print(\"\\nAgent test PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"Agent test FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Evaluation Harness Integration\n",
    "\n",
    "Once the manual test passes, uncomment and run the cells below for full evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation harness and metrics\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from evaluation import (\n",
    "    ExperimentHarness,\n",
    "    fact_recall,\n",
    "    citation_precision,\n",
    "    coherence_judge,\n",
    "    depth_judge,\n",
    "    relevance_judge,\n",
    "    minimum_sources_check\n",
    ")\n",
    "\n",
    "# Initialize harness with the golden test dataset\n",
    "harness = ExperimentHarness(\n",
    "    dataset_path=\"../data/deep_research_agent_test_dataset.yaml\",\n",
    "    langsmith_dataset_name=\"deep-research-golden-v2\"\n",
    ")\n",
    "\n",
    "print(\"Evaluation harness initialized successfully!\")\n",
    "print(f\"Dataset: {harness.dataset_path}\")\n",
    "print(f\"LangSmith dataset name: {harness.langsmith_dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Evaluation on All 20 Questions\n",
    "# EXPENSIVE - Only uncomment when ready for full evaluation\n",
    "# Uncomment to run:\n",
    "\n",
    "# evaluators = [\n",
    "#     fact_recall,\n",
    "#     citation_precision,\n",
    "#     minimum_sources_check,\n",
    "#     coherence_judge,\n",
    "#     depth_judge,\n",
    "#     relevance_judge,\n",
    "# ]\n",
    "#\n",
    "# print(\"Starting FULL evaluation on all 20 questions...\")\n",
    "# print(\"Knowledge Cache Agent - this will take 1-2 hours.\")\n",
    "# print(\"=\" * 80 + \"\\n\")\n",
    "#\n",
    "# results = harness.run_evaluation(\n",
    "#     agent_fn=knowledge_cache_agent,\n",
    "#     evaluators=evaluators,\n",
    "#     experiment_name=\"knowledge_cache_v1\",\n",
    "#     monte_carlo_runs=1,\n",
    "#     max_concurrency=2,\n",
    "#     description=\"Knowledge Cache paradigm evaluation on all difficulty tiers\"\n",
    "# )\n",
    "#\n",
    "# print(\"\\n\" + \"=\" * 80)\n",
    "# print(\"FULL EVALUATION RESULTS\")\n",
    "# print(\"=\" * 80)\n",
    "# print(f\"Experiment: {results.experiment_name}\")\n",
    "# print(f\"Questions evaluated: {results.num_questions}\")\n",
    "#\n",
    "# print(f\"\\n{'Metric':<30} {'Mean':<10}\")\n",
    "# print(\"-\" * 40)\n",
    "# for metric_name in sorted(results.metrics.keys()):\n",
    "#     if not metric_name.endswith('_std'):\n",
    "#         value = results.metrics.get(metric_name, 0)\n",
    "#         print(f\"{metric_name:<30} {value:<10.3f}\")\n",
    "\n",
    "print(\"Full evaluation cell ready. Uncomment to run when ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Benefits\n",
    "\n",
    "### Why Cascading Cache Works\n",
    "\n",
    "1. **Layer 1 (Deterministic)**: Perfect precision, negligible cost\n",
    "   - Catches exact query repetitions\n",
    "   - URL deduplication prevents re-fetching\n",
    "   - O(1) hash table lookups\n",
    "\n",
    "2. **Layer 2 (Semantic)**: High recall, low cost\n",
    "   - Vector similarity finds paraphrased queries\n",
    "   - Multi-signal confidence prevents false positives\n",
    "   - Adjustable thresholds for query specificity\n",
    "\n",
    "3. **Layer 3 (LLM Judgment)**: Handles ambiguity\n",
    "   - Only invoked for medium-confidence cases\n",
    "   - Gap analysis enables targeted search\n",
    "   - Human-like reasoning about sufficiency\n",
    "\n",
    "### Multi-Sprint Verification Loop (Agile Pattern)\n",
    "\n",
    "The agent now uses a cyclical verification pattern inspired by the Agile Sprints paradigm:\n",
    "\n",
    "```\n",
    "Research Phase:\n",
    "  plan_research  execute_searches  synthesize_draft\n",
    "\n",
    "Verification Sprint Loop (up to 3 sprints):\n",
    "  extract_claims  verify_claims  verification_retrospective\n",
    "                                           \n",
    "        (continue?) \n",
    "\n",
    "Final Phase:\n",
    "  revise_report  END\n",
    "```\n",
    "\n",
    "**Why this creates cache hits:**\n",
    "\n",
    "1. **Sprint 1**: Initial verification queries against research content\n",
    "   - Some overlap with original queries  early cache hits\n",
    "   - New information enters the cache\n",
    "\n",
    "2. **Sprint 2**: New claims from Sprint 1 verification\n",
    "   - Queries overlap with Sprint 1 findings  more cache hits\n",
    "   - Verification of verification creates compounding overlap\n",
    "\n",
    "3. **Sprint 3**: Final verification pass\n",
    "   - High cache hit rate expected (60%+)\n",
    "   - Most topics already explored\n",
    "\n",
    "### Expected Performance\n",
    "\n",
    "| Phase | Queries | Expected Cache Hits |\n",
    "|-------|---------|---------------------|\n",
    "| Research | 5-7 | 0% (cold cache) |\n",
    "| Verify Sprint 1 | 4 | 10-30% |\n",
    "| Verify Sprint 2 | 4 | 30-50% |\n",
    "| Verify Sprint 3 | 4 | 50-70% |\n",
    "| **Total** | **17-19** | **25-40%** |\n",
    "\n",
    "### Expected Benefits\n",
    "\n",
    "- **Latency Reduction**: Cache hits bypass network latency (500ms-3s per search)\n",
    "- **Cost Savings**: Each avoided API call saves ~$0.01-0.05\n",
    "- **Consistency**: Same data used throughout session\n",
    "- **Token Efficiency**: Reduced redundant content in context\n",
    "- **Quality**: Multiple verification passes improve accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
