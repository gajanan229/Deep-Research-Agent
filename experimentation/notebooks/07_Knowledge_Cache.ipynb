{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paradigm 07: Cascading Knowledge Cache Research Agent\n",
    "\n",
    "This notebook implements the **Cascading Knowledge Cache (CKC)** paradigm.\n",
    "\n",
    "## Core Concept\n",
    "\n",
    "The CKC architecture introduces an intelligent intermediary layer between search intent and search execution. Rather than treating each search as isolated, accumulated results become a growing internal knowledge base consulted before incurring the cost of external retrieval.\n",
    "\n",
    "**Fundamental Principle**: Never fetch what you already know, and use what you know to fetch smarter.\n",
    "\n",
    "## Key Principles\n",
    "\n",
    "1. **Fail Fast, Fail Cheap**: Cheapest operations execute first\n",
    "2. **Graduated Confidence**: HIGH/MEDIUM/LOW classifications\n",
    "3. **Specificity Awareness**: Query precision modulates thresholds\n",
    "4. **Temporal Intelligence**: Detect time-sensitive queries\n",
    "\n",
    "## Architecture\n",
    "\n",
    "- **Layer 1**: Deterministic Deduplication (exact matching)\n",
    "- **Layer 2**: Semantic Similarity Retrieval (vector search + confidence)\n",
    "- **Layer 3**: LLM-Augmented Judgment (gap analysis, query refinement)\n",
    "\n",
    "## Technology Stack\n",
    "\n",
    "- **LLM**: gpt-5-mini-2025-08-07\n",
    "- **Web Search**: Tavily API\n",
    "- **Embeddings**: OpenAI text-embedding-3-small\n",
    "- **Tracing**: LangSmith\n",
    "- **Framework**: LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configured successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import operator\n",
    "import asyncio\n",
    "import hashlib\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Annotated, TypedDict, Literal, Optional, Any\n",
    "from urllib.parse import urlparse\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from tavily import TavilyClient\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables\n",
    "env_path = Path(\"../.env\")\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Configure LangSmith tracing\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"deep_research_new\"\n",
    "\n",
    "print(\"Environment configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-5-mini-2025-08-07\n",
      "Embedding model: text-embedding-3-small\n",
      "Confidence thresholds: HIGH >= 0.75, LOW < 0.45\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM, Tavily, and Embeddings\n",
    "MODEL_NAME = \"gpt-5-mini-2025-08-07\"\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0, max_retries=10)\n",
    "tavily_client = TavilyClient()\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Cache Configuration\n",
    "CHUNK_SIZE = 500  # characters\n",
    "CHUNK_OVERLAP = 100  # characters\n",
    "HIGH_CONFIDENCE_THRESHOLD = 0.75\n",
    "LOW_CONFIDENCE_THRESHOLD = 0.45\n",
    "SPECIFICITY_ADJUSTMENT_FACTOR = 0.2  # How much specificity raises thresholds\n",
    "TOP_K_RETRIEVAL = 5\n",
    "\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "print(f\"Embedding model: text-embedding-3-small\")\n",
    "print(f\"Confidence thresholds: HIGH >= {HIGH_CONFIDENCE_THRESHOLD}, LOW < {LOW_CONFIDENCE_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedDocument(BaseModel):\n",
    "    \"\"\"A cached web document.\"\"\"\n",
    "    url: str = Field(description=\"Original URL\")\n",
    "    normalized_url: str = Field(description=\"Normalized URL for lookup\")\n",
    "    content: str = Field(description=\"Full text content\")\n",
    "    content_hash: str = Field(description=\"SHA-256 hash of content\")\n",
    "    title: str = Field(default=\"\", description=\"Page title\")\n",
    "    retrieval_timestamp: str = Field(description=\"When this was retrieved\")\n",
    "    source_query: str = Field(default=\"\", description=\"Query that led to this content\")\n",
    "\n",
    "\n",
    "class CachedChunk(BaseModel):\n",
    "    \"\"\"A chunk of content with embedding.\"\"\"\n",
    "    chunk_id: str = Field(description=\"Unique identifier\")\n",
    "    text: str = Field(description=\"Chunk text content\")\n",
    "    embedding: List[float] = Field(description=\"Vector embedding\")\n",
    "    source_url: str = Field(description=\"Source document URL\")\n",
    "    position: int = Field(description=\"Position within source document\")\n",
    "\n",
    "\n",
    "class QueryCacheEntry(BaseModel):\n",
    "    \"\"\"A cached query and its results.\"\"\"\n",
    "    original_query: str\n",
    "    light_normalized: str\n",
    "    aggressive_normalized: str\n",
    "    timestamp: str\n",
    "    result_urls: List[str]\n",
    "    result_summary: str\n",
    "\n",
    "\n",
    "class CacheDecision(BaseModel):\n",
    "    \"\"\"Record of a cache decision for observability.\"\"\"\n",
    "    query: str\n",
    "    layer_reached: Literal[\"L1\", \"L2\", \"L3\"]\n",
    "    decision: Literal[\"HIT\", \"HIGH_CONF\", \"MEDIUM_CONF\", \"LOW_CONF\",\n",
    "                      \"SUFFICIENT\", \"PARTIAL\", \"INSUFFICIENT\"]\n",
    "    confidence_score: float = 0.0\n",
    "    action_taken: Literal[\"USE_CACHE\", \"SEARCH\", \"TARGETED_SEARCH\"]\n",
    "    reasoning: str = \"\"\n",
    "    timestamp: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryAnalysis(BaseModel):\n",
    "    \"\"\"Analysis of a query's characteristics.\"\"\"\n",
    "    original_query: str\n",
    "    specificity_score: float = Field(description=\"0.0 (general) to 1.0 (very specific)\")\n",
    "    temporal_intent_score: float = Field(description=\"0.0 (no temporal) to 1.0 (time-sensitive)\")\n",
    "    adjusted_high_threshold: float\n",
    "    adjusted_low_threshold: float\n",
    "    extracted_entities: List[str] = Field(default_factory=list)\n",
    "    extracted_dates: List[str] = Field(default_factory=list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Knowledge Base Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base initialized\n"
     ]
    }
   ],
   "source": [
    "class KnowledgeBase:\n",
    "    \"\"\"Session-scoped knowledge base with cascading cache capabilities.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.url_registry: Dict[str, CachedDocument] = {}\n",
    "        self.query_cache: Dict[str, QueryCacheEntry] = {}\n",
    "        self.chunks: List[CachedChunk] = []\n",
    "        self.chunk_embeddings: Optional[np.ndarray] = None  # For fast similarity\n",
    "\n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            \"total_queries\": 0,\n",
    "            \"l1_hits\": 0,\n",
    "            \"l2_high\": 0,\n",
    "            \"l2_medium\": 0,\n",
    "            \"l2_low\": 0,\n",
    "            \"l3_sufficient\": 0,\n",
    "            \"l3_partial\": 0,\n",
    "            \"l3_insufficient\": 0,\n",
    "            \"web_searches_executed\": 0,\n",
    "            \"web_searches_avoided\": 0\n",
    "        }\n",
    "\n",
    "    # === URL Normalization ===\n",
    "    def normalize_url(self, url: str) -> str:\n",
    "        \"\"\"Normalize URL for consistent lookup.\"\"\"\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            # Lowercase host, remove www prefix\n",
    "            host = parsed.netloc.lower()\n",
    "            if host.startswith(\"www.\"):\n",
    "                host = host[4:]\n",
    "            # Remove trailing slash from path\n",
    "            path = parsed.path.rstrip(\"/\")\n",
    "            # Sort query parameters\n",
    "            query_params = sorted(parsed.query.split(\"&\")) if parsed.query else []\n",
    "            # Remove tracking params\n",
    "            tracking_params = {\"utm_source\", \"utm_medium\", \"utm_campaign\", \"ref\", \"fbclid\"}\n",
    "            query_params = [p for p in query_params if p.split(\"=\")[0] not in tracking_params]\n",
    "            query = \"&\".join(query_params)\n",
    "            # Reconstruct\n",
    "            normalized = f\"https://{host}{path}\"\n",
    "            if query:\n",
    "                normalized += f\"?{query}\"\n",
    "            return normalized\n",
    "        except:\n",
    "            return url.lower()\n",
    "\n",
    "    # === Query Normalization ===\n",
    "    def normalize_query_light(self, query: str) -> str:\n",
    "        \"\"\"Light normalization: lowercase, collapse whitespace.\"\"\"\n",
    "        return \" \".join(query.lower().split())\n",
    "\n",
    "    def normalize_query_aggressive(self, query: str) -> str:\n",
    "        \"\"\"Aggressive normalization: remove stop words, sort terms.\"\"\"\n",
    "        stop_words = {\"the\", \"a\", \"an\", \"is\", \"are\", \"of\", \"in\", \"to\", \"for\", \"and\", \"or\", \"what\", \"how\", \"why\", \"when\", \"where\"}\n",
    "        light = self.normalize_query_light(query)\n",
    "        terms = [t for t in light.split() if t not in stop_words and len(t) > 1]\n",
    "        return \" \".join(sorted(terms))\n",
    "\n",
    "    # === Content Hashing ===\n",
    "    def compute_content_hash(self, content: str) -> str:\n",
    "        \"\"\"Compute SHA-256 hash of content.\"\"\"\n",
    "        return hashlib.sha256(content.encode()).hexdigest()\n",
    "\n",
    "    # === Document Storage ===\n",
    "    def add_document(self, url: str, content: str, title: str = \"\", source_query: str = \"\"):\n",
    "        \"\"\"Add a document to the knowledge base.\"\"\"\n",
    "        normalized_url = self.normalize_url(url)\n",
    "\n",
    "        doc = CachedDocument(\n",
    "            url=url,\n",
    "            normalized_url=normalized_url,\n",
    "            content=content,\n",
    "            content_hash=self.compute_content_hash(content),\n",
    "            title=title,\n",
    "            retrieval_timestamp=datetime.now().isoformat(),\n",
    "            source_query=source_query\n",
    "        )\n",
    "\n",
    "        self.url_registry[normalized_url] = doc\n",
    "\n",
    "        # Chunk and embed the content\n",
    "        self._chunk_and_embed(doc)\n",
    "\n",
    "        return doc\n",
    "\n",
    "    def _chunk_and_embed(self, doc: CachedDocument):\n",
    "        \"\"\"Chunk document and compute embeddings.\"\"\"\n",
    "        content = doc.content\n",
    "        chunks_text = []\n",
    "\n",
    "        # Simple chunking with overlap\n",
    "        for i in range(0, len(content), CHUNK_SIZE - CHUNK_OVERLAP):\n",
    "            chunk_text = content[i:i + CHUNK_SIZE]\n",
    "            if len(chunk_text) > 50:  # Minimum chunk size\n",
    "                chunks_text.append(chunk_text)\n",
    "\n",
    "        if not chunks_text:\n",
    "            return\n",
    "\n",
    "        # Compute embeddings (batch)\n",
    "        embeddings = embeddings_model.embed_documents(chunks_text)\n",
    "\n",
    "        # Create chunk objects\n",
    "        for i, (text, embedding) in enumerate(zip(chunks_text, embeddings)):\n",
    "            chunk = CachedChunk(\n",
    "                chunk_id=f\"{doc.content_hash[:8]}_{i}\",\n",
    "                text=text,\n",
    "                embedding=embedding,\n",
    "                source_url=doc.url,\n",
    "                position=i\n",
    "            )\n",
    "            self.chunks.append(chunk)\n",
    "\n",
    "        # Update embedding matrix for fast similarity\n",
    "        self._update_embedding_matrix()\n",
    "\n",
    "    def _update_embedding_matrix(self):\n",
    "        \"\"\"Update the numpy matrix of embeddings for fast search.\"\"\"\n",
    "        if self.chunks:\n",
    "            self.chunk_embeddings = np.array([c.embedding for c in self.chunks])\n",
    "\n",
    "    # === Query Cache ===\n",
    "    def add_query(self, query: str, result_urls: List[str], result_summary: str):\n",
    "        \"\"\"Add a query to the cache.\"\"\"\n",
    "        entry = QueryCacheEntry(\n",
    "            original_query=query,\n",
    "            light_normalized=self.normalize_query_light(query),\n",
    "            aggressive_normalized=self.normalize_query_aggressive(query),\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            result_urls=result_urls,\n",
    "            result_summary=result_summary\n",
    "        )\n",
    "\n",
    "        # Store under both normalizations\n",
    "        self.query_cache[entry.light_normalized] = entry\n",
    "        self.query_cache[entry.aggressive_normalized] = entry\n",
    "\n",
    "        return entry\n",
    "\n",
    "    # === Lookups ===\n",
    "    def lookup_url(self, url: str) -> Optional[CachedDocument]:\n",
    "        \"\"\"Check if URL is already cached.\"\"\"\n",
    "        normalized = self.normalize_url(url)\n",
    "        return self.url_registry.get(normalized)\n",
    "\n",
    "    def lookup_query_exact(self, query: str) -> Optional[QueryCacheEntry]:\n",
    "        \"\"\"Check for exact query match (light normalization).\"\"\"\n",
    "        light = self.normalize_query_light(query)\n",
    "        return self.query_cache.get(light)\n",
    "\n",
    "    def lookup_query_aggressive(self, query: str) -> Optional[QueryCacheEntry]:\n",
    "        \"\"\"Check for bag-of-words query match (aggressive normalization).\"\"\"\n",
    "        aggressive = self.normalize_query_aggressive(query)\n",
    "        return self.query_cache.get(aggressive)\n",
    "\n",
    "    # === Semantic Search ===\n",
    "    def semantic_search(self, query: str, top_k: int = TOP_K_RETRIEVAL) -> List[Tuple[CachedChunk, float]]:\n",
    "        \"\"\"Find semantically similar chunks.\"\"\"\n",
    "        if not self.chunks or self.chunk_embeddings is None:\n",
    "            return []\n",
    "\n",
    "        # Embed query\n",
    "        query_embedding = np.array(embeddings_model.embed_query(query))\n",
    "\n",
    "        # Cosine similarity\n",
    "        similarities = np.dot(self.chunk_embeddings, query_embedding) / (\n",
    "            np.linalg.norm(self.chunk_embeddings, axis=1) * np.linalg.norm(query_embedding) + 1e-8\n",
    "        )\n",
    "\n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append((self.chunks[idx], float(similarities[idx])))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_stats_summary(self) -> str:\n",
    "        \"\"\"Get human-readable stats summary.\"\"\"\n",
    "        total = self.stats[\"total_queries\"]\n",
    "        if total == 0:\n",
    "            return \"No queries processed yet.\"\n",
    "\n",
    "        avoided = self.stats[\"web_searches_avoided\"]\n",
    "        executed = self.stats[\"web_searches_executed\"]\n",
    "        hit_rate = avoided / total * 100 if total > 0 else 0\n",
    "\n",
    "        return f\"\"\"\n",
    "Cache Statistics:\n",
    "- Total queries: {total}\n",
    "- Web searches avoided: {avoided} ({hit_rate:.1f}% hit rate)\n",
    "- Web searches executed: {executed}\n",
    "- Layer 1 hits: {self.stats['l1_hits']}\n",
    "- Layer 2 HIGH confidence: {self.stats['l2_high']}\n",
    "- Layer 2 MEDIUM confidence: {self.stats['l2_medium']}\n",
    "- Layer 2 LOW confidence: {self.stats['l2_low']}\n",
    "- Layer 3 SUFFICIENT: {self.stats['l3_sufficient']}\n",
    "- Layer 3 PARTIAL: {self.stats['l3_partial']}\n",
    "- Layer 3 INSUFFICIENT: {self.stats['l3_insufficient']}\n",
    "- Documents cached: {len(self.url_registry)}\n",
    "- Chunks indexed: {len(self.chunks)}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Initialize global knowledge base (session-scoped)\n",
    "knowledge_base = KnowledgeBase()\n",
    "print(\"Knowledge base initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_specificity(query: str) -> float:\n",
    "    \"\"\"Compute query specificity score (0.0 to 1.0).\"\"\"\n",
    "    score = 0.0\n",
    "\n",
    "    # Numerical precision: dates, percentages, specific numbers\n",
    "    if re.search(r'\\b\\d{4}\\b', query):  # Years\n",
    "        score += 0.2\n",
    "    if re.search(r'\\d+%|\\$\\d+|\\d+\\s*(billion|million|thousand)', query, re.IGNORECASE):\n",
    "        score += 0.2\n",
    "    if re.search(r'Q[1-4]\\s*\\d{4}|FY\\d{4}', query, re.IGNORECASE):  # Quarters, fiscal years\n",
    "        score += 0.2\n",
    "\n",
    "    # Proper nouns (capitalized words not at sentence start)\n",
    "    proper_nouns = re.findall(r'(?<!^)(?<!\\. )[A-Z][a-z]+', query)\n",
    "    if len(proper_nouns) > 1:\n",
    "        score += 0.15\n",
    "\n",
    "    # Quoted phrases\n",
    "    if '\"' in query or \"'\" in query:\n",
    "        score += 0.15\n",
    "\n",
    "    # Specific question words\n",
    "    if re.search(r'\\b(exact|precise|specific|exactly|how many|what is the)\\b', query, re.IGNORECASE):\n",
    "        score += 0.1\n",
    "\n",
    "    return min(score, 1.0)\n",
    "\n",
    "\n",
    "def compute_temporal_intent(query: str) -> float:\n",
    "    \"\"\"Compute temporal intent score (0.0 to 1.0).\"\"\"\n",
    "    score = 0.0\n",
    "\n",
    "    # Explicit currency terms\n",
    "    if re.search(r'\\b(current|latest|now|today|recent|present|this week|this month|this year)\\b', query, re.IGNORECASE):\n",
    "        score += 0.4\n",
    "\n",
    "    # Role/status queries\n",
    "    if re.search(r'\\b(who is the|what is the current|is .+ still)\\b', query, re.IGNORECASE):\n",
    "        score += 0.3\n",
    "\n",
    "    # Comparative present\n",
    "    if re.search(r'\\b(how has .+ changed|compared to|versus last)\\b', query, re.IGNORECASE):\n",
    "        score += 0.2\n",
    "\n",
    "    # Event-driven topics\n",
    "    if re.search(r'\\b(stock price|election|weather|score|breaking)\\b', query, re.IGNORECASE):\n",
    "        score += 0.3\n",
    "\n",
    "    return min(score, 1.0)\n",
    "\n",
    "\n",
    "def analyze_query(query: str) -> QueryAnalysis:\n",
    "    \"\"\"Perform full query analysis.\"\"\"\n",
    "    specificity = compute_specificity(query)\n",
    "    temporal_intent = compute_temporal_intent(query)\n",
    "\n",
    "    # Adjust thresholds based on specificity\n",
    "    # Higher specificity = higher threshold required\n",
    "    high_adjustment = specificity * SPECIFICITY_ADJUSTMENT_FACTOR\n",
    "    adjusted_high = min(HIGH_CONFIDENCE_THRESHOLD + high_adjustment, 0.95)\n",
    "    adjusted_low = LOW_CONFIDENCE_THRESHOLD  # Low threshold stays the same\n",
    "\n",
    "    # Extract entities and dates\n",
    "    entities = re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b', query)\n",
    "    dates = re.findall(r'\\b\\d{4}\\b|\\bQ[1-4]\\s*\\d{4}\\b|\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{4}\\b', query, re.IGNORECASE)\n",
    "\n",
    "    return QueryAnalysis(\n",
    "        original_query=query,\n",
    "        specificity_score=specificity,\n",
    "        temporal_intent_score=temporal_intent,\n",
    "        adjusted_high_threshold=adjusted_high,\n",
    "        adjusted_low_threshold=adjusted_low,\n",
    "        extracted_entities=entities[:10],\n",
    "        extracted_dates=dates[:5]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Layer 1: Deterministic Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_layer1(query: str, kb: KnowledgeBase) -> Tuple[Literal[\"HIT\", \"MISS\"], Optional[str], Optional[List[str]]]:\n",
    "    \"\"\"\n",
    "    Layer 1: Deterministic deduplication.\n",
    "\n",
    "    Returns: (decision, cached_summary, cached_urls)\n",
    "    \"\"\"\n",
    "    # Check exact query match (light normalization)\n",
    "    exact_match = kb.lookup_query_exact(query)\n",
    "    if exact_match:\n",
    "        return \"HIT\", exact_match.result_summary, exact_match.result_urls\n",
    "\n",
    "    # Check bag-of-words match (aggressive normalization)\n",
    "    aggressive_match = kb.lookup_query_aggressive(query)\n",
    "    if aggressive_match:\n",
    "        return \"HIT\", aggressive_match.result_summary, aggressive_match.result_urls\n",
    "\n",
    "    return \"MISS\", None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Layer 2: Semantic Retrieval and Confidence Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confidence(\n",
    "    top_results: List[Tuple[CachedChunk, float]],\n",
    "    query: str,\n",
    "    analysis: QueryAnalysis\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Compute multi-signal confidence score.\n",
    "\n",
    "    Returns: (confidence_score, signal_breakdown)\n",
    "    \"\"\"\n",
    "    if not top_results:\n",
    "        return 0.0, {}\n",
    "\n",
    "    # Signal 1: Top score magnitude\n",
    "    top_score = top_results[0][1]\n",
    "\n",
    "    # Signal 2: Score gap (discrimination)\n",
    "    if len(top_results) > 1:\n",
    "        score_gap = top_results[0][1] - top_results[1][1]\n",
    "    else:\n",
    "        score_gap = top_score\n",
    "\n",
    "    # Signal 3: Term overlap (Jaccard similarity)\n",
    "    query_terms = set(query.lower().split())\n",
    "    top_chunk_terms = set(top_results[0][0].text.lower().split())\n",
    "    term_overlap = len(query_terms & top_chunk_terms) / len(query_terms | top_chunk_terms) if query_terms | top_chunk_terms else 0\n",
    "\n",
    "    # Weighted combination\n",
    "    weights = {\n",
    "        \"top_score\": 0.5,\n",
    "        \"score_gap\": 0.25,\n",
    "        \"term_overlap\": 0.25\n",
    "    }\n",
    "\n",
    "    raw_confidence = (\n",
    "        weights[\"top_score\"] * top_score +\n",
    "        weights[\"score_gap\"] * min(score_gap * 2, 1.0) +  # Scale gap\n",
    "        weights[\"term_overlap\"] * term_overlap\n",
    "    )\n",
    "\n",
    "    # Apply temporal penalty if needed\n",
    "    temporal_penalty = 0.0  # Within-session, all content is recent\n",
    "\n",
    "    confidence = max(0.0, min(raw_confidence - temporal_penalty, 1.0))\n",
    "\n",
    "    signals = {\n",
    "        \"top_score\": top_score,\n",
    "        \"score_gap\": score_gap,\n",
    "        \"term_overlap\": term_overlap,\n",
    "        \"temporal_penalty\": temporal_penalty,\n",
    "        \"raw_confidence\": raw_confidence,\n",
    "        \"final_confidence\": confidence\n",
    "    }\n",
    "\n",
    "    return confidence, signals\n",
    "\n",
    "\n",
    "def check_layer2(\n",
    "    query: str,\n",
    "    kb: KnowledgeBase,\n",
    "    analysis: QueryAnalysis\n",
    ") -> Tuple[Literal[\"HIGH\", \"MEDIUM\", \"LOW\"], float, List[Tuple[CachedChunk, float]], Dict]:\n",
    "    \"\"\"\n",
    "    Layer 2: Semantic retrieval with confidence scoring.\n",
    "\n",
    "    Returns: (decision, confidence, retrieved_chunks, signals)\n",
    "    \"\"\"\n",
    "    # Perform semantic search\n",
    "    results = kb.semantic_search(query, top_k=TOP_K_RETRIEVAL)\n",
    "\n",
    "    if not results:\n",
    "        return \"LOW\", 0.0, [], {}\n",
    "\n",
    "    # Compute confidence\n",
    "    confidence, signals = compute_confidence(results, query, analysis)\n",
    "\n",
    "    # Classify based on adjusted thresholds\n",
    "    if confidence >= analysis.adjusted_high_threshold:\n",
    "        decision = \"HIGH\"\n",
    "    elif confidence >= analysis.adjusted_low_threshold:\n",
    "        decision = \"MEDIUM\"\n",
    "    else:\n",
    "        decision = \"LOW\"\n",
    "\n",
    "    return decision, confidence, results, signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Layer 3: LLM-Augmented Judgment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER3_JUDGMENT_PROMPT = \"\"\"You are evaluating whether cached content answers a research query.\n",
    "\n",
    "QUERY: {query}\n",
    "\n",
    "CACHED CONTENT (top {num_chunks} relevant chunks):\n",
    "{chunks_text}\n",
    "\n",
    "Analyze on these dimensions:\n",
    "1. TOPICAL RELEVANCE: Is the cached content about the same subject?\n",
    "2. SPECIFICITY MATCH: Does it address the specific aspect asked about?\n",
    "3. COMPLETENESS: Does it provide a complete or only partial answer?\n",
    "4. FACTUAL DENSITY: Does it contain concrete facts that answer the query?\n",
    "\n",
    "Provide your assessment in this exact format:\n",
    "VERDICT: [SUFFICIENT|PARTIAL|INSUFFICIENT]\n",
    "RELEVANCE: [0.0-1.0]\n",
    "COMPLETENESS: [0.0-1.0]\n",
    "REASONING: [Your explanation in 2-3 sentences]\n",
    "GAPS: [List specific missing information, or \"None\" if sufficient]\n",
    "\"\"\"\n",
    "\n",
    "GAP_ANALYSIS_PROMPT = \"\"\"Based on your analysis, the cached content only partially answers the query.\n",
    "\n",
    "QUERY: {query}\n",
    "IDENTIFIED GAPS: {gaps}\n",
    "\n",
    "Generate 1-2 highly targeted search queries that would fill these specific gaps.\n",
    "The refined queries should:\n",
    "- Target ONLY the missing information (not repeat what we already have)\n",
    "- Be specific and searchable\n",
    "- Different from the original query\n",
    "\n",
    "Return queries one per line, no numbering or bullets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer3Output(BaseModel):\n",
    "    \"\"\"Structured output from Layer 3 judgment.\"\"\"\n",
    "    verdict: Literal[\"SUFFICIENT\", \"PARTIAL\", \"INSUFFICIENT\"]\n",
    "    relevance: float\n",
    "    completeness: float\n",
    "    reasoning: str\n",
    "    gaps: List[str]\n",
    "\n",
    "\n",
    "async def check_layer3(\n",
    "    query: str,\n",
    "    chunks: List[Tuple[CachedChunk, float]],\n",
    "    kb: KnowledgeBase\n",
    ") -> Tuple[Literal[\"SUFFICIENT\", \"PARTIAL\", \"INSUFFICIENT\"], List[str], Optional[List[str]]]:\n",
    "    \"\"\"\n",
    "    Layer 3: LLM-augmented judgment with gap analysis.\n",
    "\n",
    "    Returns: (verdict, gaps, refined_queries)\n",
    "    \"\"\"\n",
    "    # Format chunks for LLM\n",
    "    chunks_text = \"\\n\\n---\\n\\n\".join([\n",
    "        f\"[Chunk {i+1}, similarity: {score:.3f}]\\n{chunk.text}\"\n",
    "        for i, (chunk, score) in enumerate(chunks[:5])\n",
    "    ])\n",
    "\n",
    "    prompt = LAYER3_JUDGMENT_PROMPT.format(\n",
    "        query=query,\n",
    "        num_chunks=min(len(chunks), 5),\n",
    "        chunks_text=chunks_text\n",
    "    )\n",
    "\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # Parse response\n",
    "    content = response.content\n",
    "\n",
    "    # Extract verdict\n",
    "    verdict_match = re.search(r'VERDICT:\\s*(SUFFICIENT|PARTIAL|INSUFFICIENT)', content, re.IGNORECASE)\n",
    "    verdict = verdict_match.group(1).upper() if verdict_match else \"INSUFFICIENT\"\n",
    "\n",
    "    # Extract gaps\n",
    "    gaps = []\n",
    "    gaps_match = re.search(r'GAPS:\\s*(.+?)(?=\\n\\n|$)', content, re.DOTALL | re.IGNORECASE)\n",
    "    if gaps_match:\n",
    "        gaps_text = gaps_match.group(1).strip()\n",
    "        if gaps_text.lower() != \"none\":\n",
    "            gaps = [g.strip() for g in re.split(r'[-\\u2022\\n]', gaps_text) if g.strip()]\n",
    "\n",
    "    # Generate refined queries if PARTIAL\n",
    "    refined_queries = None\n",
    "    if verdict == \"PARTIAL\" and gaps:\n",
    "        refine_prompt = GAP_ANALYSIS_PROMPT.format(\n",
    "            query=query,\n",
    "            gaps=\"\\n\".join(f\"- {g}\" for g in gaps)\n",
    "        )\n",
    "        refine_response = await llm.ainvoke([HumanMessage(content=refine_prompt)])\n",
    "        refined_queries = [q.strip() for q in refine_response.content.split(\"\\n\") if q.strip()][:2]\n",
    "\n",
    "    return verdict, gaps, refined_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cascaded Search Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web(query: str, max_results: int = 8) -> Tuple[str, List[str], List[str]]:\n",
    "    \"\"\"Execute web search using Tavily. Returns (summary, results, urls).\"\"\"\n",
    "    try:\n",
    "        if len(query) > 400:\n",
    "            query = query[:400]\n",
    "\n",
    "        response = tavily_client.search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            include_answer=True\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        urls = []\n",
    "        summary = response.get(\"answer\", \"\")\n",
    "\n",
    "        for r in response.get(\"results\", []):\n",
    "            url = r.get('url', '')\n",
    "            urls.append(url)\n",
    "            content = r.get('content', '')[:500]\n",
    "            title = r.get('title', 'No title')\n",
    "            results.append(f\"[{title}] {content}... (Source: {url})\")\n",
    "\n",
    "        return summary, results, urls\n",
    "    except Exception as e:\n",
    "        return f\"Search error: {str(e)}\", [], []\n",
    "\n",
    "\n",
    "async def cascaded_search(\n",
    "    query: str,\n",
    "    kb: KnowledgeBase\n",
    ") -> Tuple[str, List[str], CacheDecision]:\n",
    "    \"\"\"\n",
    "    Execute the full cascading cache check and search if needed.\n",
    "\n",
    "    Returns: (content, urls, decision_record)\n",
    "    \"\"\"\n",
    "    kb.stats[\"total_queries\"] += 1\n",
    "    timestamp = datetime.now().isoformat()\n",
    "\n",
    "    # === Query Analysis ===\n",
    "    analysis = analyze_query(query)\n",
    "\n",
    "    # === Layer 1: Deterministic Deduplication ===\n",
    "    l1_decision, l1_summary, l1_urls = check_layer1(query, kb)\n",
    "\n",
    "    if l1_decision == \"HIT\":\n",
    "        kb.stats[\"l1_hits\"] += 1\n",
    "        kb.stats[\"web_searches_avoided\"] += 1\n",
    "        decision = CacheDecision(\n",
    "            query=query,\n",
    "            layer_reached=\"L1\",\n",
    "            decision=\"HIT\",\n",
    "            confidence_score=1.0,\n",
    "            action_taken=\"USE_CACHE\",\n",
    "            reasoning=\"Exact query match found in cache\",\n",
    "            timestamp=timestamp\n",
    "        )\n",
    "        return l1_summary, l1_urls, decision\n",
    "\n",
    "    # === Layer 2: Semantic Retrieval ===\n",
    "    l2_decision, confidence, chunks, signals = check_layer2(query, kb, analysis)\n",
    "\n",
    "    if l2_decision == \"HIGH\":\n",
    "        kb.stats[\"l2_high\"] += 1\n",
    "        kb.stats[\"web_searches_avoided\"] += 1\n",
    "\n",
    "        # Compile content from chunks\n",
    "        content = \"\\n\\n\".join([f\"[From: {c.source_url}]\\n{c.text}\" for c, _ in chunks[:3]])\n",
    "        urls = list(set([c.source_url for c, _ in chunks]))\n",
    "\n",
    "        decision = CacheDecision(\n",
    "            query=query,\n",
    "            layer_reached=\"L2\",\n",
    "            decision=\"HIGH_CONF\",\n",
    "            confidence_score=confidence,\n",
    "            action_taken=\"USE_CACHE\",\n",
    "            reasoning=f\"High semantic similarity (conf={confidence:.3f})\",\n",
    "            timestamp=timestamp\n",
    "        )\n",
    "        return content, urls, decision\n",
    "\n",
    "    elif l2_decision == \"LOW\":\n",
    "        kb.stats[\"l2_low\"] += 1\n",
    "        # Skip Layer 3, go directly to search\n",
    "        summary, results, urls = search_web(query)\n",
    "\n",
    "        kb.stats[\"web_searches_executed\"] += 1\n",
    "\n",
    "        # Cache the results\n",
    "        combined_content = f\"{summary}\\n\\n\" + \"\\n\\n\".join(results)\n",
    "        for url in urls[:5]:  # Cache top 5 URLs\n",
    "            kb.add_document(url, combined_content, source_query=query)\n",
    "        kb.add_query(query, urls, summary)\n",
    "\n",
    "        decision = CacheDecision(\n",
    "            query=query,\n",
    "            layer_reached=\"L2\",\n",
    "            decision=\"LOW_CONF\",\n",
    "            confidence_score=confidence,\n",
    "            action_taken=\"SEARCH\",\n",
    "            reasoning=f\"Low confidence ({confidence:.3f}), executed web search\",\n",
    "            timestamp=timestamp\n",
    "        )\n",
    "        return combined_content, urls, decision\n",
    "\n",
    "    # === Layer 3: LLM Judgment (MEDIUM confidence) ===\n",
    "    kb.stats[\"l2_medium\"] += 1\n",
    "\n",
    "    verdict, gaps, refined_queries = await check_layer3(query, chunks, kb)\n",
    "\n",
    "    if verdict == \"SUFFICIENT\":\n",
    "        kb.stats[\"l3_sufficient\"] += 1\n",
    "        kb.stats[\"web_searches_avoided\"] += 1\n",
    "\n",
    "        content = \"\\n\\n\".join([f\"[From: {c.source_url}]\\n{c.text}\" for c, _ in chunks[:3]])\n",
    "        urls = list(set([c.source_url for c, _ in chunks]))\n",
    "\n",
    "        decision = CacheDecision(\n",
    "            query=query,\n",
    "            layer_reached=\"L3\",\n",
    "            decision=\"SUFFICIENT\",\n",
    "            confidence_score=confidence,\n",
    "            action_taken=\"USE_CACHE\",\n",
    "            reasoning=\"LLM judged cached content sufficient\",\n",
    "            timestamp=timestamp\n",
    "        )\n",
    "        return content, urls, decision\n",
    "\n",
    "    elif verdict == \"PARTIAL\" and refined_queries:\n",
    "        kb.stats[\"l3_partial\"] += 1\n",
    "\n",
    "        # Use cached content + targeted search for gaps\n",
    "        cached_content = \"\\n\\n\".join([f\"[Cached: {c.source_url}]\\n{c.text}\" for c, _ in chunks[:2]])\n",
    "        cached_urls = [c.source_url for c, _ in chunks[:2]]\n",
    "\n",
    "        # Execute refined searches\n",
    "        for refined_query in refined_queries:\n",
    "            summary, results, new_urls = search_web(refined_query, max_results=4)\n",
    "            kb.stats[\"web_searches_executed\"] += 1\n",
    "\n",
    "            combined = f\"{summary}\\n\\n\" + \"\\n\\n\".join(results)\n",
    "            cached_content += f\"\\n\\n[Gap-fill search: {refined_query}]\\n{combined}\"\n",
    "            cached_urls.extend(new_urls)\n",
    "\n",
    "            # Cache new results\n",
    "            for url in new_urls[:3]:\n",
    "                kb.add_document(url, combined, source_query=refined_query)\n",
    "            kb.add_query(refined_query, new_urls, summary)\n",
    "\n",
    "        decision = CacheDecision(\n",
    "            query=query,\n",
    "            layer_reached=\"L3\",\n",
    "            decision=\"PARTIAL\",\n",
    "            confidence_score=confidence,\n",
    "            action_taken=\"TARGETED_SEARCH\",\n",
    "            reasoning=f\"LLM identified gaps: {gaps[:2]}. Executed {len(refined_queries)} targeted searches.\",\n",
    "            timestamp=timestamp\n",
    "        )\n",
    "        return cached_content, list(set(cached_urls)), decision\n",
    "\n",
    "    else:\n",
    "        kb.stats[\"l3_insufficient\"] += 1\n",
    "\n",
    "        # Full search needed\n",
    "        summary, results, urls = search_web(query)\n",
    "        kb.stats[\"web_searches_executed\"] += 1\n",
    "\n",
    "        combined_content = f\"{summary}\\n\\n\" + \"\\n\\n\".join(results)\n",
    "        for url in urls[:5]:\n",
    "            kb.add_document(url, combined_content, source_query=query)\n",
    "        kb.add_query(query, urls, summary)\n",
    "\n",
    "        decision = CacheDecision(\n",
    "            query=query,\n",
    "            layer_reached=\"L3\",\n",
    "            decision=\"INSUFFICIENT\",\n",
    "            confidence_score=confidence,\n",
    "            action_taken=\"SEARCH\",\n",
    "            reasoning=\"LLM judged cached content insufficient\",\n",
    "            timestamp=timestamp\n",
    "        )\n",
    "        return combined_content, urls, decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. LangGraph State and Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeCacheState(TypedDict):\n",
    "    \"\"\"State for the Knowledge Cache Research Agent.\"\"\"\n",
    "    # Input\n",
    "    question: str\n",
    "\n",
    "    # Research planning\n",
    "    search_queries: List[str]\n",
    "\n",
    "    # Accumulated content\n",
    "    accumulated_content: Annotated[List[str], operator.add]\n",
    "    source_urls: Annotated[List[str], operator.add]\n",
    "\n",
    "    # Cache decisions (for observability)\n",
    "    cache_decisions: Annotated[List[dict], operator.add]\n",
    "\n",
    "    # Output\n",
    "    final_report: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECOMPOSE_PROMPT = \"\"\"You are a research planning expert. Decompose this research question into\n",
    "5-7 specific search queries that together will comprehensively answer the question.\n",
    "\n",
    "Research Question: {question}\n",
    "\n",
    "Requirements:\n",
    "- Each query should be independently searchable\n",
    "- Cover different aspects of the question\n",
    "- Include both broad and specific queries\n",
    "- Later queries may intentionally overlap with earlier ones (to test caching)\n",
    "\n",
    "Return ONLY the search queries, one per line.\n",
    "\"\"\"\n",
    "\n",
    "SYNTHESIS_PROMPT = \"\"\"You are a senior research analyst writing a comprehensive report.\n",
    "\n",
    "Research Question: {question}\n",
    "\n",
    "Accumulated Research Content:\n",
    "{content}\n",
    "\n",
    "Source URLs:\n",
    "{sources}\n",
    "\n",
    "Write a comprehensive research report (1000-1500 words) that:\n",
    "1. Directly answers the research question\n",
    "2. Synthesizes information from multiple sources\n",
    "3. Includes inline citations using [Source: URL] format\n",
    "4. Acknowledges any limitations or areas of uncertainty\n",
    "\n",
    "Structure the report with clear sections appropriate to the topic.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def plan_research(state: KnowledgeCacheState) -> dict:\n",
    "    \"\"\"Decompose the research question into search queries.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Phase 1: Planning Research\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    prompt = DECOMPOSE_PROMPT.format(question=question)\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    queries = [q.strip() for q in response.content.split(\"\\n\") if q.strip()][:7]\n",
    "\n",
    "    print(f\"  Generated {len(queries)} search queries\")\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        print(f\"    {i}. {q[:60]}...\")\n",
    "\n",
    "    return {\"search_queries\": queries}\n",
    "\n",
    "\n",
    "async def execute_searches(state: KnowledgeCacheState) -> dict:\n",
    "    \"\"\"Execute all searches through the cascading cache.\"\"\"\n",
    "    queries = state.get(\"search_queries\", [])\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Phase 2: Executing Cascaded Searches\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    all_content = []\n",
    "    all_urls = []\n",
    "    all_decisions = []\n",
    "\n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\n  [{i}/{len(queries)}] Query: {query[:50]}...\")\n",
    "\n",
    "        content, urls, decision = await cascaded_search(query, knowledge_base)\n",
    "\n",
    "        all_content.append(f\"### Query: {query}\\n\\n{content}\")\n",
    "        all_urls.extend(urls)\n",
    "        all_decisions.append(decision.model_dump())\n",
    "\n",
    "        # Log decision\n",
    "        action_symbol = {\n",
    "            \"USE_CACHE\": \"CACHE HIT\",\n",
    "            \"SEARCH\": \"WEB SEARCH\",\n",
    "            \"TARGETED_SEARCH\": \"TARGETED\"\n",
    "        }\n",
    "        print(f\"      {action_symbol.get(decision.action_taken, '?')} | \"\n",
    "              f\"Layer: {decision.layer_reached} | \"\n",
    "              f\"Conf: {decision.confidence_score:.2f}\")\n",
    "\n",
    "    print(f\"\\n  --- Search Phase Complete ---\")\n",
    "    print(knowledge_base.get_stats_summary())\n",
    "\n",
    "    return {\n",
    "        \"accumulated_content\": all_content,\n",
    "        \"source_urls\": all_urls,\n",
    "        \"cache_decisions\": all_decisions\n",
    "    }\n",
    "\n",
    "\n",
    "async def synthesize_report(state: KnowledgeCacheState) -> dict:\n",
    "    \"\"\"Synthesize final report from accumulated content.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    content = state.get(\"accumulated_content\", [])\n",
    "    urls = list(set(state.get(\"source_urls\", [])))\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Phase 3: Synthesizing Report\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    combined_content = \"\\n\\n---\\n\\n\".join(content)\n",
    "\n",
    "    # Token efficiency: limit content\n",
    "    if len(combined_content) > 15000:\n",
    "        combined_content = combined_content[:15000] + \"\\n\\n[... truncated ...]\"\n",
    "\n",
    "    prompt = SYNTHESIS_PROMPT.format(\n",
    "        question=question,\n",
    "        content=combined_content,\n",
    "        sources=\"\\n\".join(urls[:20])\n",
    "    )\n",
    "\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    print(f\"  Report generated: {len(response.content)} characters\")\n",
    "    print(f\"  Sources cited: {len(urls)}\")\n",
    "\n",
    "    return {\"final_report\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Cache Research Agent compiled successfully\n",
      "\n",
      "Key features:\n",
      "  - Layer 1: Deterministic deduplication (URL + query matching)\n",
      "  - Layer 2: Semantic retrieval with multi-signal confidence\n",
      "  - Layer 3: LLM judgment with gap analysis\n",
      "  - Session-scoped knowledge base\n",
      "  - Full observability of cache decisions\n"
     ]
    }
   ],
   "source": [
    "# Build the Knowledge Cache Research Agent graph\n",
    "kc_builder = StateGraph(KnowledgeCacheState)\n",
    "\n",
    "# Add nodes\n",
    "kc_builder.add_node(\"plan_research\", plan_research)\n",
    "kc_builder.add_node(\"execute_searches\", execute_searches)\n",
    "kc_builder.add_node(\"synthesize_report\", synthesize_report)\n",
    "\n",
    "# Add edges - linear flow\n",
    "kc_builder.add_edge(START, \"plan_research\")\n",
    "kc_builder.add_edge(\"plan_research\", \"execute_searches\")\n",
    "kc_builder.add_edge(\"execute_searches\", \"synthesize_report\")\n",
    "kc_builder.add_edge(\"synthesize_report\", END)\n",
    "\n",
    "# Compile\n",
    "knowledge_cache_graph = kc_builder.compile()\n",
    "\n",
    "print(\"Knowledge Cache Research Agent compiled successfully\")\n",
    "print(\"\\nKey features:\")\n",
    "print(\"  - Layer 1: Deterministic deduplication (URL + query matching)\")\n",
    "print(\"  - Layer 2: Semantic retrieval with multi-signal confidence\")\n",
    "print(\"  - Layer 3: LLM judgment with gap analysis\")\n",
    "print(\"  - Session-scoped knowledge base\")\n",
    "print(\"  - Full observability of cache decisions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(knowledge_cache_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Agent Wrapper for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def knowledge_cache_agent_async(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Async version of the Knowledge Cache research agent.\n",
    "    Use this version when calling from Jupyter notebooks.\n",
    "    \"\"\"\n",
    "    # Reset knowledge base for fresh session\n",
    "    global knowledge_base\n",
    "    knowledge_base = KnowledgeBase()\n",
    "\n",
    "    question = inputs.get(\"question\", \"\")\n",
    "\n",
    "    result = await knowledge_cache_graph.ainvoke(\n",
    "        {\"question\": question},\n",
    "        config={\"recursion_limit\": 50}\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"output\": result.get(\"final_report\", \"\"),\n",
    "        \"source_urls\": list(set(result.get(\"source_urls\", []))),\n",
    "        \"cache_decisions\": result.get(\"cache_decisions\", []),\n",
    "        \"cache_stats\": knowledge_base.stats.copy()\n",
    "    }\n",
    "\n",
    "\n",
    "def knowledge_cache_agent(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Sync wrapper function for Knowledge Cache research agent.\n",
    "\n",
    "    Compatible with evaluation harness.\n",
    "    \"\"\"\n",
    "    question = inputs.get(\"question\", \"\")\n",
    "\n",
    "    async def _execute():\n",
    "        global knowledge_base\n",
    "        knowledge_base = KnowledgeBase()\n",
    "\n",
    "        return await knowledge_cache_graph.ainvoke(\n",
    "            {\"question\": question},\n",
    "            config={\"recursion_limit\": 50}\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        import concurrent.futures\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future = executor.submit(asyncio.run, _execute())\n",
    "            result = future.result()\n",
    "    except RuntimeError:\n",
    "        result = asyncio.run(_execute())\n",
    "\n",
    "    return {\n",
    "        \"output\": result.get(\"final_report\", \"\"),\n",
    "        \"source_urls\": list(set(result.get(\"source_urls\", []))),\n",
    "        \"cache_decisions\": result.get(\"cache_decisions\", []),\n",
    "        \"cache_stats\": knowledge_base.stats.copy()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Manual Test\n",
    "\n",
    "Run this cell to verify the agent works correctly with a simple test question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a question that naturally involves overlapping queries\n",
    "test_question = \"What are the key benefits and challenges of using large language models in enterprise applications?\"\n",
    "\n",
    "print(f\"Testing Knowledge Cache Agent with question:\\n{test_question}\\n\")\n",
    "print(\"Running cascaded cache research (this may take several minutes)...\\n\")\n",
    "\n",
    "try:\n",
    "    result = await knowledge_cache_agent_async({\"question\": test_question})\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FINAL REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    print(result[\"output\"][:3000] + \"...\" if len(result[\"output\"]) > 3000 else result[\"output\"])\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CACHE PERFORMANCE\")\n",
    "    print(\"=\" * 80)\n",
    "    stats = result.get(\"cache_stats\", {})\n",
    "    print(f\"Total queries processed: {stats.get('total_queries', 0)}\")\n",
    "    print(f\"Web searches avoided: {stats.get('web_searches_avoided', 0)}\")\n",
    "    print(f\"Web searches executed: {stats.get('web_searches_executed', 0)}\")\n",
    "    hit_rate = (stats.get('web_searches_avoided', 0) / max(stats.get('total_queries', 1), 1)) * 100\n",
    "    print(f\"Cache hit rate: {hit_rate:.1f}%\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CACHE DECISION TRACE\")\n",
    "    print(\"=\" * 80)\n",
    "    for i, decision in enumerate(result.get(\"cache_decisions\", [])[:10], 1):\n",
    "        print(f\"{i}. [{decision.get('layer_reached')}] {decision.get('action_taken')} - {decision.get('reasoning', '')[:60]}...\")\n",
    "\n",
    "    print(\"\\nAgent test PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"Agent test FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Evaluation Harness Integration\n",
    "\n",
    "Once the manual test passes, uncomment and run the cells below for full evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation harness and metrics\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from evaluation import (\n",
    "    ExperimentHarness,\n",
    "    fact_recall,\n",
    "    citation_precision,\n",
    "    coherence_judge,\n",
    "    depth_judge,\n",
    "    relevance_judge,\n",
    "    minimum_sources_check\n",
    ")\n",
    "\n",
    "# Initialize harness with the golden test dataset\n",
    "harness = ExperimentHarness(\n",
    "    dataset_path=\"../data/deep_research_agent_test_dataset.yaml\",\n",
    "    langsmith_dataset_name=\"deep-research-golden-v2\"\n",
    ")\n",
    "\n",
    "print(\"Evaluation harness initialized successfully!\")\n",
    "print(f\"Dataset: {harness.dataset_path}\")\n",
    "print(f\"LangSmith dataset name: {harness.langsmith_dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Evaluation on All 20 Questions\n",
    "# EXPENSIVE - Only uncomment when ready for full evaluation\n",
    "# Uncomment to run:\n",
    "\n",
    "# evaluators = [\n",
    "#     fact_recall,\n",
    "#     citation_precision,\n",
    "#     minimum_sources_check,\n",
    "#     coherence_judge,\n",
    "#     depth_judge,\n",
    "#     relevance_judge,\n",
    "# ]\n",
    "#\n",
    "# print(\"Starting FULL evaluation on all 20 questions...\")\n",
    "# print(\"Knowledge Cache Agent - this will take 1-2 hours.\")\n",
    "# print(\"=\" * 80 + \"\\n\")\n",
    "#\n",
    "# results = harness.run_evaluation(\n",
    "#     agent_fn=knowledge_cache_agent,\n",
    "#     evaluators=evaluators,\n",
    "#     experiment_name=\"knowledge_cache_v1\",\n",
    "#     monte_carlo_runs=1,\n",
    "#     max_concurrency=2,\n",
    "#     description=\"Knowledge Cache paradigm evaluation on all difficulty tiers\"\n",
    "# )\n",
    "#\n",
    "# print(\"\\n\" + \"=\" * 80)\n",
    "# print(\"FULL EVALUATION RESULTS\")\n",
    "# print(\"=\" * 80)\n",
    "# print(f\"Experiment: {results.experiment_name}\")\n",
    "# print(f\"Questions evaluated: {results.num_questions}\")\n",
    "#\n",
    "# print(f\"\\n{'Metric':<30} {'Mean':<10}\")\n",
    "# print(\"-\" * 40)\n",
    "# for metric_name in sorted(results.metrics.keys()):\n",
    "#     if not metric_name.endswith('_std'):\n",
    "#         value = results.metrics.get(metric_name, 0)\n",
    "#         print(f\"{metric_name:<30} {value:<10.3f}\")\n",
    "\n",
    "print(\"Full evaluation cell ready. Uncomment to run when ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Benefits\n",
    "\n",
    "### Why Cascading Cache Works\n",
    "\n",
    "1. **Layer 1 (Deterministic)**: Perfect precision, negligible cost\n",
    "   - Catches exact query repetitions\n",
    "   - URL deduplication prevents re-fetching\n",
    "   - O(1) hash table lookups\n",
    "\n",
    "2. **Layer 2 (Semantic)**: High recall, low cost\n",
    "   - Vector similarity finds paraphrased queries\n",
    "   - Multi-signal confidence prevents false positives\n",
    "   - Adjustable thresholds for query specificity\n",
    "\n",
    "3. **Layer 3 (LLM Judgment)**: Handles ambiguity\n",
    "   - Only invoked for medium-confidence cases\n",
    "   - Gap analysis enables targeted search\n",
    "   - Human-like reasoning about sufficiency\n",
    "\n",
    "### Expected Benefits\n",
    "\n",
    "- **Latency Reduction**: Cache hits bypass network latency (500ms-3s per search)\n",
    "- **Cost Savings**: Each avoided API call saves ~$0.01-0.05\n",
    "- **Consistency**: Same data used throughout session\n",
    "- **Token Efficiency**: Reduced redundant content in context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
