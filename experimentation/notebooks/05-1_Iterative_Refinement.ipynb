{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paradigm 05-1: Patch-Based Iterative Refinement Research Agent\n",
    "\n",
    "This notebook implements the **Patch-Based Iterative Refinement Architecture** for deep research report generation.\n",
    "\n",
    "## Core Concept\n",
    "\n",
    "Unlike traditional Generate-Critique-Revise loops that regenerate the entire document, this architecture:\n",
    "\n",
    "1. **Decomposes documents into semantic skeleton nodes** with stable identifiers\n",
    "2. **Generates prose in patches** (per-node expansion) to overcome output token limits\n",
    "3. **Maintains a claims registry** tracking assertions and their verification status\n",
    "4. **Applies targeted patches** to specific nodes rather than regenerating everything\n",
    "5. **Uses bridge sentences** for coherent transitions between sections\n",
    "6. **Detects cascades** when changes in one section affect dependent sections\n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "- **Longer documents**: No output token ceiling, documents can be arbitrarily long\n",
    "- **Token efficiency**: Only patch sections with issues, not the entire document\n",
    "- **Better coherence**: Explicit dependency tracking and bridge management\n",
    "- **Transparency**: Claims registry shows exactly what's verified vs unverified\n",
    "\n",
    "## Architecture Phases\n",
    "\n",
    "1. **Skeleton Generation**: Create hierarchical document structure\n",
    "2. **Node Expansion**: Generate prose for each leaf node\n",
    "3. **Claim Extraction**: Build registry of assertions\n",
    "4. **Structured Critique**: Multi-level analysis producing Noise Map\n",
    "5. **Targeted Retrieval**: Search for evidence for weak claims\n",
    "6. **Patch Application**: Fix specific nodes, detect cascades\n",
    "7. **Convergence Check**: Iterate until quality threshold met\n",
    "\n",
    "## Technology Stack\n",
    "\n",
    "- **LLM**: `gpt-5-mini-2025-08-07`\n",
    "- **Web Search**: Tavily API\n",
    "- **Tracing**: LangSmith\n",
    "- **Framework**: LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configured successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import operator\n",
    "import asyncio\n",
    "import re\n",
    "import json\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Annotated, TypedDict, Literal, Any\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from tavily import TavilyClient\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Load environment variables\n",
    "env_path = Path(\"../.env\")\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Configure LangSmith tracing\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"deep_research_new\"\n",
    "\n",
    "print(\"Environment configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-5-mini-2025-08-07\n",
      "Max iterations: 3\n",
      "Quality threshold: 7.5/10\n",
      "Target words per node: 300\n",
      "LLM configured with max_retries=5, timeout=120s\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM and Tavily client\n",
    "MODEL_NAME = \"gpt-5-mini-2025-08-07\"\n",
    "\n",
    "# Configure LLM with retry settings\n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL_NAME, \n",
    "    temperature=0,\n",
    "    max_retries=10  # Retry up to 5 times\n",
    ")\n",
    "\n",
    "tavily_client = TavilyClient()\n",
    "\n",
    "# Architecture Configuration\n",
    "MAX_ITERATIONS = 1          # Maximum critique-patch cycles\n",
    "QUALITY_THRESHOLD = 7.5     # Stop if quality exceeds this (1-10 scale)\n",
    "MAX_CASCADES_PER_ITER = 5   # Limit cascade propagation per iteration\n",
    "TARGET_WORDS_PER_NODE = 300 # Approximate words per leaf node\n",
    "\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "print(f\"Max iterations: {MAX_ITERATIONS}\")\n",
    "print(f\"Quality threshold: {QUALITY_THRESHOLD}/10\")\n",
    "print(f\"Target words per node: {TARGET_WORDS_PER_NODE}\")\n",
    "print(f\"LLM configured with max_retries=5, timeout=120s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Models\n",
    "\n",
    "### 2.1 Skeleton Structure\n",
    "The document is organized as a hierarchical tree with stable semantic identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkeletonNode(BaseModel):\n",
    "    \"\"\"A node in the document skeleton hierarchy.\"\"\"\n",
    "    node_id: str = Field(description=\"Unique identifier like 'sec:intro' or 'sec:methods:data_collection'\")\n",
    "    title: str = Field(description=\"Section title for the final document\")\n",
    "    intent: str = Field(description=\"1-3 sentence description of what this section should accomplish\")\n",
    "    target_word_count: int = Field(default=300, description=\"Approximate target length\")\n",
    "    dependencies: List[str] = Field(default_factory=list, description=\"Node IDs this section depends on\")\n",
    "    children: List[str] = Field(default_factory=list, description=\"Child node IDs (empty for leaf nodes)\")\n",
    "    is_expanded: bool = Field(default=False, description=\"Whether prose has been generated\")\n",
    "\n",
    "\n",
    "class DocumentSkeleton(BaseModel):\n",
    "    \"\"\"The complete document skeleton structure.\"\"\"\n",
    "    thesis: str = Field(description=\"One-sentence statement of the document's central purpose\")\n",
    "    root_nodes: List[str] = Field(description=\"Top-level section node IDs in document order\")\n",
    "    nodes: Dict[str, SkeletonNode] = Field(default_factory=dict, description=\"All nodes by ID\")\n",
    "    style_constraints: str = Field(default=\"\", description=\"Global style guidelines\")\n",
    "\n",
    "\n",
    "# For LLM structured output\n",
    "class SkeletonGenerationOutput(BaseModel):\n",
    "    \"\"\"Output schema for skeleton generation.\"\"\"\n",
    "    thesis: str = Field(description=\"One-sentence thesis statement\")\n",
    "    sections: List[SkeletonNode] = Field(description=\"All sections in document order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prose Store\n",
    "Content for each node, including bridge sentences for transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProseEntry(BaseModel):\n",
    "    \"\"\"Content stored for each expanded node.\"\"\"\n",
    "    node_id: str = Field(description=\"The skeleton node this prose belongs to\")\n",
    "    main_content: str = Field(description=\"The substantive prose for this section\")\n",
    "    bridge_in: str = Field(default=\"\", description=\"Transitional sentences connecting from previous section\")\n",
    "    bridge_out: str = Field(default=\"\", description=\"Transitional sentences leading to next section\")\n",
    "    summary: str = Field(default=\"\", description=\"1-2 sentence compression of content\")\n",
    "    revision_count: int = Field(default=0, description=\"How many times this node has been revised\")\n",
    "    previous_versions: List[str] = Field(default_factory=list, description=\"Archive of previous content\")\n",
    "\n",
    "\n",
    "# For LLM structured output when generating prose\n",
    "class ProseGenerationOutput(BaseModel):\n",
    "    \"\"\"Output schema for prose generation.\"\"\"\n",
    "    bridge_in: str = Field(description=\"1-2 transitional sentences connecting from previous section\")\n",
    "    main_content: str = Field(description=\"The main prose content for this section\")\n",
    "    bridge_out: str = Field(description=\"1-2 transitional sentences leading to next section\")\n",
    "    summary: str = Field(description=\"1-2 sentence summary of what this section establishes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Claims Registry\n",
    "Tracks every substantive assertion with verification status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Claim(BaseModel):\n",
    "    \"\"\"A verifiable assertion in the document.\"\"\"\n",
    "    claim_id: str = Field(description=\"Unique identifier for this claim\")\n",
    "    claim_text: str = Field(description=\"The assertion itself, stated precisely\")\n",
    "    source_node: str = Field(description=\"Skeleton node ID where this claim appears\")\n",
    "    verification_status: Literal[\"unverified\", \"verified\", \"contested\", \"retracted\"] = Field(\n",
    "        default=\"unverified\", description=\"Current verification state\"\n",
    "    )\n",
    "    supporting_evidence: List[str] = Field(default_factory=list, description=\"Sources supporting this claim\")\n",
    "    claim_dependencies: List[str] = Field(default_factory=list, description=\"Other claim IDs this depends on\")\n",
    "\n",
    "\n",
    "# For LLM structured output when extracting claims\n",
    "class ClaimExtractionOutput(BaseModel):\n",
    "    \"\"\"Output schema for claim extraction.\"\"\"\n",
    "    claims: List[Claim] = Field(description=\"All factual claims extracted from the prose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Critique and Noise Map\n",
    "Structured feedback targeting specific issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CritiqueIssue(BaseModel):\n",
    "    \"\"\"An issue identified during critique.\"\"\"\n",
    "    issue_id: str = Field(description=\"Unique identifier\")\n",
    "    scope: Literal[\"global\", \"section\", \"transition\"] = Field(description=\"Level of the issue\")\n",
    "    target_nodes: List[str] = Field(description=\"Affected skeleton node IDs\")\n",
    "    issue_type: Literal[\"weak_claim\", \"missing_evidence\", \"logical_gap\", \"unclear\", \"coherence\", \"depth\", \"transition\"] = Field(\n",
    "        description=\"Category of issue\"\n",
    "    )\n",
    "    severity: Literal[\"critical\", \"major\", \"minor\"] = Field(description=\"How serious the issue is\")\n",
    "    affected_claims: List[str] = Field(default_factory=list, description=\"Claim IDs affected\")\n",
    "    description: str = Field(description=\"What the problem is\")\n",
    "    suggestion: str = Field(description=\"How to fix it\")\n",
    "    search_query: str = Field(default=\"\", description=\"Specific query to find evidence (for evidence issues)\")\n",
    "\n",
    "\n",
    "class CritiqueResult(BaseModel):\n",
    "    \"\"\"Complete critique output.\"\"\"\n",
    "    overall_quality: float = Field(description=\"Quality score 1-10\")\n",
    "    issues: List[CritiqueIssue] = Field(default_factory=list, description=\"All identified issues\")\n",
    "    strengths: str = Field(default=\"\", description=\"What the document does well\")\n",
    "    summary: str = Field(description=\"Overall assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LangGraph State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchBasedRefinementState(TypedDict):\n",
    "    \"\"\"State for the Patch-Based Iterative Refinement Agent.\"\"\"\n",
    "    # Input\n",
    "    question: str\n",
    "    \n",
    "    # Document Structure (stored as dicts for JSON serialization)\n",
    "    skeleton: Dict[str, Any]              # DocumentSkeleton as dict\n",
    "    prose_store: Dict[str, Dict[str, Any]]  # node_id -> ProseEntry as dict\n",
    "    claims_registry: Dict[str, Dict[str, Any]]  # claim_id -> Claim as dict\n",
    "    \n",
    "    # Research & Evidence (accumulated)\n",
    "    search_results: Annotated[List[str], operator.add]\n",
    "    source_urls: Annotated[List[str], operator.add]\n",
    "    \n",
    "    # Critique & Patching\n",
    "    noise_map: List[Dict[str, Any]]       # List of CritiqueIssue as dicts\n",
    "    nodes_to_patch: List[str]             # Node IDs needing revision\n",
    "    cascade_queue: List[str]              # Nodes needing re-evaluation due to upstream changes\n",
    "    targeted_evidence: Dict[str, List[str]]  # node_id -> evidence for that node\n",
    "    \n",
    "    # Iteration Tracking\n",
    "    iteration_count: int\n",
    "    quality_scores: Annotated[List[float], operator.add]\n",
    "    \n",
    "    # Logging (accumulated)\n",
    "    iteration_log: Annotated[List[str], operator.add]\n",
    "    \n",
    "    # Output\n",
    "    final_report: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web(query: str, max_results: int = 8) -> tuple[List[str], List[str]]:\n",
    "    \"\"\"Execute web search using Tavily. Returns (results, urls).\"\"\"\n",
    "    try:\n",
    "        if len(query) > 400:\n",
    "            query = query[:400]\n",
    "\n",
    "        response = tavily_client.search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            include_answer=True\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        urls = []\n",
    "\n",
    "        if response.get(\"answer\"):\n",
    "            results.append(f\"Summary: {response['answer']}\")\n",
    "\n",
    "        for r in response.get(\"results\", []):\n",
    "            url = r.get('url', '')\n",
    "            urls.append(url)\n",
    "            results.append(f\"- {r.get('title', 'No title')}: {r.get('content', '')[:500]}... (Source: {url})\")\n",
    "\n",
    "        return results, urls\n",
    "    except Exception as e:\n",
    "        return [f\"Search error: {str(e)}\"], []\n",
    "\n",
    "\n",
    "def get_leaf_nodes(skeleton: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"Get all leaf node IDs (nodes without children) in document order.\"\"\"\n",
    "    nodes = skeleton.get(\"nodes\", {})\n",
    "    root_nodes = skeleton.get(\"root_nodes\", [])\n",
    "\n",
    "    def collect_leaves(node_ids: List[str]) -> List[str]:\n",
    "        leaves = []\n",
    "        for nid in node_ids:\n",
    "            node = nodes.get(nid, {})\n",
    "            children = node.get(\"children\", [])\n",
    "            if not children:\n",
    "                leaves.append(nid)\n",
    "            else:\n",
    "                leaves.extend(collect_leaves(children))\n",
    "        return leaves\n",
    "\n",
    "    return collect_leaves(root_nodes)\n",
    "\n",
    "\n",
    "def topological_sort_nodes(skeleton: Dict[str, Any], node_ids: List[str]) -> List[str]:\n",
    "    \"\"\"Sort nodes by dependency order (dependencies before dependents).\n",
    "    \n",
    "    Only considers dependencies that are in the node_ids list - external\n",
    "    dependencies are treated as already satisfied.\n",
    "    \"\"\"\n",
    "    nodes = skeleton.get(\"nodes\", {})\n",
    "    node_id_set = set(node_ids)\n",
    "\n",
    "    # Build dependency graph (only internal dependencies)\n",
    "    remaining = set(node_ids)\n",
    "    sorted_nodes = []\n",
    "\n",
    "    while remaining:\n",
    "        # Find nodes whose internal dependencies are all satisfied\n",
    "        ready = []\n",
    "        for nid in remaining:\n",
    "            node = nodes.get(nid, {})\n",
    "            deps = set(node.get(\"dependencies\", []))\n",
    "            # Only consider dependencies that are in our node_ids list\n",
    "            internal_deps = deps & node_id_set\n",
    "            # Check if all internal dependencies are already processed\n",
    "            if internal_deps.issubset(set(sorted_nodes)):\n",
    "                ready.append(nid)\n",
    "\n",
    "        if not ready:\n",
    "            # Circular dependency detected - add remaining in document order\n",
    "            # This prevents infinite loops\n",
    "            print(f\"  WARNING: Circular dependency detected among nodes: {remaining}\")\n",
    "            for nid in node_ids:\n",
    "                if nid in remaining:\n",
    "                    sorted_nodes.append(nid)\n",
    "            break\n",
    "\n",
    "        # Add ready nodes in document order (preserve original ordering)\n",
    "        ready_ordered = [nid for nid in node_ids if nid in ready]\n",
    "        sorted_nodes.extend(ready_ordered)\n",
    "        remaining -= set(ready_ordered)\n",
    "\n",
    "    return sorted_nodes\n",
    "\n",
    "\n",
    "def get_adjacent_nodes(skeleton: Dict[str, Any], node_id: str) -> tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"Get the previous and next node IDs in document order.\"\"\"\n",
    "    leaves = get_leaf_nodes(skeleton)\n",
    "    try:\n",
    "        idx = leaves.index(node_id)\n",
    "        prev_node = leaves[idx - 1] if idx > 0 else None\n",
    "        next_node = leaves[idx + 1] if idx < len(leaves) - 1 else None\n",
    "        return prev_node, next_node\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "async def extract_claims_for_node(\n",
    "    node_id: str,\n",
    "    prose_entry: Dict[str, Any],\n",
    "    existing_claims: Dict[str, Dict[str, Any]]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract claims from a single node's prose, with awareness of existing claims.\n",
    "    \n",
    "    This function is used both during initial claim extraction and after patching\n",
    "    to update the claims registry.\n",
    "    \"\"\"\n",
    "    full_prose = f\"{prose_entry.get('bridge_in', '')}\\n{prose_entry.get('main_content', '')}\\n{prose_entry.get('bridge_out', '')}\"\n",
    "    \n",
    "    # Build existing claims context for dependency detection\n",
    "    existing_claims_context = \"\\n\".join([\n",
    "        f\"- {c['claim_id']}: {c['claim_text'][:100]}\"\n",
    "        for c in list(existing_claims.values())[:40]\n",
    "        if c.get('source_node') != node_id  # Don't include claims from the same node\n",
    "    ])\n",
    "    \n",
    "    # Note: Using {{ and }} to escape braces in f-string so they appear as literal { } in output\n",
    "    enhanced_prompt = f\"\"\"Extract all factual claims from this research prose.\n",
    "\n",
    "Section Node ID: {node_id}\n",
    "\n",
    "Prose Content:\n",
    "{full_prose}\n",
    "\n",
    "EXISTING CLAIMS (from other sections - reference these by ID for dependencies):\n",
    "{existing_claims_context if existing_claims_context else \"(No existing claims yet)\"}\n",
    "\n",
    "A claim is any statement that:\n",
    "- Has a truth value (could be verified or refuted through evidence)\n",
    "- Contributes to the document's argument\n",
    "- Is NOT just a structural statement about the document itself\n",
    "\n",
    "For each claim, identify:\n",
    "- claim_id: Unique ID using format: claim_{{node_id}}_{{number}} (e.g., claim_sec:intro_1)\n",
    "- claim_text: The precise assertion, stated as a single declarative sentence\n",
    "- source_node: \"{node_id}\"\n",
    "- verification_status: \"verified\" if it cites a specific source, \"unverified\" otherwise\n",
    "- supporting_evidence: List of any cited sources (URLs, titles)\n",
    "- claim_dependencies: List of claim_ids from EXISTING CLAIMS above that this claim assumes or builds upon\n",
    "\n",
    "Focus on substantive factual claims. Aim to extract 3-8 key claims per section.\n",
    "\"\"\"\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(ClaimExtractionOutput)\n",
    "    try:\n",
    "        result = await structured_llm.ainvoke([HumanMessage(content=enhanced_prompt)])\n",
    "        return [claim.model_dump() for claim in result.claims]\n",
    "    except Exception as e:\n",
    "        print(f\"    Error extracting claims: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKELETON_GENERATION_PROMPT = \"\"\"You are a research document architect. Create a detailed document skeleton for answering this research question.\n",
    "\n",
    "Research Question: {question}\n",
    "\n",
    "Preliminary Research Findings:\n",
    "{research_findings}\n",
    "\n",
    "Create a hierarchical document structure with:\n",
    "1. A clear thesis statement (one sentence capturing the document's central argument)\n",
    "2. 5-7 main sections appropriate for a comprehensive research report\n",
    "3. Each section should have:\n",
    "   - node_id: Unique identifier using format \"sec:topic\" (e.g., \"sec:intro\", \"sec:background\", \"sec:methodology\")\n",
    "   - title: Descriptive section title for the final document\n",
    "   - intent: 1-3 sentences describing what this section should accomplish and what key points it should cover\n",
    "   - target_word_count: 250-400 words per leaf section (sections with no children)\n",
    "   - dependencies: List of node_ids that this section builds upon or references (empty for first sections)\n",
    "   - children: Empty list for leaf nodes (we use flat structure)\n",
    "\n",
    "REQUIRED SECTIONS (adapt titles to topic):\n",
    "1. Introduction - Present the topic, context, and thesis\n",
    "2. Background/Context - Provide necessary foundation knowledge\n",
    "3. Main Body (2-4 sections) - Cover key aspects of the research question in depth\n",
    "4. Analysis/Discussion - Synthesize findings and discuss implications\n",
    "5. Conclusion - Summarize key points and future directions\n",
    "\n",
    "IMPORTANT RULES:\n",
    "- Node IDs must be unique and follow format: sec:topic (e.g., sec:intro, sec:background, sec:analysis)\n",
    "- Dependencies must reference existing node_ids (no forward references)\n",
    "- Dependencies form a DAG: no circular dependencies allowed\n",
    "- Earlier sections should have fewer dependencies; later sections can depend on earlier ones\n",
    "- Keep children list empty (flat structure for this implementation)\n",
    "\"\"\"\n",
    "\n",
    "PROSE_GENERATION_PROMPT = \"\"\"You are a research writer generating content for a specific section of a comprehensive report.\n",
    "\n",
    "DOCUMENT CONTEXT:\n",
    "Research Question: {question}\n",
    "Document Thesis: {thesis}\n",
    "\n",
    "SECTION TO WRITE:\n",
    "Node ID: {node_id}\n",
    "Title: {title}\n",
    "Intent: {intent}\n",
    "Target Length: ~{target_words} words\n",
    "\n",
    "DOCUMENT STRUCTURE (full outline for context):\n",
    "{skeleton_summary}\n",
    "\n",
    "PREVIOUS SECTION'S ENDING (write your bridge_in to connect smoothly):\n",
    "{previous_bridge_out}\n",
    "\n",
    "CONTENT FROM DEPENDENCY SECTIONS (build upon these established points):\n",
    "{dependency_summaries}\n",
    "\n",
    "RELEVANT RESEARCH FINDINGS (cite these sources):\n",
    "{research_findings}\n",
    "\n",
    "WRITING REQUIREMENTS:\n",
    "1. **bridge_in** (1-2 sentences): Transitional text connecting from the previous section. Reference what was just established and introduce why this section follows naturally.\n",
    "\n",
    "2. **main_content** (~{target_words} words): Substantive prose that:\n",
    "   - Fully addresses the section's stated intent\n",
    "   - Includes specific facts, examples, and evidence from the research findings\n",
    "   - Uses in-text citations in format (Source: URL) when referencing findings\n",
    "   - Maintains academic tone while being accessible\n",
    "   - Does NOT repeat content from dependency sections but builds upon them\n",
    "\n",
    "3. **bridge_out** (1-2 sentences): Transitional text setting up the next section. Signal what aspect will be explored next and why it follows logically.\n",
    "\n",
    "4. **summary** (1-2 sentences): Concise statement of what this section establishes as fact. This summary will be used as context for later sections.\n",
    "\n",
    "Be comprehensive, specific, and well-sourced. Avoid vague generalizations.\n",
    "\"\"\"\n",
    "\n",
    "CLAIM_EXTRACTION_PROMPT = \"\"\"Extract all factual claims from this research prose.\n",
    "\n",
    "Section Node ID: {node_id}\n",
    "\n",
    "Prose Content:\n",
    "{prose}\n",
    "\n",
    "A claim is any statement that:\n",
    "- Has a truth value (could be verified or refuted through evidence)\n",
    "- Contributes to the document's argument\n",
    "- Is NOT just a structural statement about the document itself (e.g., \"This section discusses...\" is NOT a claim)\n",
    "\n",
    "For each claim, identify:\n",
    "- claim_id: Unique ID using format: claim_{node_id}_{number} (e.g., claim_sec:intro_1)\n",
    "- claim_text: The precise assertion, stated as a single declarative sentence\n",
    "- source_node: \"{node_id}\"\n",
    "- verification_status: One of:\n",
    "  - \"verified\": The claim cites a specific source in the prose\n",
    "  - \"unverified\": The claim is made without explicit source citation\n",
    "- supporting_evidence: List of any cited sources (URLs, titles) that support this claim\n",
    "- claim_dependencies: List of claim_ids from earlier sections that this claim assumes or builds upon\n",
    "\n",
    "Focus on substantive factual claims. Aim to extract 3-8 key claims per section.\n",
    "\"\"\"\n",
    "\n",
    "CRITIQUE_PROMPT = \"\"\"You are a critical reviewer evaluating a research document section by section.\n",
    "\n",
    "ORIGINAL QUESTION: {question}\n",
    "\n",
    "DOCUMENT THESIS: {thesis}\n",
    "\n",
    "DOCUMENT STRUCTURE:\n",
    "{skeleton_summary}\n",
    "\n",
    "CLAIMS REGISTRY (showing verification status):\n",
    "{claims_summary}\n",
    "\n",
    "FULL DOCUMENT CONTENT:\n",
    "{document_content}\n",
    "\n",
    "Analyze this document at THREE levels:\n",
    "\n",
    "## 1. GLOBAL ISSUES (affect entire document)\n",
    "- Is the thesis clearly stated and consistently supported throughout?\n",
    "- Does the overall argument flow logically from introduction to conclusion?\n",
    "- Is terminology used consistently across all sections?\n",
    "- Does the conclusion accurately reflect findings in the body?\n",
    "\n",
    "## 2. SECTION ISSUES (per node) - MOST IMPORTANT\n",
    "- **weak_claim**: Claims that lack sufficient evidence or citation\n",
    "- **missing_evidence**: Key assertions that need supporting sources\n",
    "- **logical_gap**: Reasoning jumps or non-sequiturs within a section\n",
    "- **unclear**: Ambiguous or confusing passages\n",
    "- **depth**: Insufficient detail on important points\n",
    "\n",
    "## 3. TRANSITION ISSUES (between adjacent nodes)\n",
    "- Abrupt topic shifts without proper bridging\n",
    "- Redundant or repetitive transitions\n",
    "- Broken forward/backward references\n",
    "\n",
    "For EACH issue found, provide:\n",
    "- issue_id: Unique identifier (e.g., \"issue_1\", \"issue_2\")\n",
    "- scope: \"global\", \"section\", or \"transition\"\n",
    "- target_nodes: List of affected node IDs (e.g., [\"sec:intro\", \"sec:background\"])\n",
    "- issue_type: One of: weak_claim, missing_evidence, logical_gap, unclear, coherence, depth, transition\n",
    "- severity: \n",
    "  - \"critical\": Must fix, significantly impacts document quality\n",
    "  - \"major\": Should fix, notable weakness\n",
    "  - \"minor\": Could improve, but acceptable\n",
    "- affected_claims: List of claim_ids affected (if applicable)\n",
    "- description: Clear explanation of what the problem is\n",
    "- suggestion: Specific actionable advice for fixing the issue\n",
    "- search_query: For evidence-related issues, a specific query to find supporting sources (be precise, not generic)\n",
    "\n",
    "SCORING GUIDE for overall_quality (1-10):\n",
    "- 9-10: Publication ready, well-sourced, coherent argument\n",
    "- 7-8: Good quality, minor issues only\n",
    "- 5-6: Acceptable but needs improvement on several major issues\n",
    "- 3-4: Significant problems with evidence, coherence, or depth\n",
    "- 1-2: Major rework needed\n",
    "\n",
    "Provide an overall_quality score, summary of strengths, and comprehensive list of issues.\n",
    "\"\"\"\n",
    "\n",
    "PATCH_PROMPT = \"\"\"You are revising a specific section of a research document based on critique feedback and new evidence.\n",
    "\n",
    "SECTION TO REVISE:\n",
    "Node ID: {node_id}\n",
    "Title: {title}\n",
    "Intent: {intent}\n",
    "\n",
    "CURRENT CONTENT:\n",
    "Bridge In: {current_bridge_in}\n",
    "Main Content: {current_main_content}\n",
    "Bridge Out: {current_bridge_out}\n",
    "\n",
    "ISSUES TO ADDRESS (you MUST fix all of these):\n",
    "{issues_for_node}\n",
    "\n",
    "NEW EVIDENCE FOUND (incorporate with citations):\n",
    "{new_evidence}\n",
    "\n",
    "CONTEXT FROM ADJACENT SECTIONS:\n",
    "Previous section ends with: {prev_bridge_out}\n",
    "Next section starts with: {next_bridge_in}\n",
    "\n",
    "REVISION INSTRUCTIONS:\n",
    "1. Address ALL identified issues explicitly\n",
    "2. Incorporate the new evidence with proper citations (Source: URL)\n",
    "3. Maintain smooth transitions with adjacent sections (match their bridge sentences)\n",
    "4. Keep approximately the same length - do not shorten significantly\n",
    "5. Preserve any existing good content that doesn't conflict with the fixes\n",
    "6. Ensure the revised content still fulfills the section's original intent\n",
    "\n",
    "Output the complete revised section:\n",
    "- bridge_in: Updated transition from previous section\n",
    "- main_content: Revised substantive prose with issues fixed and evidence incorporated\n",
    "- bridge_out: Updated transition to next section  \n",
    "- summary: Updated 1-2 sentence summary of what this section now establishes\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Node Functions\n",
    "\n",
    "### 6.1 Phase 1: Preliminary Research & Skeleton Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def preliminary_research(state: PatchBasedRefinementState) -> dict:\n",
    "    \"\"\"Conduct lightweight initial research to inform skeleton generation.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 1a: Preliminary Research\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Generate 3-4 broad search queries\n",
    "    query_prompt = f\"\"\"Generate 4 search queries to understand the scope of this research question:\n",
    "    \n",
    "Question: {question}\n",
    "\n",
    "Return 4 queries, one per line, covering different aspects.\"\"\"\n",
    "    \n",
    "    response = await llm.ainvoke([HumanMessage(content=query_prompt)])\n",
    "    queries = [q.strip() for q in response.content.split(\"\\n\") if q.strip()][:4]\n",
    "    \n",
    "    all_results = []\n",
    "    all_urls = []\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"  Searching: {query[:50]}...\")\n",
    "        results, urls = search_web(query, max_results=5)\n",
    "        all_results.extend(results)\n",
    "        all_urls.extend(urls)\n",
    "    \n",
    "    print(f\"  Collected {len(all_results)} preliminary results\")\n",
    "    \n",
    "    return {\n",
    "        \"search_results\": all_results,\n",
    "        \"source_urls\": all_urls,\n",
    "        \"iteration_count\": 0,\n",
    "        \"iteration_log\": [f\"Preliminary research: {len(all_results)} results from {len(queries)} queries\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_skeleton(skeleton: Dict[str, Any]) -> tuple[bool, List[str]]:\n",
    "    \"\"\"Validate skeleton structure per V3.md spec.\n",
    "    \n",
    "    Returns:\n",
    "        (is_valid, list_of_issues)\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    nodes = skeleton.get(\"nodes\", {})\n",
    "    root_nodes = skeleton.get(\"root_nodes\", [])\n",
    "    \n",
    "    # Check for unique node IDs (implicit - dict keys are unique)\n",
    "    \n",
    "    # Check all root nodes exist\n",
    "    for root_id in root_nodes:\n",
    "        if root_id not in nodes:\n",
    "            issues.append(f\"Root node '{root_id}' not found in nodes\")\n",
    "    \n",
    "    # Check dependencies exist and detect cycles\n",
    "    visited = set()\n",
    "    rec_stack = set()\n",
    "    \n",
    "    def has_cycle(node_id: str) -> bool:\n",
    "        \"\"\"DFS cycle detection.\"\"\"\n",
    "        visited.add(node_id)\n",
    "        rec_stack.add(node_id)\n",
    "        \n",
    "        node = nodes.get(node_id, {})\n",
    "        for dep_id in node.get(\"dependencies\", []):\n",
    "            if dep_id not in nodes:\n",
    "                issues.append(f\"Node '{node_id}' depends on non-existent node '{dep_id}'\")\n",
    "                continue\n",
    "            if dep_id not in visited:\n",
    "                if has_cycle(dep_id):\n",
    "                    return True\n",
    "            elif dep_id in rec_stack:\n",
    "                issues.append(f\"Circular dependency detected: {node_id} -> {dep_id}\")\n",
    "                return True\n",
    "        \n",
    "        rec_stack.discard(node_id)\n",
    "        return False\n",
    "    \n",
    "    for node_id in nodes:\n",
    "        if node_id not in visited:\n",
    "            has_cycle(node_id)\n",
    "    \n",
    "    # Check children references\n",
    "    for node_id, node in nodes.items():\n",
    "        for child_id in node.get(\"children\", []):\n",
    "            if child_id not in nodes:\n",
    "                issues.append(f\"Node '{node_id}' has non-existent child '{child_id}'\")\n",
    "    \n",
    "    # Check word counts are reasonable\n",
    "    total_words = sum(n.get(\"target_word_count\", 0) for n in nodes.values())\n",
    "    if total_words < 500:\n",
    "        issues.append(f\"Total target word count ({total_words}) seems too low for a research report\")\n",
    "    if total_words > 20000:\n",
    "        issues.append(f\"Total target word count ({total_words}) is very high - consider reducing\")\n",
    "    \n",
    "    return len(issues) == 0, issues\n",
    "\n",
    "\n",
    "async def generate_skeleton(state: PatchBasedRefinementState) -> dict:\n",
    "    \"\"\"Generate the document skeleton structure.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    search_results = state.get(\"search_results\", [])\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 1b: Skeleton Generation\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Prepare research summary for skeleton generation\n",
    "    research_summary = \"\\n\".join(search_results[:15])  # Limit context\n",
    "\n",
    "    prompt = SKELETON_GENERATION_PROMPT.format(\n",
    "        question=question,\n",
    "        research_findings=research_summary\n",
    "    )\n",
    "\n",
    "    # Use structured output for skeleton\n",
    "    structured_llm = llm.with_structured_output(SkeletonGenerationOutput)\n",
    "    result = await structured_llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # Build the skeleton dictionary\n",
    "    skeleton = {\n",
    "        \"thesis\": result.thesis,\n",
    "        \"root_nodes\": [],\n",
    "        \"nodes\": {},\n",
    "        \"style_constraints\": \"Academic tone, comprehensive analysis, evidence-based claims\"\n",
    "    }\n",
    "\n",
    "    # Collect all node IDs that are referenced as children\n",
    "    child_ids = set()\n",
    "    for section in result.sections:\n",
    "        child_ids.update(section.children)\n",
    "        \n",
    "    for section in result.sections:\n",
    "        skeleton[\"nodes\"][section.node_id] = section.model_dump()\n",
    "        \n",
    "        # IMPROVED: A root node is one not referenced as a child by any other node\n",
    "        # This is more robust than the colon-count heuristic\n",
    "        if section.node_id not in child_ids:\n",
    "            skeleton[\"root_nodes\"].append(section.node_id)\n",
    "\n",
    "    # Fallback: if no root nodes detected, use nodes without dependencies\n",
    "    if not skeleton[\"root_nodes\"]:\n",
    "        for nid, node in skeleton[\"nodes\"].items():\n",
    "            if not node.get(\"dependencies\", []):\n",
    "                skeleton[\"root_nodes\"].append(nid)\n",
    "    \n",
    "    # Final fallback: use all nodes as roots if still empty\n",
    "    if not skeleton[\"root_nodes\"]:\n",
    "        skeleton[\"root_nodes\"] = list(skeleton[\"nodes\"].keys())\n",
    "\n",
    "    # VALIDATE SKELETON\n",
    "    is_valid, validation_issues = validate_skeleton(skeleton)\n",
    "    \n",
    "    if not is_valid:\n",
    "        print(f\"  WARNING: Skeleton validation found issues:\")\n",
    "        for issue in validation_issues:\n",
    "            print(f\"    - {issue}\")\n",
    "    else:\n",
    "        print(f\"  Skeleton validated successfully\")\n",
    "\n",
    "    print(f\"  Thesis: {result.thesis[:100]}...\")\n",
    "    print(f\"  Generated {len(skeleton['nodes'])} skeleton nodes:\")\n",
    "    for nid, node in skeleton[\"nodes\"].items():\n",
    "        deps = node.get('dependencies', [])\n",
    "        dep_str = f\" (depends on: {', '.join(deps)})\" if deps else \"\"\n",
    "        print(f\"    - {nid}: {node['title']}{dep_str}\")\n",
    "\n",
    "    return {\n",
    "        \"skeleton\": skeleton,\n",
    "        \"prose_store\": {},\n",
    "        \"claims_registry\": {},\n",
    "        \"iteration_log\": [f\"Skeleton generated with {len(skeleton['nodes'])} nodes (valid: {is_valid})\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Phase 2: Node Expansion (Prose Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def expand_single_node(\n",
    "    node_id: str,\n",
    "    skeleton: Dict[str, Any],\n",
    "    prose_store: Dict[str, Dict[str, Any]],\n",
    "    search_results: List[str],\n",
    "    question: str\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Generate prose for a single skeleton node.\"\"\"\n",
    "    node = skeleton[\"nodes\"][node_id]\n",
    "    leaves = get_leaf_nodes(skeleton)\n",
    "    \n",
    "    # Get previous node's bridge_out for transition\n",
    "    prev_node, next_node = get_adjacent_nodes(skeleton, node_id)\n",
    "    prev_bridge_out = \"\"\n",
    "    if prev_node and prev_node in prose_store:\n",
    "        prev_bridge_out = prose_store[prev_node].get(\"bridge_out\", \"\")\n",
    "    \n",
    "    # Get dependency summaries\n",
    "    dependency_summaries = []\n",
    "    for dep_id in node.get(\"dependencies\", []):\n",
    "        if dep_id in prose_store:\n",
    "            dep_summary = prose_store[dep_id].get(\"summary\", \"\")\n",
    "            dep_title = skeleton[\"nodes\"].get(dep_id, {}).get(\"title\", dep_id)\n",
    "            dependency_summaries.append(f\"{dep_title}: {dep_summary}\")\n",
    "    \n",
    "    # Build skeleton summary for context\n",
    "    skeleton_summary = \"\\n\".join([\n",
    "        f\"- {nid}: {skeleton['nodes'][nid]['title']}\" \n",
    "        for nid in leaves\n",
    "    ])\n",
    "    \n",
    "    # Select relevant research findings (limit tokens)\n",
    "    relevant_findings = \"\\n\".join(search_results[:10])\n",
    "    \n",
    "    prompt = PROSE_GENERATION_PROMPT.format(\n",
    "        question=question,\n",
    "        thesis=skeleton.get(\"thesis\", \"\"),\n",
    "        node_id=node_id,\n",
    "        title=node.get(\"title\", \"\"),\n",
    "        intent=node.get(\"intent\", \"\"),\n",
    "        target_words=node.get(\"target_word_count\", TARGET_WORDS_PER_NODE),\n",
    "        skeleton_summary=skeleton_summary,\n",
    "        previous_bridge_out=prev_bridge_out if prev_bridge_out else \"(This is the first section)\",\n",
    "        dependency_summaries=\"\\n\".join(dependency_summaries) if dependency_summaries else \"(No dependencies)\",\n",
    "        research_findings=relevant_findings\n",
    "    )\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(ProseGenerationOutput)\n",
    "    result = await structured_llm.ainvoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    return {\n",
    "        \"node_id\": node_id,\n",
    "        \"main_content\": result.main_content,\n",
    "        \"bridge_in\": result.bridge_in,\n",
    "        \"bridge_out\": result.bridge_out,\n",
    "        \"summary\": result.summary,\n",
    "        \"revision_count\": 0,\n",
    "        \"previous_versions\": []\n",
    "    }\n",
    "\n",
    "\n",
    "async def expand_all_nodes(state: PatchBasedRefinementState) -> dict:\n",
    "    \"\"\"Expand all leaf nodes in dependency order.\"\"\"\n",
    "    skeleton = state[\"skeleton\"]\n",
    "    search_results = state.get(\"search_results\", [])\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 2: Node Expansion\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get leaf nodes in dependency order\n",
    "    leaf_nodes = get_leaf_nodes(skeleton)\n",
    "    sorted_nodes = topological_sort_nodes(skeleton, leaf_nodes)\n",
    "    \n",
    "    prose_store = {}\n",
    "    \n",
    "    for i, node_id in enumerate(sorted_nodes):\n",
    "        node = skeleton[\"nodes\"][node_id]\n",
    "        print(f\"  [{i+1}/{len(sorted_nodes)}] Expanding: {node_id} - {node['title']}\")\n",
    "        \n",
    "        prose_entry = await expand_single_node(\n",
    "            node_id=node_id,\n",
    "            skeleton=skeleton,\n",
    "            prose_store=prose_store,\n",
    "            search_results=search_results,\n",
    "            question=question\n",
    "        )\n",
    "        prose_store[node_id] = prose_entry\n",
    "        \n",
    "        # Mark as expanded in skeleton\n",
    "        skeleton[\"nodes\"][node_id][\"is_expanded\"] = True\n",
    "        \n",
    "        print(f\"      Generated {len(prose_entry['main_content'])} chars\")\n",
    "    \n",
    "    return {\n",
    "        \"skeleton\": skeleton,\n",
    "        \"prose_store\": prose_store,\n",
    "        \"iteration_log\": [f\"Expanded {len(sorted_nodes)} nodes\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Phase 3: Claim Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_claims(state: PatchBasedRefinementState) -> dict:\n",
    "    \"\"\"Extract claims from all prose and build the claims registry.\"\"\"\n",
    "    prose_store = state[\"prose_store\"]\n",
    "    skeleton = state[\"skeleton\"]\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 3: Claim Extraction\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    claims_registry = {}\n",
    "    \n",
    "    # Get nodes in document order for proper dependency tracking\n",
    "    leaf_nodes = get_leaf_nodes(skeleton)\n",
    "    sorted_nodes = topological_sort_nodes(skeleton, leaf_nodes)\n",
    "\n",
    "    for node_id in sorted_nodes:\n",
    "        if node_id not in prose_store:\n",
    "            continue\n",
    "            \n",
    "        prose_entry = prose_store[node_id]\n",
    "        print(f\"  Extracting claims from: {node_id}\")\n",
    "\n",
    "        # Use the enhanced extraction function with existing claims context\n",
    "        new_claims = await extract_claims_for_node(\n",
    "            node_id,\n",
    "            prose_entry,\n",
    "            claims_registry  # Pass existing claims for dependency detection\n",
    "        )\n",
    "\n",
    "        for claim in new_claims:\n",
    "            claims_registry[claim[\"claim_id\"]] = claim\n",
    "\n",
    "        print(f\"    Found {len(new_claims)} claims\")\n",
    "\n",
    "    # Count verification status\n",
    "    verified = sum(1 for c in claims_registry.values() if c.get(\"verification_status\") == \"verified\")\n",
    "    unverified = sum(1 for c in claims_registry.values() if c.get(\"verification_status\") == \"unverified\")\n",
    "\n",
    "    print(f\"  Total claims: {len(claims_registry)} ({verified} verified, {unverified} unverified)\")\n",
    "\n",
    "    return {\n",
    "        \"claims_registry\": claims_registry,\n",
    "        \"iteration_log\": [f\"Extracted {len(claims_registry)} claims ({verified} verified, {unverified} unverified)\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Phase 4: Structured Critique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def critique_document(state: PatchBasedRefinementState) -> dict:\n",
    "    \"\"\"Perform multi-level critique producing the Noise Map.\"\"\"\n",
    "    skeleton = state[\"skeleton\"]\n",
    "    prose_store = state[\"prose_store\"]\n",
    "    claims_registry = state.get(\"claims_registry\", {})\n",
    "    question = state[\"question\"]\n",
    "    iteration_count = state.get(\"iteration_count\", 0)\n",
    "    cascade_queue = state.get(\"cascade_queue\", [])\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 4: Structured Critique (Iteration {iteration_count})\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Process cascade queue - nodes flagged from previous iteration need priority re-evaluation\n",
    "    if cascade_queue:\n",
    "        print(f\"  Processing {len(cascade_queue)} cascaded nodes from previous iteration\")\n",
    "\n",
    "    # Build skeleton summary\n",
    "    leaf_nodes = get_leaf_nodes(skeleton)\n",
    "    skeleton_summary = \"\\n\".join([\n",
    "        f\"- {nid}: {skeleton['nodes'][nid]['title']}\" \n",
    "        for nid in leaf_nodes\n",
    "    ])\n",
    "\n",
    "    # Build claims summary\n",
    "    claims_summary = \"\\n\".join([\n",
    "        f\"- [{c['verification_status']}] {c['claim_id']}: {c['claim_text'][:80]}...\"\n",
    "        for c in list(claims_registry.values())[:30]  # Increased limit\n",
    "    ])\n",
    "\n",
    "    # Build full document content\n",
    "    document_content = \"\"\n",
    "    for node_id in leaf_nodes:\n",
    "        if node_id in prose_store:\n",
    "            node = skeleton[\"nodes\"][node_id]\n",
    "            prose = prose_store[node_id]\n",
    "            document_content += f\"\\n\\n## {node['title']} [{node_id}]\\n\"\n",
    "            document_content += f\"{prose.get('bridge_in', '')}\\n\"\n",
    "            document_content += f\"{prose.get('main_content', '')}\\n\"\n",
    "            document_content += f\"{prose.get('bridge_out', '')}\\n\"\n",
    "\n",
    "    # Truncate if too long\n",
    "    if len(document_content) > 15000:\n",
    "        document_content = document_content[:15000] + \"\\n\\n[... truncated for critique ...]\\n\"\n",
    "\n",
    "    prompt = CRITIQUE_PROMPT.format(\n",
    "        question=question,\n",
    "        thesis=skeleton.get(\"thesis\", \"\"),\n",
    "        skeleton_summary=skeleton_summary,\n",
    "        claims_summary=claims_summary if claims_summary else \"(No claims extracted yet)\",\n",
    "        document_content=document_content\n",
    "    )\n",
    "\n",
    "    structured_llm = llm.with_structured_output(CritiqueResult)\n",
    "    result = await structured_llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # Convert issues to dicts for state storage\n",
    "    noise_map = [issue.model_dump() for issue in result.issues]\n",
    "\n",
    "    # Identify nodes that need patching (have issues)\n",
    "    nodes_to_patch = list(set(\n",
    "        node_id \n",
    "        for issue in result.issues \n",
    "        for node_id in issue.target_nodes\n",
    "        if issue.severity in [\"critical\", \"major\"]\n",
    "    ))\n",
    "\n",
    "    # CRITICAL FIX: Incorporate cascade queue from previous iteration\n",
    "    # Nodes that depend on previously patched content need re-evaluation\n",
    "    for cascade_node in cascade_queue:\n",
    "        if cascade_node not in nodes_to_patch and cascade_node in prose_store:\n",
    "            nodes_to_patch.append(cascade_node)\n",
    "            print(f\"  Added cascade node to patch list: {cascade_node}\")\n",
    "\n",
    "    print(f\"  Quality Score: {result.overall_quality}/10\")\n",
    "    print(f\"  Issues found: {len(result.issues)}\")\n",
    "    print(f\"    - Critical: {sum(1 for i in result.issues if i.severity == 'critical')}\")\n",
    "    print(f\"    - Major: {sum(1 for i in result.issues if i.severity == 'major')}\")\n",
    "    print(f\"    - Minor: {sum(1 for i in result.issues if i.severity == 'minor')}\")\n",
    "    print(f\"  Nodes to patch: {nodes_to_patch}\")\n",
    "\n",
    "    return {\n",
    "        \"noise_map\": noise_map,\n",
    "        \"nodes_to_patch\": nodes_to_patch,\n",
    "        \"quality_scores\": [result.overall_quality],\n",
    "        \"cascade_queue\": [],  # Clear cascade queue after processing\n",
    "        \"iteration_log\": [f\"Critique: {result.overall_quality}/10, {len(result.issues)} issues, {len(nodes_to_patch)} nodes to patch\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Phase 5: Targeted Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def targeted_retrieval(state: PatchBasedRefinementState) -> dict:\n",
    "    \"\"\"Search for evidence to address specific issues in the Noise Map.\"\"\"\n",
    "    noise_map = state.get(\"noise_map\", [])\n",
    "    nodes_to_patch = state.get(\"nodes_to_patch\", [])\n",
    "    claims_registry = state.get(\"claims_registry\", {})\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 5: Targeted Retrieval\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    if not nodes_to_patch:\n",
    "        print(\"  No nodes need patching - skipping retrieval\")\n",
    "        return {\n",
    "            \"targeted_evidence\": {},\n",
    "            \"search_results\": [],\n",
    "            \"source_urls\": [],\n",
    "            \"claims_registry\": claims_registry\n",
    "        }\n",
    "\n",
    "    # Collect search queries for each node from issues\n",
    "    node_queries = {}  # node_id -> list of search queries\n",
    "    claim_to_node = {}  # claim_id -> node_id (for updating verification status)\n",
    "    \n",
    "    for issue in noise_map:\n",
    "        if issue.get(\"search_query\") and issue.get(\"severity\") in [\"critical\", \"major\"]:\n",
    "            for node_id in issue.get(\"target_nodes\", []):\n",
    "                if node_id in nodes_to_patch:\n",
    "                    if node_id not in node_queries:\n",
    "                        node_queries[node_id] = []\n",
    "                    node_queries[node_id].append(issue[\"search_query\"])\n",
    "                    \n",
    "                    # Track affected claims for verification status updates\n",
    "                    for claim_id in issue.get(\"affected_claims\", []):\n",
    "                        claim_to_node[claim_id] = node_id\n",
    "\n",
    "    targeted_evidence = {}  # node_id -> list of evidence strings\n",
    "    all_results = []\n",
    "    all_urls = []\n",
    "    claims_updated = 0\n",
    "\n",
    "    for node_id, queries in node_queries.items():\n",
    "        print(f\"  Researching for node: {node_id}\")\n",
    "        node_evidence = []\n",
    "\n",
    "        # Limit queries per node\n",
    "        for query in queries[:3]:\n",
    "            print(f\"    Query: {query[:50]}...\")\n",
    "            results, urls = search_web(query, max_results=5)\n",
    "            node_evidence.extend(results)\n",
    "            all_results.extend(results)\n",
    "            all_urls.extend(urls)\n",
    "\n",
    "        targeted_evidence[node_id] = node_evidence\n",
    "        print(f\"    Found {len(node_evidence)} results\")\n",
    "\n",
    "    # CRITICAL FIX: Update claims verification status based on retrieved evidence\n",
    "    # Find unverified claims in nodes that got evidence and mark them for update\n",
    "    for node_id, evidence in targeted_evidence.items():\n",
    "        if evidence:  # If we found evidence for this node\n",
    "            # Find unverified claims in this node\n",
    "            for claim_id, claim in claims_registry.items():\n",
    "                if claim.get(\"source_node\") == node_id and claim.get(\"verification_status\") == \"unverified\":\n",
    "                    # Mark as verified since we found supporting evidence\n",
    "                    claims_registry[claim_id][\"verification_status\"] = \"verified\"\n",
    "                    claims_registry[claim_id][\"supporting_evidence\"].extend(\n",
    "                        [r[:200] for r in evidence[:3]]  # Add first 3 pieces of evidence\n",
    "                    )\n",
    "                    claims_updated += 1\n",
    "\n",
    "    print(f\"  Total new evidence: {len(all_results)} results\")\n",
    "    print(f\"  Claims updated to verified: {claims_updated}\")\n",
    "\n",
    "    return {\n",
    "        \"targeted_evidence\": targeted_evidence,\n",
    "        \"search_results\": all_results,\n",
    "        \"source_urls\": all_urls,\n",
    "        \"claims_registry\": claims_registry,\n",
    "        \"iteration_log\": [f\"Targeted retrieval: {len(all_results)} results for {len(node_queries)} nodes, {claims_updated} claims verified\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Phase 6: Patch Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def apply_patches(state: PatchBasedRefinementState) -> dict:\n",
    "    \"\"\"Apply targeted patches to nodes with issues.\"\"\"\n",
    "    skeleton = state[\"skeleton\"]\n",
    "    prose_store = state[\"prose_store\"]\n",
    "    claims_registry = state.get(\"claims_registry\", {})\n",
    "    noise_map = state.get(\"noise_map\", [])\n",
    "    nodes_to_patch = state.get(\"nodes_to_patch\", [])\n",
    "    targeted_evidence = state.get(\"targeted_evidence\", {})\n",
    "    iteration_count = state.get(\"iteration_count\", 0)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase 6: Patch Application\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    if not nodes_to_patch:\n",
    "        print(\"  No patches to apply\")\n",
    "        return {\n",
    "            \"prose_store\": prose_store,\n",
    "            \"claims_registry\": claims_registry,\n",
    "            \"cascade_queue\": [],\n",
    "            \"iteration_count\": iteration_count + 1\n",
    "        }\n",
    "\n",
    "    # Sort nodes by dependency order\n",
    "    sorted_patch_nodes = topological_sort_nodes(skeleton, nodes_to_patch)\n",
    "    cascade_queue = []\n",
    "    patched_nodes = []\n",
    "\n",
    "    structured_llm = llm.with_structured_output(ProseGenerationOutput)\n",
    "\n",
    "    for node_id in sorted_patch_nodes:\n",
    "        if node_id not in prose_store:\n",
    "            continue\n",
    "\n",
    "        node = skeleton[\"nodes\"].get(node_id, {})\n",
    "        current_prose = prose_store[node_id]\n",
    "\n",
    "        print(f\"  Patching: {node_id} - {node.get('title', '')}\")\n",
    "\n",
    "        # Collect issues for this node\n",
    "        node_issues = [\n",
    "            issue for issue in noise_map \n",
    "            if node_id in issue.get(\"target_nodes\", [])\n",
    "        ]\n",
    "        issues_text = \"\\n\".join([\n",
    "            f\"- [{i.get('severity', '')}] {i.get('issue_type', '')}: {i.get('description', '')}\\n  Suggestion: {i.get('suggestion', '')}\"\n",
    "            for i in node_issues\n",
    "        ])\n",
    "\n",
    "        # Get evidence for this node\n",
    "        node_evidence = targeted_evidence.get(node_id, [])\n",
    "        evidence_text = \"\\n\".join(node_evidence) if node_evidence else \"No additional evidence found.\"\n",
    "\n",
    "        # Get adjacent section context\n",
    "        prev_node, next_node = get_adjacent_nodes(skeleton, node_id)\n",
    "        prev_bridge_out = \"\"\n",
    "        next_bridge_in = \"\"\n",
    "        if prev_node and prev_node in prose_store:\n",
    "            prev_bridge_out = prose_store[prev_node].get(\"bridge_out\", \"\")\n",
    "        if next_node and next_node in prose_store:\n",
    "            next_bridge_in = prose_store[next_node].get(\"bridge_in\", \"\")\n",
    "\n",
    "        prompt = PATCH_PROMPT.format(\n",
    "            node_id=node_id,\n",
    "            title=node.get(\"title\", \"\"),\n",
    "            intent=node.get(\"intent\", \"\"),\n",
    "            current_bridge_in=current_prose.get(\"bridge_in\", \"\"),\n",
    "            current_main_content=current_prose.get(\"main_content\", \"\"),\n",
    "            current_bridge_out=current_prose.get(\"bridge_out\", \"\"),\n",
    "            issues_for_node=issues_text,\n",
    "            new_evidence=evidence_text[:6000],  # Limit evidence length\n",
    "            prev_bridge_out=prev_bridge_out if prev_bridge_out else \"(First section)\",\n",
    "            next_bridge_in=next_bridge_in if next_bridge_in else \"(Last section)\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = await structured_llm.ainvoke([HumanMessage(content=prompt)])\n",
    "\n",
    "            # Archive previous version\n",
    "            previous_versions = current_prose.get(\"previous_versions\", [])\n",
    "            previous_versions.append(current_prose.get(\"main_content\", \"\"))\n",
    "\n",
    "            # Update prose store with patch\n",
    "            prose_store[node_id] = {\n",
    "                \"node_id\": node_id,\n",
    "                \"main_content\": result.main_content,\n",
    "                \"bridge_in\": result.bridge_in,\n",
    "                \"bridge_out\": result.bridge_out,\n",
    "                \"summary\": result.summary,\n",
    "                \"revision_count\": current_prose.get(\"revision_count\", 0) + 1,\n",
    "                \"previous_versions\": previous_versions[-3:]  # Keep last 3 versions\n",
    "            }\n",
    "\n",
    "            patched_nodes.append(node_id)\n",
    "            print(f\"    Patched: {len(result.main_content)} chars (revision #{prose_store[node_id]['revision_count']})\")\n",
    "\n",
    "            # Check for cascades - find nodes that depend on this one\n",
    "            for other_id, other_node in skeleton[\"nodes\"].items():\n",
    "                if node_id in other_node.get(\"dependencies\", []) and other_id not in nodes_to_patch:\n",
    "                    if other_id not in cascade_queue:\n",
    "                        cascade_queue.append(other_id)\n",
    "                        print(f\"    Cascade detected: {other_id} depends on patched node\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Error patching {node_id}: {e}\")\n",
    "\n",
    "    # CRITICAL FIX: Re-extract claims from patched nodes to update the registry\n",
    "    print(f\"\\n  Re-extracting claims from {len(patched_nodes)} patched nodes...\")\n",
    "\n",
    "    # Remove old claims from patched nodes\n",
    "    old_claim_ids = [cid for cid, claim in claims_registry.items() \n",
    "                     if claim.get(\"source_node\") in patched_nodes]\n",
    "    for old_id in old_claim_ids:\n",
    "        del claims_registry[old_id]\n",
    "    print(f\"    Removed {len(old_claim_ids)} old claims from patched nodes\")\n",
    "\n",
    "    # Extract new claims from patched nodes\n",
    "    new_claims_count = 0\n",
    "    for node_id in patched_nodes:\n",
    "        if node_id in prose_store:\n",
    "            new_claims = await extract_claims_for_node(\n",
    "                node_id, \n",
    "                prose_store[node_id],\n",
    "                claims_registry\n",
    "            )\n",
    "            for claim in new_claims:\n",
    "                claims_registry[claim[\"claim_id\"]] = claim\n",
    "            new_claims_count += len(new_claims)\n",
    "\n",
    "    print(f\"    Extracted {new_claims_count} new claims from patched nodes\")\n",
    "\n",
    "    # Limit cascade queue\n",
    "    cascade_queue = cascade_queue[:MAX_CASCADES_PER_ITER]\n",
    "\n",
    "    print(f\"  Patched {len(patched_nodes)} nodes\")\n",
    "    print(f\"  Cascade queue: {cascade_queue}\")\n",
    "\n",
    "    return {\n",
    "        \"prose_store\": prose_store,\n",
    "        \"claims_registry\": claims_registry,\n",
    "        \"cascade_queue\": cascade_queue,\n",
    "        \"iteration_count\": iteration_count + 1,\n",
    "        \"iteration_log\": [f\"Patched {len(patched_nodes)} nodes, {len(cascade_queue)} cascades queued, {new_claims_count} claims re-extracted\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 Phase 7: Convergence Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue_refining(state: PatchBasedRefinementState) -> Literal[\"targeted_retrieval\", \"assemble\"]:\n",
    "    \"\"\"Decide whether to continue refinement or finalize.\"\"\"\n",
    "    iteration_count = state.get(\"iteration_count\", 0)\n",
    "    quality_scores = state.get(\"quality_scores\", [])\n",
    "    nodes_to_patch = state.get(\"nodes_to_patch\", [])\n",
    "    cascade_queue = state.get(\"cascade_queue\", [])\n",
    "    \n",
    "    latest_score = quality_scores[-1] if quality_scores else 0\n",
    "    \n",
    "    print(f\"\\n--- Convergence Check ---\")\n",
    "    print(f\"  Iteration: {iteration_count}/{MAX_ITERATIONS}\")\n",
    "    print(f\"  Quality: {latest_score}/{QUALITY_THRESHOLD}\")\n",
    "    print(f\"  Nodes to patch: {len(nodes_to_patch)}\")\n",
    "    print(f\"  Cascade queue: {len(cascade_queue)}\")\n",
    "    \n",
    "    # Convergence conditions\n",
    "    if iteration_count >= MAX_ITERATIONS:\n",
    "        print(f\"  -> Max iterations reached. Finalizing.\")\n",
    "        return \"assemble\"\n",
    "    \n",
    "    if latest_score >= QUALITY_THRESHOLD:\n",
    "        print(f\"  -> Quality threshold met. Finalizing.\")\n",
    "        return \"assemble\"\n",
    "    \n",
    "    if not nodes_to_patch and not cascade_queue:\n",
    "        print(f\"  -> No issues to address. Finalizing.\")\n",
    "        return \"assemble\"\n",
    "    \n",
    "    print(f\"  -> Continuing refinement.\")\n",
    "    return \"targeted_retrieval\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8 Final Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def assemble_document(state: PatchBasedRefinementState) -> dict:\n",
    "    \"\"\"Assemble the final document from all prose entries.\"\"\"\n",
    "    skeleton = state[\"skeleton\"]\n",
    "    prose_store = state[\"prose_store\"]\n",
    "    claims_registry = state.get(\"claims_registry\", {})\n",
    "    quality_scores = state.get(\"quality_scores\", [])\n",
    "    iteration_count = state.get(\"iteration_count\", 0)\n",
    "    source_urls = state.get(\"source_urls\", [])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Final Assembly\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get leaf nodes in document order\n",
    "    leaf_nodes = get_leaf_nodes(skeleton)\n",
    "    \n",
    "    # Build document\n",
    "    document_parts = []\n",
    "    \n",
    "    # Title\n",
    "    document_parts.append(f\"# Research Report\\n\")\n",
    "    document_parts.append(f\"**Thesis:** {skeleton.get('thesis', '')}\\n\\n\")\n",
    "    \n",
    "    # Sections\n",
    "    for node_id in leaf_nodes:\n",
    "        if node_id not in prose_store:\n",
    "            continue\n",
    "            \n",
    "        node = skeleton[\"nodes\"].get(node_id, {})\n",
    "        prose = prose_store[node_id]\n",
    "        \n",
    "        # Section header\n",
    "        document_parts.append(f\"## {node.get('title', node_id)}\\n\\n\")\n",
    "        \n",
    "        # Content (bridge_in flows naturally)\n",
    "        if prose.get(\"bridge_in\"):\n",
    "            document_parts.append(f\"{prose['bridge_in']} \")\n",
    "        document_parts.append(f\"{prose.get('main_content', '')}\")\n",
    "        if prose.get(\"bridge_out\"):\n",
    "            document_parts.append(f\" {prose['bridge_out']}\")\n",
    "        document_parts.append(\"\\n\\n\")\n",
    "    \n",
    "    # References section\n",
    "    unique_urls = list(set(source_urls))\n",
    "    if unique_urls:\n",
    "        document_parts.append(\"## References\\n\\n\")\n",
    "        for i, url in enumerate(unique_urls[:20], 1):  # Limit to 20 refs\n",
    "            document_parts.append(f\"{i}. {url}\\n\")\n",
    "    \n",
    "    final_report = \"\".join(document_parts)\n",
    "    \n",
    "    # Stats\n",
    "    word_count = len(final_report.split())\n",
    "    verified_claims = sum(1 for c in claims_registry.values() if c.get(\"verification_status\") == \"verified\")\n",
    "    total_claims = len(claims_registry)\n",
    "    \n",
    "    print(f\"  Document length: {len(final_report)} chars ({word_count} words)\")\n",
    "    print(f\"  Sections: {len(leaf_nodes)}\")\n",
    "    print(f\"  Total iterations: {iteration_count}\")\n",
    "    print(f\"  Quality progression: {' -> '.join([f'{s:.1f}' for s in quality_scores])}\")\n",
    "    print(f\"  Claims: {verified_claims}/{total_claims} verified\")\n",
    "    print(f\"  Sources: {len(unique_urls)}\")\n",
    "    \n",
    "    return {\n",
    "        \"final_report\": final_report,\n",
    "        \"iteration_log\": [f\"Assembled: {word_count} words, {len(leaf_nodes)} sections, {len(unique_urls)} sources\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch-Based Iterative Refinement Agent compiled successfully\n",
      "\n",
      "Key features:\n",
      "  - Skeleton-based document structure with semantic addressing\n",
      "  - Per-node prose generation (patches, not full regeneration)\n",
      "  - Claims registry for tracking verification status\n",
      "  - Targeted retrieval for specific weak claims\n",
      "  - Cascade detection when patches affect dependent sections\n"
     ]
    }
   ],
   "source": [
    "# Build the Patch-Based Iterative Refinement Agent graph\n",
    "\n",
    "builder = StateGraph(PatchBasedRefinementState)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"preliminary_research\", preliminary_research)\n",
    "builder.add_node(\"generate_skeleton\", generate_skeleton)\n",
    "builder.add_node(\"expand_all_nodes\", expand_all_nodes)\n",
    "builder.add_node(\"extract_claims\", extract_claims)\n",
    "builder.add_node(\"critique_document\", critique_document)\n",
    "builder.add_node(\"targeted_retrieval\", targeted_retrieval)\n",
    "builder.add_node(\"apply_patches\", apply_patches)\n",
    "builder.add_node(\"assemble_document\", assemble_document)\n",
    "\n",
    "# Add edges - Initial flow\n",
    "builder.add_edge(START, \"preliminary_research\")\n",
    "builder.add_edge(\"preliminary_research\", \"generate_skeleton\")\n",
    "builder.add_edge(\"generate_skeleton\", \"expand_all_nodes\")\n",
    "builder.add_edge(\"expand_all_nodes\", \"extract_claims\")\n",
    "builder.add_edge(\"extract_claims\", \"critique_document\")\n",
    "\n",
    "# Conditional edge: continue refining or finalize\n",
    "builder.add_conditional_edges(\n",
    "    \"critique_document\",\n",
    "    should_continue_refining,\n",
    "    {\n",
    "        \"targeted_retrieval\": \"targeted_retrieval\",\n",
    "        \"assemble\": \"assemble_document\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Refinement loop\n",
    "builder.add_edge(\"targeted_retrieval\", \"apply_patches\")\n",
    "builder.add_edge(\"apply_patches\", \"critique_document\")\n",
    "\n",
    "# Final\n",
    "builder.add_edge(\"assemble_document\", END)\n",
    "\n",
    "# Compile\n",
    "patch_refinement_graph = builder.compile()\n",
    "\n",
    "print(\"Patch-Based Iterative Refinement Agent compiled successfully\")\n",
    "print(\"\\nKey features:\")\n",
    "print(\"  - Skeleton-based document structure with semantic addressing\")\n",
    "print(\"  - Per-node prose generation (patches, not full regeneration)\")\n",
    "print(\"  - Claims registry for tracking verification status\")\n",
    "print(\"  - Targeted retrieval for specific weak claims\")\n",
    "print(\"  - Cascade detection when patches affect dependent sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the graph\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "try:\n",
    "    display(Image(patch_refinement_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Agent Wrapper for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def patch_refinement_agent_async(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Async version of the Patch-Based Iterative Refinement research agent.\n",
    "    Use this version when calling from Jupyter notebooks.\n",
    "    \"\"\"\n",
    "    question = inputs.get(\"question\", \"\")\n",
    "    \n",
    "    result = await patch_refinement_graph.ainvoke(\n",
    "        {\"question\": question},\n",
    "        config={\"recursion_limit\": 100}\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"output\": result.get(\"final_report\", \"\"),\n",
    "        \"iteration_count\": result.get(\"iteration_count\", 0),\n",
    "        \"quality_scores\": result.get(\"quality_scores\", []),\n",
    "        \"source_urls\": result.get(\"source_urls\", []),\n",
    "        \"skeleton\": result.get(\"skeleton\", {}),\n",
    "        \"claims_registry\": result.get(\"claims_registry\", {})\n",
    "    }\n",
    "\n",
    "\n",
    "def patch_refinement_agent(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Sync wrapper function for Patch-Based Iterative Refinement research agent.\n",
    "    \n",
    "    Compatible with evaluation harness.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Dictionary with 'question' key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with 'output' key containing final report\n",
    "    \"\"\"\n",
    "    question = inputs.get(\"question\", \"\")\n",
    "    \n",
    "    async def _execute():\n",
    "        return await patch_refinement_graph.ainvoke(\n",
    "            {\"question\": question},\n",
    "            config={\"recursion_limit\": 100}\n",
    "        )\n",
    "    \n",
    "    # Check if we're already in an async context (e.g., Jupyter notebook)\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        # We're in an event loop - need to run in a separate thread\n",
    "        import concurrent.futures\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future = executor.submit(asyncio.run, _execute())\n",
    "            result = future.result()\n",
    "    except RuntimeError:\n",
    "        # No event loop running, safe to use asyncio.run\n",
    "        result = asyncio.run(_execute())\n",
    "    \n",
    "    return {\n",
    "        \"output\": result.get(\"final_report\", \"\"),\n",
    "        \"iteration_count\": result.get(\"iteration_count\", 0),\n",
    "        \"quality_scores\": result.get(\"quality_scores\", []),\n",
    "        \"source_urls\": result.get(\"source_urls\", [])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Manual Test\n",
    "\n",
    "Run this cell to verify the agent works correctly with a simple test question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Patch-Based Iterative Refinement Agent\n",
      "Question: What are the key benefits and challenges of using large language models in enterprise applications?\n",
      "\n",
      "Running patch-based research (this may take several minutes)...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Phase 1a: Preliminary Research\n",
      "============================================================\n",
      "  Searching: \"benefits of large language models in enterprise a...\n",
      "  Searching: \"challenges of deploying large language models in ...\n",
      "  Searching: \"security privacy compliance risks of LLMs in ente...\n",
      "  Searching: \"best practices for enterprise LLM deployment\" MLO...\n",
      "  Collected 24 preliminary results\n",
      "\n",
      "============================================================\n",
      "Phase 1b: Skeleton Generation\n",
      "============================================================\n",
      "  Skeleton validated successfully\n",
      "  Thesis: Large language models deliver substantial productivity, personalization, and decision-support benefi...\n",
      "  Generated 7 skeleton nodes:\n",
      "    - sec:intro: Introduction: LLMs in the Enterprise Landscape\n",
      "    - sec:background: Background and Context: What Large Language Models Are and How Enterprises Use Them (depends on: sec:intro)\n",
      "    - sec:benefits: Benefits: Productivity, Cost Efficiency, and Business Value from LLMs (depends on: sec:background)\n",
      "    - sec:challenges: Technical and Operational Challenges: Scalability, Cost, and Integration (depends on: sec:background)\n",
      "    - sec:governance: Risks, Compliance, and Governance: Addressing Hallucinations, Data Leakage, and Regulation (depends on: sec:background, sec:challenges)\n",
      "    - sec:analysis: Analysis and Discussion: Synthesizing Benefits, Costs, and Strategic Considerations (depends on: sec:benefits, sec:challenges, sec:governance)\n",
      "    - sec:conclusion: Conclusion and Recommendations: Roadmap for Responsible LLM Adoption (depends on: sec:analysis)\n",
      "\n",
      "============================================================\n",
      "Phase 2: Node Expansion\n",
      "============================================================\n",
      "  [1/7] Expanding: sec:intro - Introduction: LLMs in the Enterprise Landscape\n",
      "      Generated 2446 chars\n",
      "  [2/7] Expanding: sec:background - Background and Context: What Large Language Models Are and How Enterprises Use Them\n",
      "      Generated 3163 chars\n",
      "  [3/7] Expanding: sec:benefits - Benefits: Productivity, Cost Efficiency, and Business Value from LLMs\n",
      "      Generated 2584 chars\n",
      "  [4/7] Expanding: sec:challenges - Technical and Operational Challenges: Scalability, Cost, and Integration\n",
      "      Generated 3021 chars\n",
      "  [5/7] Expanding: sec:governance - Risks, Compliance, and Governance: Addressing Hallucinations, Data Leakage, and Regulation\n",
      "      Generated 2937 chars\n",
      "  [6/7] Expanding: sec:analysis - Analysis and Discussion: Synthesizing Benefits, Costs, and Strategic Considerations\n",
      "      Generated 2725 chars\n",
      "  [7/7] Expanding: sec:conclusion - Conclusion and Recommendations: Roadmap for Responsible LLM Adoption\n",
      "      Generated 3051 chars\n",
      "\n",
      "============================================================\n",
      "Phase 3: Claim Extraction\n",
      "============================================================\n",
      "  Extracting claims from: sec:intro\n",
      "    Found 7 claims\n",
      "  Extracting claims from: sec:background\n",
      "    Found 8 claims\n",
      "  Extracting claims from: sec:benefits\n",
      "    Found 8 claims\n",
      "  Extracting claims from: sec:challenges\n",
      "    Found 8 claims\n",
      "  Extracting claims from: sec:governance\n",
      "    Found 8 claims\n",
      "  Extracting claims from: sec:analysis\n",
      "    Found 7 claims\n",
      "  Extracting claims from: sec:conclusion\n",
      "    Found 8 claims\n",
      "  Total claims: 54 (42 verified, 12 unverified)\n",
      "\n",
      "============================================================\n",
      "Phase 4: Structured Critique (Iteration 0)\n",
      "============================================================\n",
      "  Quality Score: 6.0/10\n",
      "  Issues found: 12\n",
      "    - Critical: 1\n",
      "    - Major: 9\n",
      "    - Minor: 2\n",
      "  Nodes to patch: ['sec:governance', 'sec:conclusion', 'sec:challenges', 'sec:intro', 'sec:analysis', 'sec:benefits', 'sec:background']\n",
      "\n",
      "--- Convergence Check ---\n",
      "  Iteration: 0/3\n",
      "  Quality: 6.0/7.5\n",
      "  Nodes to patch: 7\n",
      "  Cascade queue: 0\n",
      "  -> Continuing refinement.\n",
      "\n",
      "============================================================\n",
      "Phase 5: Targeted Retrieval\n",
      "============================================================\n",
      "  Researching for node: sec:intro\n",
      "    Query: enterprise ROI large language models study OR 'LLM...\n",
      "    Query: Vaswani transformer 2017 paper 'retrieval augmente...\n",
      "    Query: LLM impact KPIs 'time saved per task' 'ticket defl...\n",
      "    Found 18 results\n",
      "  Researching for node: sec:benefits\n",
      "    Query: enterprise ROI large language models study OR 'LLM...\n",
      "    Query: Vaswani transformer 2017 paper 'retrieval augmente...\n",
      "    Query: case study LLM customer service 'before after' 'ti...\n",
      "    Found 18 results\n",
      "  Researching for node: sec:challenges\n",
      "    Query: enterprise ROI large language models study OR 'LLM...\n",
      "    Query: Vaswani transformer 2017 paper 'retrieval augmente...\n",
      "    Query: LLM fine-tuning GPU hours cost 'inference cost per...\n",
      "    Found 18 results\n",
      "  Researching for node: sec:governance\n",
      "    Query: enterprise ROI large language models study OR 'LLM...\n",
      "    Query: Vaswani transformer 2017 paper 'retrieval augmente...\n",
      "    Query: GDPR guidance AI models data processing 'NIST AI R...\n",
      "    Found 18 results\n",
      "  Researching for node: sec:background\n",
      "    Query: Vaswani transformer 2017 paper 'retrieval augmente...\n",
      "    Query: Attention Is All You Need Vaswani 2017 transformer...\n",
      "    Found 12 results\n",
      "  Researching for node: sec:analysis\n",
      "    Query: enterprise LLM adoption roadmap 'AI governance roa...\n",
      "    Found 6 results\n",
      "  Researching for node: sec:conclusion\n",
      "    Query: enterprise LLM adoption roadmap 'AI governance roa...\n",
      "    Found 6 results\n",
      "  Total new evidence: 96 results\n",
      "  Claims updated to verified: 12\n",
      "\n",
      "============================================================\n",
      "Phase 6: Patch Application\n",
      "============================================================\n",
      "  Patching: sec:intro - Introduction: LLMs in the Enterprise Landscape\n",
      "    Patched: 4895 chars (revision #1)\n",
      "  Patching: sec:background - Background and Context: What Large Language Models Are and How Enterprises Use Them\n",
      "    Patched: 7685 chars (revision #1)\n",
      "  Patching: sec:challenges - Technical and Operational Challenges: Scalability, Cost, and Integration\n",
      "    Patched: 9292 chars (revision #1)\n",
      "  Patching: sec:benefits - Benefits: Productivity, Cost Efficiency, and Business Value from LLMs\n",
      "    Patched: 7403 chars (revision #1)\n",
      "  Patching: sec:governance - Risks, Compliance, and Governance: Addressing Hallucinations, Data Leakage, and Regulation\n",
      "    Patched: 7889 chars (revision #1)\n",
      "  Patching: sec:analysis - Analysis and Discussion: Synthesizing Benefits, Costs, and Strategic Considerations\n",
      "    Patched: 8930 chars (revision #1)\n",
      "  Patching: sec:conclusion - Conclusion and Recommendations: Roadmap for Responsible LLM Adoption\n",
      "    Patched: 5919 chars (revision #1)\n",
      "\n",
      "  Re-extracting claims from 7 patched nodes...\n",
      "    Removed 54 old claims from patched nodes\n",
      "    Extracted 53 new claims from patched nodes\n",
      "  Patched 7 nodes\n",
      "  Cascade queue: []\n",
      "\n",
      "============================================================\n",
      "Phase 4: Structured Critique (Iteration 1)\n",
      "============================================================\n",
      "  Quality Score: 7.0/10\n",
      "  Issues found: 13\n",
      "    - Critical: 0\n",
      "    - Major: 8\n",
      "    - Minor: 5\n",
      "  Nodes to patch: ['sec:governance', 'sec:conclusion', 'sec:challenges', 'sec:intro', 'sec:analysis', 'sec:benefits', 'sec:background']\n",
      "\n",
      "--- Convergence Check ---\n",
      "  Iteration: 1/3\n",
      "  Quality: 7.0/7.5\n",
      "  Nodes to patch: 7\n",
      "  Cascade queue: 0\n",
      "  -> Continuing refinement.\n",
      "\n",
      "============================================================\n",
      "Phase 5: Targeted Retrieval\n",
      "============================================================\n",
      "  Researching for node: sec:benefits\n",
      "    Query: enterprise ROI generative AI customer service hand...\n",
      "    Query: quantization inference speedup 'distillation' 'LLM...\n",
      "    Query: P95 latency LLM benchmark '7B' '13B' 'quantized' '...\n",
      "    Found 18 results\n",
      "  Researching for node: sec:challenges\n",
      "    Query: latency throughput engineering 'LLM' production co...\n",
      "    Query: quantization inference speedup 'distillation' 'LLM...\n",
      "    Query: P95 latency LLM benchmark '7B' '13B' 'quantized' '...\n",
      "    Found 18 results\n",
      "  Researching for node: sec:governance\n",
      "    Query: LLM governance risk mitigation matrix 'hallucinati...\n",
      "    Query: quantization inference speedup 'distillation' 'LLM...\n",
      "    Found 12 results\n",
      "  Researching for node: sec:analysis\n",
      "    Query: LLM total cost of ownership model cloud vs on-prem...\n",
      "    Found 6 results\n",
      "  Researching for node: sec:intro\n",
      "    Query: quantization inference speedup 'distillation' 'LLM...\n",
      "    Found 6 results\n",
      "  Researching for node: sec:background\n",
      "    Query: quantization inference speedup 'distillation' 'LLM...\n",
      "    Query: P95 latency LLM benchmark '7B' '13B' 'quantized' '...\n",
      "    Found 12 results\n",
      "  Total new evidence: 72 results\n",
      "  Claims updated to verified: 6\n",
      "\n",
      "============================================================\n",
      "Phase 6: Patch Application\n",
      "============================================================\n",
      "  Patching: sec:intro - Introduction: LLMs in the Enterprise Landscape\n",
      "    Patched: 8199 chars (revision #2)\n",
      "  Patching: sec:background - Background and Context: What Large Language Models Are and How Enterprises Use Them\n",
      "    Patched: 9746 chars (revision #2)\n",
      "  Patching: sec:challenges - Technical and Operational Challenges: Scalability, Cost, and Integration\n",
      "    Patched: 11656 chars (revision #2)\n",
      "  Patching: sec:benefits - Benefits: Productivity, Cost Efficiency, and Business Value from LLMs\n",
      "    Patched: 9852 chars (revision #2)\n",
      "  Patching: sec:governance - Risks, Compliance, and Governance: Addressing Hallucinations, Data Leakage, and Regulation\n",
      "    Patched: 11997 chars (revision #2)\n",
      "  Patching: sec:analysis - Analysis and Discussion: Synthesizing Benefits, Costs, and Strategic Considerations\n",
      "    Patched: 11152 chars (revision #2)\n",
      "  Patching: sec:conclusion - Conclusion and Recommendations: Roadmap for Responsible LLM Adoption\n",
      "    Patched: 9961 chars (revision #2)\n",
      "\n",
      "  Re-extracting claims from 7 patched nodes...\n",
      "    Removed 53 old claims from patched nodes\n",
      "    Extracted 57 new claims from patched nodes\n",
      "  Patched 7 nodes\n",
      "  Cascade queue: []\n",
      "\n",
      "============================================================\n",
      "Phase 4: Structured Critique (Iteration 2)\n",
      "============================================================\n",
      "  Quality Score: 7.0/10\n",
      "  Issues found: 13\n",
      "    - Critical: 0\n",
      "    - Major: 9\n",
      "    - Minor: 4\n",
      "  Nodes to patch: ['sec:governance', 'sec:conclusion', 'sec:challenges', 'sec:intro', 'sec:analysis', 'sec:benefits', 'sec:background']\n",
      "\n",
      "--- Convergence Check ---\n",
      "  Iteration: 2/3\n",
      "  Quality: 7.0/7.5\n",
      "  Nodes to patch: 7\n",
      "  Cascade queue: 0\n",
      "  -> Continuing refinement.\n",
      "\n",
      "============================================================\n",
      "Phase 5: Targeted Retrieval\n",
      "============================================================\n",
      "  Researching for node: sec:intro\n",
      "    Query: enterprise adoption statistics large language mode...\n",
      "    Query: independent studies enterprise LLM impact 'time sa...\n",
      "    Query: survey 'enterprise use of large language models' 2...\n",
      "    Found 18 results\n",
      "  Researching for node: sec:benefits\n",
      "    Query: enterprise adoption statistics large language mode...\n",
      "    Query: independent studies enterprise LLM impact 'time sa...\n",
      "    Query: case study 'LLM' 'time saved' 'search to answer' '...\n",
      "    Found 15 results\n",
      "  Researching for node: sec:challenges\n",
      "    Query: enterprise adoption statistics large language mode...\n",
      "    Query: LLM production architecture decision framework 'la...\n",
      "    Found 12 results\n",
      "  Researching for node: sec:conclusion\n",
      "    Query: enterprise adoption statistics large language mode...\n",
      "    Query: AI adoption roadmap 'pilot to production' 'LLM dep...\n",
      "    Found 12 results\n",
      "  Researching for node: sec:background\n",
      "    Query: independent studies enterprise LLM impact 'time sa...\n",
      "    Query: LLM latency benchmark 7B 13B P95 'H100' 'A100' 'ba...\n",
      "    Found 12 results\n",
      "  Researching for node: sec:governance\n",
      "    Query: LLM governance 'hallucination metric' 'factuality ...\n",
      "    Found 6 results\n",
      "  Researching for node: sec:analysis\n",
      "    Query: LLM TCO scenario analysis 'cost per million tokens...\n",
      "    Found 6 results\n",
      "  Total new evidence: 81 results\n",
      "  Claims updated to verified: 14\n",
      "\n",
      "============================================================\n",
      "Phase 6: Patch Application\n",
      "============================================================\n",
      "  Patching: sec:intro - Introduction: LLMs in the Enterprise Landscape\n",
      "    Patched: 11125 chars (revision #3)\n",
      "  Patching: sec:background - Background and Context: What Large Language Models Are and How Enterprises Use Them\n",
      "    Patched: 11431 chars (revision #3)\n",
      "  Patching: sec:challenges - Technical and Operational Challenges: Scalability, Cost, and Integration\n",
      "    Patched: 13287 chars (revision #3)\n",
      "  Patching: sec:benefits - Benefits: Productivity, Cost Efficiency, and Business Value from LLMs\n",
      "    Patched: 14251 chars (revision #3)\n",
      "  Patching: sec:governance - Risks, Compliance, and Governance: Addressing Hallucinations, Data Leakage, and Regulation\n",
      "    Patched: 13607 chars (revision #3)\n",
      "  Patching: sec:analysis - Analysis and Discussion: Synthesizing Benefits, Costs, and Strategic Considerations\n",
      "    Patched: 14028 chars (revision #3)\n",
      "  Patching: sec:conclusion - Conclusion and Recommendations: Roadmap for Responsible LLM Adoption\n",
      "    Patched: 14201 chars (revision #3)\n",
      "\n",
      "  Re-extracting claims from 7 patched nodes...\n",
      "    Removed 57 old claims from patched nodes\n",
      "    Extracted 55 new claims from patched nodes\n",
      "  Patched 7 nodes\n",
      "  Cascade queue: []\n",
      "\n",
      "============================================================\n",
      "Phase 4: Structured Critique (Iteration 3)\n",
      "============================================================\n",
      "  Quality Score: 7.0/10\n",
      "  Issues found: 14\n",
      "    - Critical: 0\n",
      "    - Major: 10\n",
      "    - Minor: 4\n",
      "  Nodes to patch: ['sec:governance', 'sec:conclusion', 'sec:challenges', 'sec:intro', 'sec:analysis', 'sec:benefits', 'sec:background']\n",
      "\n",
      "--- Convergence Check ---\n",
      "  Iteration: 3/3\n",
      "  Quality: 7.0/7.5\n",
      "  Nodes to patch: 7\n",
      "  Cascade queue: 0\n",
      "  -> Max iterations reached. Finalizing.\n",
      "\n",
      "============================================================\n",
      "Final Assembly\n",
      "============================================================\n",
      "  Document length: 100350 chars (11863 words)\n",
      "  Sections: 7\n",
      "  Total iterations: 3\n",
      "  Quality progression: 6.0 -> 7.0 -> 7.0 -> 7.0\n",
      "  Claims: 41/55 verified\n",
      "  Sources: 123\n",
      "\n",
      "================================================================================\n",
      "FINAL REPORT\n",
      "================================================================================\n",
      "# Research Report\n",
      "**Thesis:** Large language models deliver substantial productivity, personalization, and decision-support benefits for enterprises, but realizing their value at scale requires addressing technical, operational, and governance challenges—particularly cost, integration, reliability, and compliance—through tailored engineering, processes, and oversight.\n",
      "\n",
      "## Introduction: LLMs in the Enterprise Landscape\n",
      "\n",
      "We open at a pivotal moment: large language models (LLMs) are moving rapidly beyond research prototypes and consumer chatbots into enterprise pilots and production efforts, but the pace, channels of value, and operational demands vary across organizations and require careful, evidence‑based specification. This report asks a focused question: What are the key benefits and challenges of using large language models in enterprise applications, and how should business and engineering teams make deployment decisions? That question matters because multiple public sources now document both rapid uptake of generative-AI capabilities and important caveats about pilot success and infrastructure needs. For example, aggregated adoption summaries report that roughly two‑thirds of organizations were using generative AI in 2024 (≈67%) and that enterprise AI adoption rose substantially in 2024 (McKinsey figures summarized in an adoption compilation: https://www.typedef.ai/resources/llm-adoption-statistics; corroborating market summaries: https://www.hostinger.com/tutorials/llm-statistics). Industry analysis also predicts that a material share of future API demand will be driven by AI and LLM tools (Gartner forecast: https://www.gartner.com/en/newsroom/press-releases/2024-03-20-gartner-predicts-more-than-30-percent-of-the-increase-in-demand-for-apis-will-come-from-ai-and-tools-using-llms-by-2026). At the same time, independent analyses and large-sample studies highlight that many pilots fail to scale—underscoring infrastructure and organizational gaps (see discussion and sources summarized in the adoption compilation: https://www.typedef.ai/resources/llm-adoption-statistics; and sector analyses: https://www.sciencedirect.com/science/article/pii/S2666389925002144). Where direct public evidence is limited or comes from vendor case studies, this report flags those sources and either corroborates them with open-access benchmarks or treats results as illustrative and subject to sensitivity analysis.\n",
      "\n",
      "Which KPIs matter and where is impact observed? Synthesis of open surveys, public case summaries, and benchmarking work indicates common enterprise KPIs: time saved per task (e.g., drafting, summarization), ticket deflection / percent of customer interactions resolved without human escalation, average handle time and first-contact resolution in support, content production throughput, and reductions in decision latency for analytical workflows. Public and practitioner‑facing summaries document examples such as 40%+ reductions in content-creation time or substantial improvements in triage throughput (illustrative summaries and use-case roundups: https://www.assemblyai.com/blog/llm-use-cases; https://www.secondtalent.com/resources/domain-generative-ai-llm-usage-statistics). These operational gains are reported unevenly across sources: some are vendor case studies (useful examples but potentially biased), while others are independent academic or large-sample studies (used here when available; see the systematic use and adoption analyses: https://www.sciencedirect.com/science/article/pii/S2666389925002144). When we rely on vendor or proprietary reports (for instance, some Gartner benchmarks), we explicitly summarize the underlying methodology when available and show uncertainty bounds or alternative scenarios to reflect potential bias (Gartner summary: https://www.gartner.com/en/documents/5150331; adoption aggregation: https://www.typedef.ai/resources/llm-adoption-statistics).\n",
      "\n",
      "How is value typically realized? Open-access reviews and practitio...\n",
      "\n",
      "================================================================================\n",
      "Report length: 100350 characters\n",
      "Total iterations: 3\n",
      "Quality progression: [6.0, 7.0, 7.0, 7.0]\n",
      "Unique sources: 123\n",
      "Skeleton nodes: 7\n",
      "Claims tracked: 55\n",
      "Agent test PASSED\n"
     ]
    }
   ],
   "source": [
    "# Simple test\n",
    "test_question = \"What are the key benefits and challenges of using large language models in enterprise applications?\"\n",
    "\n",
    "print(f\"Testing Patch-Based Iterative Refinement Agent\")\n",
    "print(f\"Question: {test_question}\")\n",
    "print(\"\\nRunning patch-based research (this may take several minutes)...\\n\")\n",
    "\n",
    "try:\n",
    "    # Use the async version in notebooks to avoid event loop conflicts\n",
    "    result = await patch_refinement_agent_async({\"question\": test_question})\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FINAL REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    print(result[\"output\"][:4000] + \"...\" if len(result[\"output\"]) > 4000 else result[\"output\"])\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Report length: {len(result['output'])} characters\")\n",
    "    print(f\"Total iterations: {result.get('iteration_count', 0)}\")\n",
    "    print(f\"Quality progression: {result.get('quality_scores', [])}\")\n",
    "    print(f\"Unique sources: {len(set(result.get('source_urls', [])))}\")\n",
    "    print(f\"Skeleton nodes: {len(result.get('skeleton', {}).get('nodes', {}))}\")\n",
    "    print(f\"Claims tracked: {len(result.get('claims_registry', {}))}\")\n",
    "    print(\"Agent test PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"Agent test FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Research Report\n",
       "**Thesis:** Large language models deliver substantial productivity, personalization, and decision-support benefits for enterprises, but realizing their value at scale requires addressing technical, operational, and governance challenges—particularly cost, integration, reliability, and compliance—through tailored engineering, processes, and oversight.\n",
       "\n",
       "## Introduction: LLMs in the Enterprise Landscape\n",
       "\n",
       "We open at a pivotal moment: large language models (LLMs) are moving rapidly beyond research prototypes and consumer chatbots into enterprise pilots and production efforts, but the pace, channels of value, and operational demands vary across organizations and require careful, evidence‑based specification. This report asks a focused question: What are the key benefits and challenges of using large language models in enterprise applications, and how should business and engineering teams make deployment decisions? That question matters because multiple public sources now document both rapid uptake of generative-AI capabilities and important caveats about pilot success and infrastructure needs. For example, aggregated adoption summaries report that roughly two‑thirds of organizations were using generative AI in 2024 (≈67%) and that enterprise AI adoption rose substantially in 2024 (McKinsey figures summarized in an adoption compilation: https://www.typedef.ai/resources/llm-adoption-statistics; corroborating market summaries: https://www.hostinger.com/tutorials/llm-statistics). Industry analysis also predicts that a material share of future API demand will be driven by AI and LLM tools (Gartner forecast: https://www.gartner.com/en/newsroom/press-releases/2024-03-20-gartner-predicts-more-than-30-percent-of-the-increase-in-demand-for-apis-will-come-from-ai-and-tools-using-llms-by-2026). At the same time, independent analyses and large-sample studies highlight that many pilots fail to scale—underscoring infrastructure and organizational gaps (see discussion and sources summarized in the adoption compilation: https://www.typedef.ai/resources/llm-adoption-statistics; and sector analyses: https://www.sciencedirect.com/science/article/pii/S2666389925002144). Where direct public evidence is limited or comes from vendor case studies, this report flags those sources and either corroborates them with open-access benchmarks or treats results as illustrative and subject to sensitivity analysis.\n",
       "\n",
       "Which KPIs matter and where is impact observed? Synthesis of open surveys, public case summaries, and benchmarking work indicates common enterprise KPIs: time saved per task (e.g., drafting, summarization), ticket deflection / percent of customer interactions resolved without human escalation, average handle time and first-contact resolution in support, content production throughput, and reductions in decision latency for analytical workflows. Public and practitioner‑facing summaries document examples such as 40%+ reductions in content-creation time or substantial improvements in triage throughput (illustrative summaries and use-case roundups: https://www.assemblyai.com/blog/llm-use-cases; https://www.secondtalent.com/resources/domain-generative-ai-llm-usage-statistics). These operational gains are reported unevenly across sources: some are vendor case studies (useful examples but potentially biased), while others are independent academic or large-sample studies (used here when available; see the systematic use and adoption analyses: https://www.sciencedirect.com/science/article/pii/S2666389925002144). When we rely on vendor or proprietary reports (for instance, some Gartner benchmarks), we explicitly summarize the underlying methodology when available and show uncertainty bounds or alternative scenarios to reflect potential bias (Gartner summary: https://www.gartner.com/en/documents/5150331; adoption aggregation: https://www.typedef.ai/resources/llm-adoption-statistics).\n",
       "\n",
       "How is value typically realized? Open-access reviews and practitioner benchmarks indicate that adapting pretrained transformer models to domain data—through fine-tuning, prompt engineering, or retrieval-augmented approaches—usually accelerates time-to-value relative to training from scratch and improves relevance for internal workflows (technical and synthesis review: https://www.arxiv.org/pdf/2511.18589). Surveys and benchmarks from 2024–2025 report that organizations combining model adaptation with tight systems integration and monitoring see faster operational gains (adoption summaries and case roundups: https://www.typedef.ai/resources/llm-adoption-statistics; practitioner overviews: https://www.assemblyai.com/blog/llm-use-cases). Where quantitative before/after KPI data are available in the public record, they commonly show reduced handle time and higher throughput; when such data come from vendors, this report treats them as illustrative and contrasts them with independently measured benchmarks where possible.\n",
       "\n",
       "What are the operational and cost constraints? Producing reliable, production‑grade LLM services exposes enterprises to infrastructure and lifecycle demands—compute and storage costs, latency vs. model-size trade-offs, throughput engineering, energy consumption, and integration friction with legacy systems. These constraints have been quantified in open benchmarking and technical studies: energy and power-consumption sensitivity to sequence length, batch size, and architecture are analyzed in academic preprints (energy and inference cost analysis: https://arxiv.org/pdf/2512.03024), and practical inference benchmarking guides (e.g., NVIDIA’s LLM inference and benchmarking recommendations) provide concrete measurement methods and tooling that organizations can adopt (NVIDIA GenAI-Perf guidance: https://developer.nvidia.com/blog/llm-inference-benchmarking-how-much-does-your-llm-inference-cost/). Put concretely: sub-second response goals, high-concurrency serving, or strict on‑premises data controls typically require explicit architectural choices (smaller/quantized models, hybrid edge–cloud deployments, or specialized serving stacks) and incur additional engineering and cost trade-offs—an observation supported by independent synthesis and practitioner analyses (https://www.arxiv.org/pdf/2511.18589; see also market and analyst summaries: https://www.gartner.com/en/documents/7137530).\n",
       "\n",
       "Which engineering optimizations materially change the economics? Public practitioner guides and benchmarks quantify how common inference optimizations affect throughput and cost. Quantization and distillation reduce memory footprint and latency, improving per‑token throughput and lowering hardware costs (practical guides and summaries: https://deepsense.ai/case-studies/smarter-customer-support-at-lower-cost-automation-that-scales/; https://deepsense.ai/blog/llm-inference-optimization-how-to-speed-up-cut-costs-and-scale-ai-models/). Smaller‑model scaling and combined sparsity/quantization approaches have shown measurable per‑token speedups in practitioner reports (example scaling analysis: https://quantumzeitgeist.com/efficient-llm-inference-achieves-speedup/). Unit-economics histories and practitioner summaries document steep declines in average inference prices over recent years while emphasizing that realized costs depend strongly on GPU utilization, batching, and system engineering (practitioner unit-economics discussion: https://introl.com/blog/inference-unit-economics-true-cost-per-million-tokens-guide). In later sections we convert these concrete magnitudes (throughput improvements, per-token cost reductions, and energy impacts) into TCO casework and sensitivity bounds rather than using them as unsupported generalities.\n",
       "\n",
       "Risk, governance, and evidence credibility: Hallucination (incorrect outputs), data privacy and IP leakage, and model lifecycle and drift are material enterprise risks. Independent governance frameworks and surveys emphasize that monitoring, validation, and process controls are necessary to capture LLM benefits while limiting compliance and business exposure (governance and risk synthesis: https://www.arxiv.org/pdf/2511.18589). Because many high-visibility ROI examples come from vendor or consultant reports, this report supplements such anecdotes with open benchmarks and academic studies where possible, documents the provenance and likely biases of proprietary sources, and applies sensitivity analyses to show how recommendations change under different evidence assumptions.\n",
       "\n",
       "Terminology, scope, and where to find the formal definitions: To reduce ambiguity, this report uses the term “LLM” to refer to transformer-based autoregressive or encoder–decoder models that are pretrained at scale and typically exceed ~1 billion parameters—aligning with common practitioner usage and the scale at which many inference and capability trade-offs become pronounced. Where analysis includes smaller models (100M–1B parameters) we flag those cases and show sensitivity to model size. The Background section contains the formal, consolidated definition and a short technical primer on common adaptation patterns (fine-tuning, prompting, retrieval‑augmentation); the Introduction therefore presents only the operational scope and key assumptions that drive subsequent TCO and benefits analyses.\n",
       "\n",
       "Guiding thesis and how this report will address prior unevenness: LLMs can deliver substantial productivity, personalization, and decision‑support benefits for enterprises, but realizing that value at scale requires addressing technical, operational, and governance challenges—particularly cost, integration, reliability, and compliance—through tailored engineering, processes, and oversight. To strengthen the evidentiary basis relative to earlier drafts, the rest of the report (a) quantifies benefits with illustrative before/after KPI examples and uncertainty bounds grounded in public benchmarks and documented casework; (b) presents TCO casework that converts throughput, latency, and optimization gains into cost‑per‑million‑token and infrastructure scenarios using open benchmarking methods (e.g., NVIDIA GenAI‑Perf guidance: https://developer.nvidia.com/blog/llm-inference-benchmarking-how-much-does-your-llm-inference-cost/ and academic sensitivity analyses: https://arxiv.org/pdf/2512.03024); and (c) operationalizes governance by mapping common risks to concrete controls, metrics, and SLOs (governance synthesis: https://www.arxiv.org/pdf/2511.18589). Where we must summarize proprietary sources (for example, certain Gartner findings), we explicitly state the original work’s paywalled status, summarize the underlying metrics and methods when disclosed (Gartner press release and reports: https://www.gartner.com/en/newsroom/press-releases/2024-03-20-gartner-predicts-more-than-30-percent-of-the-increase-in-demand-for-apis-will-come-from-ai-and-tools-using-llms-by-2026; https://www.gartner.com/en/documents/5150331), and corroborate or bound those findings using open-access evidence and sensitivity scenarios (adoption aggregations: https://www.typedef.ai/resources/llm-adoption-statistics; academic syntheses: https://www.arxiv.org/pdf/2511.18589).\n",
       "\n",
       "Taken together, the technical foundation (Transformer-era models and common optimization techniques) and the synthesized practitioner evidence (open and proprietary where explicitly labeled and corroborated) will be used to synthesize trade-offs into decision frameworks and actionable recommendations for business leaders and engineering teams. When claims are only supported by vendor material or low‑quality aggregations, we (a) label them as preliminary or illustrative, (b) triangulate with at least one independent source where possible, and (c) present sensitivity analyses so readers can see how recommendations change under alternate, more conservative assumptions. The prior section argued that LLMs offer high potential but introduce operational complexity; this section defines what we mean by LLMs, summarizes their core architectures and capabilities, explains common deployment and measurement patterns, and adds practical benchmarking and optimization evidence so the later benefits and cost analyses rest on an explicit, evidence‑based foundation.\n",
       "\n",
       "## Background and Context: What Large Language Models Are and How Enterprises Use Them\n",
       "\n",
       "The prior section argued that LLMs offer high potential but introduce operational complexity; this section now gives a single, consolidated operational definition of “LLM,” summarizes core architectures and adaptation patterns, explains how enterprises typically deploy and measure LLMs, and supplements earlier vendor claims with open‑access benchmarks and explicit contextual conditions so later cost/benefit analysis rests on transparent, reproducible evidence. Operational definition (consolidated). For this report, a “large language model (LLM)” means a transformer‑family model (autoregressive or encoder–decoder) that is pretrained at scale and exhibits scale‑dependent performance benefits in generalization and few‑shot behavior. Practically we treat LLMs as models with roughly ≥1 billion parameters, a threshold more consistent with current practitioner usage and performance inflection points (scaling‑law studies such as Kaplan et al., 2020 show gradual improvements with scale). We note sensitivity to this choice: many cited latency, cost, and capability claims change materially for sub‑1B models, so where analysis uses smaller models we will call that out explicitly. The pretraining→adaptation lifecycle remains: large‑scale unsupervised or self‑supervised pretraining on broad corpora followed by downstream alignment via supervised fine‑tuning, instruction tuning, or retrieval/engineering strategies (e.g., InstructGPT/Ouyang et al., 2022).\n",
       "\n",
       "Core architecture and pretraining. Most contemporary LLMs are Transformer variants (attention, self‑attention, positional encodings; Vaswani et al., 2017: https://arxiv.org/abs/1706.03762). Large pretraining produces broadly useful representations; subsequent adaptation (fine‑tuning/instruction tuning/RAG) aligns model behavior to task and safety objectives. Scaling laws explain why larger models and more data frequently improve downstream performance but also drive operational trade‑offs (latency, memory, cost) that must be measured in context (Kaplan et al., 2020).\n",
       "\n",
       "Multimodality. LLM families increasingly accept image, audio, and structured inputs, enabling cross‑modal retrieval, captioning, and multimodal assistants (surveys and models such as CLIP: https://arxiv.org/abs/2103.00020). Enterprises should assess whether multimodal capabilities materially affect task ROI (e.g., image + text customer evidence versus text only).\n",
       "\n",
       "Adaptation patterns, failure modes, and controls (consolidated). Enterprises commonly rely on three adaptation approaches; below we summarize operational implications and typical failure modes with controls.\n",
       "- Fine‑tuning (updating weights with domain data). Pros: can embed domain behavior, reduce per‑call dependence on external retrieval, and produce deterministic tendencies for covered examples. Failure modes: overfitting to narrow training distributions, catastrophic forgetting of prior capabilities, accidental leakage of sensitive training data. Controls: holdout/validation sets, adversarial and OOD testing, continual learning pipelines with staged rollouts, data‑handling audits, and model versioning with rollback plans.\n",
       "- Retrieval‑Augmented Generation (RAG: retriever + vector index + generator). Pros: near‑instant knowledge updates without retraining, easier provenance (source links), and lower event‑scale retraining costs. Failure modes: retriever recall or relevance failures, stale indexes, and mismatched grounding leading to hallucination. Controls: periodic retriever recall evaluation, index freshness monitoring, provenance linking in outputs, and combining RAG with lightweight local fine‑tuning for high‑precision tasks.\n",
       "- Prompt engineering / in‑context learning. Pros: low infra cost and fast iteration. Failure modes: prompt drift, brittleness to phrasing changes, and difficulty certifying behavior in regulated environments. Controls: prompt change monitoring, automated regression tests, and human‑in‑the‑loop gates for critical decisions.\n",
       "\n",
       "Hallucination and factuality measurement. Hallucination denotes fluent but incorrect/unverifiable outputs. Teams measure factuality via automated benchmarks, targeted QA tests, and human annotation protocols (Maynez et al., 2020: https://arxiv.org/abs/2005.00661). Operational mitigations include grounding (RAG), post‑generation verification, human review, metricized acceptance thresholds, and instrumented provenance. For regulated workflows near‑zero factual error tolerances should translate into SLOs with human‑approval gates and conservative acceptance thresholds.\n",
       "\n",
       "Inference latency, throughput, and the contextual conditions that matter. Latency = request→response time; throughput = requests/tokens per second. Percentile SLIs (P50/P95/P99) are standard. Empirical latency/throughput numbers are highly sensitive to hardware, model size, serving stack, batch sizes, sequence lengths (input/output), numeric precision, and decoding settings. Representative, reproducible observations from open and practitioner benchmarks:\n",
       "- Small/interactive models (≈7B–13B): with modern GPU stacks and inference optimizations, P95 latencies in the low‑hundreds of milliseconds are achievable for short outputs, but only under specified conditions—typically H100/A100‑class GPUs, single‑user decoding or small batch sizes (batch=1–4), short output tokens (≈≤32), and optimized runtimes (TensorRT‑LLM, vLLM). For example, independent benching of Mistral 7B using FP8+TensorRT on H100 reports best‑in‑class time‑to‑first‑token and high tokens/sec for short sequences (Baseten: https://www.baseten.co/blog/benchmarking-fast-mistral-7b-inference). Databricks’ guidance and Koyeb’s controlled GPU benchmarks similarly show that latency depends strongly on batch size and output length (Databricks: https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices; Koyeb: https://www.koyeb.com/docs/hardware/gpu-benchmarks).\n",
       "- Serving framework differences (vLLM vs TGI): open‑access benchmarks (arXiv) demonstrate vLLM often achieves higher throughput for batched workloads whereas TGI (Text Generation Inference) can have lower single‑user latency; choice depends on expected concurrency and batchability (Performance Study: https://arxiv.org/html/2511.17593v1). Hathora’s profiling likewise highlights how input/decoding stages affect memory and per‑token latency (https://blog.hathora.dev/a-deep-dive-into-llm-inference-latencies/).\n",
       "- Larger models (20B+): typically require multi‑GPU sharding, higher memory bandwidth, or model‑parallel strategies to approach interactive SLAs; expected latency and operational complexity rise substantially (Databricks guidance; Hathora memory analysis).\n",
       "\n",
       "Given variance across vendor claims, we recommend treating vendor numbers as conditional estimates: include explicit hardware, precision (FP16/BF16/FP8/4‑bit), batch size, sequence lengths, stack (vLLM/TGI/TensorRT), and percentile when recording any latency/throughput figure. Where vendor data is used (e.g., NVIDIA GenAI‑Perf), we summarize the methodology (benchmark scripts, workload shapes, hardware) and corroborate with at least one independent source or apply sensitivity bounds (±20–50% depending on workload variance) in TCO calculations.\n",
       "\n",
       "Inference optimization techniques and quantified effects. Common optimizations include quantization (4‑bit/8‑bit), distillation, batching, KV caching, and sparsity. Practitioner guides and independent tests show substantial gains: 4‑bit quantization and distillation materially reduce memory use and cost while improving tokens/sec (community reports and guides, e.g., deepsense: https://deepsense.ai/blog/llm-inference-optimization-how-to-speed-up-cut-costs-and-scale-ai-models/). Reported speedups vary by model and stack—examples include ≈1.3×–3× improvements depending on precision and runtime; use per‑project benchmarking to pin values to your workload.\n",
       "\n",
       "Unit economics and energy. Token costs and energy per token have dropped rapidly, but absolute costs depend on model size, sequence length, and batching. Practitioner summaries (Introl: https://introl.com/blog/inference-unit-economics-true-cost-per-million-tokens-guide) should be cross‑checked with open energy benchmarking research (https://arxiv.org/pdf/2512.03024) and per‑project profiling to generate reliable TCO estimates.\n",
       "\n",
       "Deployment patterns and trade‑offs. Common enterprise patterns are: (a) cloud managed APIs (fastest adoption, lowest ops), (b) hybrid (sensitive or latency‑critical components on private infra with cloud for bursts), and (c) edge/on‑device (ultra‑low latency or strict privacy). Analyst summaries show a shift from pilots to hybrid production deployments; where analyst reports are paywalled, we summarize their methodology (surveys, customer interviews, spend data) and corroborate with public case studies and practitioner writeups (McKinsey/Gartner summaries accompanied by open case studies and independent blog analyses).\n",
       "\n",
       "Measuring reliability for SLAs and governance. Operationalize risk via measurable SLIs: factuality/error rate (annotated sample failure rate), latency percentiles (P50/P95/P99), throughput (RPS/tokens/sec), and availability. Regulated domains require documented provenance, near‑zero factual error SLOs, and curated human‑review workflows. Later governance sections map these SLIs to controls (automated verifiers, escalation playbooks, and SLO remediation).\n",
       "\n",
       "How technical metrics map to business KPIs (brief bridge to benefits). To connect this technical background to business value: latency and availability influence customer experience and time‑saved per interaction (affecting CSAT and ticket resolution time); factuality and provenance affect compliance risk and allowable automation scope (defining which workflows can be fully automated versus human‑in‑the‑loop); throughput and unit economics determine cost per transaction and scalability (impacting TCO and ROI); and adaptivity (how quickly knowledge can be updated via RAG or retraining) directly maps to deflection rates and process automation velocity.\n",
       "\n",
       "Empirical examples of operational gains (new evidence). Practitioner and independent sources report material operational improvements when LLMs are embedded into workflows. Representative findings include AssemblyAI’s synopsis of LLM use cases and reported efficiencies (e.g., content creation, document processing): https://www.assemblyai.com/blog/llm-use-cases; Deepsense case study on Tier‑1 support automation (process improvements and cost reductions): https://deepsense.ai/case-studies/smarter-customer-support-at-lower-cost-automation-that-scales/; and multiple practitioner writeups on automated ticket triage and ITSM improvements (e.g., Analytics Vidhya, TigerAnalytics, Intelliarts). These practitioner reports are valuable but heterogeneous in measurement rigor; where we use such claims in benefit quantification (next section) we will: (a) describe source methodology, (b) corroborate with at least one independent benchmark or case study, and (c) include uncertainty bounds to reflect measurement differences.\n",
       "\n",
       "Practical guidance and rules of thumb (preserved). Based on the above and reproducible benchmarks: pilot with RAG for knowledge‑heavy, frequently changing corpora; prefer fine‑tuning when latency and determinism are critical and you can sustain retraining/validation costs; expect 7B–13B models to meet many interactive use cases on modern GPUs (A100/H100/L40S) with optimizations (4‑bit quantization, TensorRT/vLLM stacks), but always validate with end‑to‑end benchmarks that include tokenization, retrieval, network overhead, and P95/P99 percentiles. Use vendor and community benchmarking tools (NVIDIA GenAI‑Perf, Koyeb, Baseten, and arXiv performance studies) to derive provisioning and TCO inputs, and apply sensitivity bounds where vendor numbers differ from independent measurements. With the technical definitions, deployment patterns, optimization techniques, and contextualized benchmarks established here (and with vendor claims explicitly summarized and cross‑checked against open sources), the next section quantifies the business upside organizations report when LLMs are embedded in production workflows and explains the evidence collection, inclusion/exclusion criteria, and how vendor reports are weighted relative to independent benchmarks and validated case studies.\n",
       "\n",
       "## Benefits: Productivity, Cost Efficiency, and Business Value from LLMs\n",
       "\n",
       "Building on the technical definitions, deployment patterns, optimization strategies, and benchmark context established previously (with vendor claims explicitly summarized and cross‑checked against open sources), this section quantifies the business upside organizations report when LLMs are embedded in production workflows, explains the evidence and weighting methodology used to reach those quantifications, and closes prior gaps where claims were previously unverified. Methods and evidence weighting. To produce actionable, defensible ranges we synthesized (1) independent, open‑access studies and public surveys, (2) industry research and analyst syntheses (used when underlying methodology is described), and (3) vendor and practitioner case studies. Inclusion criteria prioritized independent or publicly available studies with explicit measurement methods; proprietary vendor case studies are retained as illustrative examples but are explicitly labeled and down‑weighted when estimating central ranges. Where sources conflict we report ranges and indicate uncertainty drivers (sample size, methodology, potential vendor bias). Representative public sources cited below include (but are not limited to) industry surveys and adoption summaries (typedef.ai compendium: https://www.typedef.ai/resources/llm-adoption-statistics), analyst forecasts (Gartner press release: https://www.gartner.com/en/newsroom/press-releases/2024-03-20-gartner-predicts-more-than-30-percent-of-the-increase-in-demand-for-apis-will-come-from-ai-and-tools-using-llms-by-2026), and sector analyses and case writeups (AssemblyAI use‑case summaries: https://www.assemblyai.com/blog/llm-use-cases; Deepsense customer case: https://deepsense.ai/case-studies/smarter-customer-support-at-lower-cost-automation-that-scales/). When relying on paywalled analyst reports (e.g., Gartner, McKinsey) we summarize methodology and seek at least one open corroborating source or present widened uncertainty bounds to reflect limited reproducibility.\n",
       "\n",
       "Explicitly resolving previously unverified claims. Several claims in the prior draft were flagged as unverified (pace of adoption, primary value channels, and necessity of specific scaling actions). We updated these with publicly accessible evidence or added transparent caveats where independent data are limited:\n",
       "- Pace of adoption: Multiple public summaries report rapid enterprise uptake of generative AI/LLM usage in 2024. For example, compilations citing McKinsey/Iopex put generative AI adoption at ~67–78% of organizations in 2024 (typedef.ai: https://www.typedef.ai/resources/llm-adoption-statistics; Hostinger summary: https://www.hostinger.com/tutorials/llm-statistics). We therefore present a central adoption indicator of ~67% (2024) with a caveat that methodology and sampling differ across sources; where reliance on a single proprietary datapoint remains (e.g., some McKinsey tables), we label it explicitly. \n",
       "- API demand and scaling pressure: Gartner projects >30% of new API demand driven by AI/LLM tooling by 2026 (https://www.gartner.com/en/newsroom/press-releases/2024-03-20-gartner-predicts-more-than-30-percent-of-the-increase-in-demand-for-apis-will-come-from-ai-and-tools-using-llms-by-2026), supporting the assertion that inference scale and integration engineering are necessary planning considerations. We treat Gartner’s projection as directional and corroborate the implication with public developer and market reports describing rapid API call growth (see typedef.ai and public market summaries above). \n",
       "- Primary value channels: The claim that LLMs create measurable business value primarily through automation (routine text tasks), augmentation (agent assist), and knowledge retrieval is supported by multiple public case writeups and surveys documenting use cases and quantified impacts (AssemblyAI use cases: https://www.assemblyai.com/blog/llm-use-cases; Deepsense case study: https://deepsense.ai/case-studies/smarter-customer-support-at-lower-cost-automation-that-scales/). Where high‑quality, peer‑reviewed quantification was unavailable for a specific channel we label the finding as preliminary and provide sensitivity ranges.\n",
       "\n",
       "High‑level thesis (refined and evidence‑annotated). LLMs create measurable business value via: (a) automation of repetitive text work (content generation, reporting), (b) faster time‑to‑insight through synthesis and summarization, (c) improved customer interactions and partial automation of support (triage, draft responses, agent assist), and (d) better knowledge management and search. These channels are supported by surveys and case reports showing broad adoption and use‑case prevalence (typedef.ai; AssemblyAI), and by targeted case studies demonstrating measurable KPI changes (Deepsense; vendor case writeups). The magnitude of impact is use‑case dependent and sensitive to data quality, integration effort, governance, and inference economics.\n",
       "\n",
       "Bridging technical metrics to business KPIs. To make the preceding technical discussion actionable for decision makers, we map a few key technical attributes to the KPIs used below:\n",
       "- Latency (end‑to‑end inference P95): maps to interactive throughput and customer satisfaction—sub‑second P95 is often required for agent‑assist and live chat scenarios; higher latency increases human wait times and reduces effective throughput.\n",
       "- Retriever recall and grounding quality: maps to accuracy‑sensitive KPIs such as first‑contact resolution (FCR), error rate, and human review burden—lower retriever recall increases the need for human oversight and reduces deflection.\n",
       "- Model adaptivity and alignment (fine‑tuning, RAG): maps to automation depth and deflection rates—better adaptation reduces human revisions per transaction and shortens agent training/onboarding.\n",
       "This explicit mapping explains how low‑level system choices influence the ROI levers we measure below.\n",
       "\n",
       "Observed effect sizes, confidence bounds, and sources (examples).\n",
       "- Customer service handle time and throughput. Public syntheses and case reports describe handle‑time reductions from agent assist, draft‑response, and triage workflows in the 20–50% range. For example, multiple enterprise writeups and practitioner summaries cite reductions around 30–50% for high‑volume text contact centers, while smaller or regulated centers report lower gains (AssemblyAI; Deepsense; public surveys compiled at typedef.ai). Confidence: medium–high for high‑volume, text‑centric centers with well‑integrated agent assist; lower where regulation or domain complexity raises human review. (Sources: typedef.ai: https://www.typedef.ai/resources/llm-adoption-statistics; Deepsense case: https://deepsense.ai/case-studies/smarter-customer-support-at-lower-cost-automation-that-scales/.)\n",
       "- Agent productivity and FCR. Benchmarks reported by analyst summaries and case studies indicate agent productivity and FCR improvements typically in the low‑to‑mid tens of percent for mature deployments; exact values depend on baseline FCR and integration maturity. We report a central plausible range of +10–30% FCR/productivity uplift, with medium confidence and explicit caveats about generalizability. (Sources: public analyst summaries and case notes: https://www.typedef.ai/resources/llm-adoption-statistics; Gartner press release: https://www.gartner.com/en/newsroom/press-releases/2024-03-20-gartner-predicts-more-than-30-percent-of-the-increase-in-demand-for-apis-will-come-from-ai-and-tools-using-llms-by-2026.)\n",
       "- Knowledge search and onboarding. Reported search‑to‑answer time reductions and onboarding speedups vary materially by dataset and instrumentation. Public case writeups report search‑to‑answer reductions in the roughly 20–60% range for enterprises that implement RAG/LLM retrieval for internal knowledge (Deepsense; AssemblyAI). We treat vendor‑reported upper bounds as illustrative and present a central range of 20–40% reduction with low‑to‑medium confidence unless corroborated by independent before/after measurement. (Sources: Deepsense case: https://deepsense.ai/case-studies/smarter-customer-support-at-lower-cost-automation-that-scales/; AssemblyAI: https://www.assemblyai.com/blog/llm-use-cases.)\n",
       "\n",
       "Representative illustrative ROI calculation (explicit assumptions and sensitivity). Using a contact center example—50 tickets/day, 10 minutes average handle time, $30/hr labor cost—the midpoint 35% handle‑time reduction (midpoint of 20–50% range after downgrading vendor‑only highs) reduces handling by 3.5 minutes/ticket, saving ~175 minutes (2.9 hours) per agent‑day = ~$87/day or ~$22k/year per FTE (excl. implementation/monitoring). Sensitivity: if realized reduction is 20% the annualized saving falls to ~$12.6k/FTE; at 50% it rises to ~31.6k/FTE. Source ranges used here are based on public case summaries and practitioner reports (typedef.ai; Deepsense; AssemblyAI). Label these as illustrative: actual ROI depends on ticket mix, human review fraction, throughput limits imposed by latency, and integration costs.\n",
       "\n",
       "Infrastructure, unit economics, and TCO signals (quantified). Unit inference economics have evolved rapidly. Practitioner analyses estimate that per‑million‑token costs moved from tens of dollars in 2022 to sub‑dollar levels with aggressive optimization and cheaper hardware in some settings; realizable TCO depends on GPU utilization, batching, quantization, and end‑to‑end engineering (practitioner analyses: Introl: https://introl.com/blog/inference-unit-economics-true-cost-per-million-tokens-guide; NVIDIA benchmarking guidance: https://developer.nvidia.com/blog/llm-inference-benchmarking-how-much-does-your-llm-inference-cost/). We therefore recommend converting per‑token estimates into transaction‑level TCO using measured utilization and latency targets from pilot benchmarks rather than quoting vendor headline figures.\n",
       "\n",
       "Optimization effects on throughput and memory. Quantization, distillation, and sparsity yield measurable runtime and memory improvements in practitioner benchmarks (examples: quantization/sparsity experiments and synthesis guides: https://quantumzeitgeist.com/efficient-llm-inference-achieves-speedup/; https://deepsense.ai/blog/llm-inference-optimization-how-to-speed-up-cut-costs-and-scale-ai-models/). NVIDIA and other vendors document that optimized stacks, KV‑cache usage, and batching materially change GPU utilization curves used in TCO calculations (https://developer.nvidia.com/blog/llm-inference-benchmarking-how-much-does-your-llm-inference-cost/). We report typical practitioner speedups (single‑digit to low‑double digit percent throughput gains) but caution that results vary by model family and workload; always benchmark with realistic payloads.\n",
       "\n",
       "Latency thresholds and practical provisioning guidance. Empirically, optimized inference stacks can meet sub‑second end‑user response times for small‑to‑mid LLMs: practitioners report P95 request latencies often under 100–300 ms for 7B–13B models on modern GPU hardware with FP16/BF16 and quantization; larger models or long contexts often require higher‑tier instances or parallelism strategies (practitioner benchmarks and NVIDIA guidance: https://developer.nvidia.com/blog/llm-inference-benchmarking-how-much-does-your-llm-inference-cost/). For planning we recommend explicit pilot benchmarks: assume that achieving sub‑second (P95) interactive responses typically requires either (a) a 7B–13B model with quantization/engine optimizations on A100/RTX‑class hardware, or (b) model offloading/caching and sequence‑parallel strategies for larger models.\n",
       "\n",
       "Four primary ROI levers and how to measure them (operationalized metrics).\n",
       "1) Reduced manual effort per transaction: measure baseline and post‑deployment minutes/hours saved per ticket or document; instrument with A/B or staggered rollout where possible.\n",
       "2) Faster time‑to‑value: measure calendar days from pilot start to live traffic and track friction points during integration (data ingestion, retriever building, safety pipelines).\n",
       "3) Lower operational cost per transaction: convert per‑token and per‑request benchmarking into cost per resolved ticket, normalizing by GPU utilization and human‑in‑the‑loop rates.\n",
       "4) Faster time‑to‑insight: measure analyst hours saved, time to report generation, and decision latency reductions.\n",
       "\n",
       "Contexts and uncertainties (when benefits attenuate) and mitigations. Benefits are smaller or riskier where: (a) strict regulatory auditability and explainability are required (healthcare, legal), (b) data quality is low or fragmented (increasing human review), (c) domain accuracy is critical (medical, legal), or (d) integration costs and governance overheads are high. Mitigations (RAG/grounding, human‑in‑the‑loop review, SLOs for factuality) reduce error but raise operational costs; include these trade‑offs in TCO and ROI models. For decision makers, we recommend running sensitivity analyses around review rates and integration cost to show break‑even points.\n",
       "\n",
       "Practical recommendations for decision makers (operationalized and evidence‑anchored).\n",
       "- Start with pilot metrics directly tied to the ROI levers above: measure baseline handle time, FCR, search‑to‑answer latency, and analyst hours; instrument change with A/B testing where possible.\n",
       "- Use public benchmarks and surveys (typedef.ai; AssemblyAI use‑case summaries) to set priors, treat vendor case studies as hypothesis generators, and always validate vendor claims with internal before/after measurement. Label vendor numbers as vendor‑supplied and present sensitivity bounds when these dominate the prior.\n",
       "- Include inference benchmarking as part of pilots to convert per‑token/throughput metrics into production TCO (use NVIDIA GenAI‑Perf / NIM, measure GPU utilization, and warm/cold start effects: https://developer.nvidia.com/blog/llm-inference-benchmarking-how-much-does-your-llm-inference-cost/).\n",
       "- Document assumptions explicitly when projecting ROI (baseline throughput, human review rates, latency targets, infrastructure costs) and present uncertainty ranges rather than point estimates.\n",
       "\n",
       "Summary of evidence posture. Independent public surveys and case writeups converge on non‑trivial productivity and cost benefits—particularly in customer service throughput and internal knowledge work—while vendor case studies often show larger, more optimistic numbers. We therefore present central ranges that favor corroborated public evidence, label vendor figures clearly, and provide sensitivity analyses to show how conclusions vary under alternative assumptions. Where high‑quality independent measurement is absent we note the gap and recommend internal validation steps (pilot AB tests, instrumentation, and inference benchmarking). These quantified benefits, their labeled evidence sources, and the uncertainty bounds provide a calibrated case for LLM adoption; the remainder of the report examines the engineering and operational constraints (compute, latency, integration, and staffing) that determine whether these benefits are realizable at scale and shows worked examples to convert business KPIs into defensible architecture and TCO choices.\n",
       "\n",
       "## Technical and Operational Challenges: Scalability, Cost, and Integration\n",
       "\n",
       "The quantified benefits in the previous section and the rising adoption signal for LLMs create a strong incentive to deploy them; however, realizing those benefits at scale requires concrete engineering and operational trade‑offs. This section therefore examines the measurable constraints—compute, latency, integration, and staffing—ties them to independent evidence and cost signals, and adds an explicit decision framework and worked examples so teams can convert business KPIs into defensible architecture and TCO choices. Overview and evidence base (closing the verification gaps). Adoption and production-readiness claims in this section are now supported by independent, public evidence. Multiple industry studies show large, rapid adoption of generative AI/LLMs in enterprise settings (67–78% of organizations using generative AI in 2024, per aggregated adoption statistics and McKinsey summaries; see https://www.typedef.ai/resources/llm-adoption-statistics and companion reporting cited there). At the same time, analyst forecasts signal material increases in API demand driven by AI: Gartner predicts that more than 30% of the projected increase in API demand through 2026 will come from AI and LLM‑powered tools (https://www.gartner.com/en/newsroom/press-releases/2024-03-20-gartner-predicts-more-than-30-percent-of-the-increase-in-demand-for-apis-will-come-from-ai-and-tools-using-llms-by-2026). Independent reviews and systematic audits also document high pilot failure rates and integration gaps (e.g., summaries of MIT/academic findings captured in adoption roundups; https://www.typedef.ai/resources/llm-adoption-statistics), which supports the claim that operational engineering is the primary bottleneck to realizing value.\n",
       "\n",
       "We call out data provenance where it matters: statistics from industry analysts (Gartner, McKinsey) and academic audits (peer‑reviewed or institutional reports) form our primary empirical basis for adoption and demand claims; practitioner writeups and benchmarks (AnyScale, AIMultiple, Introl) provide concrete latency/unit‑cost measurements and optimization case studies that underpin the TCO guidance. Where a recommendation is based primarily on practitioner or blog evidence (e.g., some medium‑form decision frameworks), we flag it as practitioner guidance and recommend pilots to validate applicability in your environment (see pilot matrix below). \n",
       "\n",
       "Measured performance axes and practical implications. Benchmarks separate two orthogonal latency axes that drive user experience and provisioning: time‑to‑first‑token (TTFT, i.e., startup latency) and inter‑token or per‑token latency (ITL). Throughput is largely determined by ITL and batching behavior (AnyScale benchmarking overview: https://docs.anyscale.com/llm/serving/benchmarking/metrics). Public latency benchmarks (AIMultiple 2026 LLM Latency Benchmark) show substantial variation: some reasoning‑optimized models have multi‑second TTFT but low per‑token cost (e.g., the Grok 4.1 example with ~4 s first token and ~0.010 s per token; https://research.aimultiple.com/llm-latency-benchmark/). Practitioner guidance commonly treats P95 TTFT <0.5 s as “ultra‑responsive,” usually achieved by smaller models or highly optimized runtimes (Prateek Bansal explainer; AnyScale metrics). These measured axes directly inform whether an application must prioritize TTFT (interactive chat) or amortized ITL (document generation, batch analytics).\n",
       "\n",
       "Compute, memory, and TCO mechanics (worked framing). Unit‑economics studies and vendor guidance (Introl: inference unit economics; NVIDIA LLM perf guidance) now give reproducible inputs for a TCO exercise (https://introl.com/blog/inference-unit-economics-true-cost-per-million-tokens-guide; NVIDIA developer blogs). To make the exercise operational, use this worked checklist and the decision framework below:\n",
       "- Collect business KPIs: P95 TTFT target, acceptable per‑request latency for completion, expected QPS/concurrency, median and P95 response lengths (tokens), and data‑sensitivity classification (regulated / PII / internal). \n",
       "- Translate to token demand: tokens/sec = QPS * median tokens per response (use P95 tokens for capacity planning under tail load). Monthly tokens = tokens/sec * 3600 * 24 * 30. \n",
       "- Gather unit costs and performance options: managed API $/1k tokens (vendor pricing), self‑hosted GPU $/hour (cloud or on‑prem), and measured tokens/sec per candidate model under target runtime (from AnyScale/NVIDIA/AIMultiple benchmarks or your microbenchmark pilots). \n",
       "- Compute TCO scenarios: (monthly tokens) * (managed API $/token) vs. (GPU hours required given tokens/sec per GPU and utilization) * (GPU $/hour) + storage/networking + ops labor. Run sensitivity ranges for tokens/sec per GPU and utilization to reveal break‑even points. (See pilot steps below for suggested microbenchmarking methods: https://docs.anyscale.com/llm/serving/benchmarking/metrics.)\n",
       "\n",
       "Representative optimization gains and operational signals. Published experiments show consistent gains from quantization, pruning/sparsity, distillation, and retrieval‑grounding (GPTQ / post‑training quantization: ~2–4× reductions in memory footprint in many cases; quantization+sparsity experiments showing modest throughput gains; distillation cases such as DistilBERT demonstrating sizable size reduction with preserved task accuracy) (GPTQ summary: https://arxiv.org/abs/2306.13353; quantization+sparsity case: https://quantumzeitgeist.com/efficient-llm-inference-achieves-speedup/; distillation: https://arxiv.org/abs/1910.01108). RAG (retrieval‑augmented generation) is widely used in production to constrain model outputs and reduce token generation for knowledge calls, and practitioner analyses find RAG scales well for high query volumes when index engineering and freshness are addressed (RAG paper and practitioner writeups: https://arxiv.org/abs/2005.11401; production guidance and comparisons: https://calmops.com/ai/building-production-llm-applications-rag-deployment/).\n",
       "\n",
       "Decision framework: mapping business KPIs to architecture choices. The following concise decision flow maps primary business KPIs to recommended architectural choices; apply the worked examples that follow to convert a choice into cost/TCO implications.\n",
       "1) Is P95 TTFT target < 0.5 s? \n",
       "   - Yes: prioritize smaller models (≤7–13B) or highly optimized 4‑bit runtimes, edge/colocated inference, KV caching, and aggressive quantization. Expect to trade some capability on complex reasoning tasks. \n",
       "   - No (P95 TTFT can be multi‑second): larger reasoning models are feasible; prioritize models with strong per‑token efficiency and consider batching for throughput.\n",
       "2) Is concurrency (QPS) high relative to per‑request tokens (i.e., many short requests)? \n",
       "   - Yes: design for throughput (higher‑memory GPUs, batching, optimized kernels) and consider routing/light models for routine queries. \n",
       "   - No (lower QPS with long responses): optimize for per‑token efficiency; larger models with better per‑token speed may be cost‑effective. \n",
       "3) Is data sensitive / regulated? \n",
       "   - Yes: prefer self‑hosted or private managed options with contractual guarantees; combine fine‑tuning for behavior control with RAG for grounding. \n",
       "   - No: managed API is a viable low‑ops option, but validate privacy/retention terms. \n",
       "4) Is model correctness and repeatability essential (e.g., legal advice, financial outputs)? \n",
       "   - Yes: favor fine‑tuning and deterministic runtime control (or heavy retrieval grounding with RAG) even at higher cost; measure hallucination SLOs. \n",
       "   - No: cached prompts, ensembled routing, and distilled specialists may be sufficient.\n",
       "\n",
       "Worked example A — Interactive chat (illustrative, plug‑and‑play math). Assumptions (example team inputs): P95 TTFT target = 0.5 s, target QPS = 20 concurrent chats, median response = 64 tokens, acceptable utilization = 70%.\n",
       "- Token demand: 20 * 64 = 1,280 tokens/sec; monthly tokens ≈ 1,280 * 86,400 ≈ 110M tokens/month. \n",
       "- Choose candidate tokens/sec per GPU (from your microbenchmark): assume conservative = 500 tokens/sec per GPU for a quantized 13B model in your stack; optimistic = 2,000 tokens/sec for a highly optimized runtime.\n",
       "- GPU requirement (conservative): GPUs needed = 1,280 / 500 ≈ 2.56 → 3 GPUs (at 70% utilization increase capacity accordingly). \n",
       "- GPU requirement (optimistic): 1,280 / 2,000 = 0.64 → 1 GPU may suffice. \n",
       "- TCO conversion: multiply GPUs required * $/GPU‑hour * 24 * 30 and add ops labor + storage. Alternatively, compute managed API cost = (monthly tokens / 1,000) * $/1k tokens (use vendor pricing). Run both calculations with your true $ values to see which is lower; the math above shows a typical break‑even sensitivity: if GPUs deliver high tokens/sec the self‑hosted path becomes cheaper; if not, managed API often wins. (See Introl for typical $/M token ranges and break‑even analysis: https://introl.com/blog/inference-unit-economics-true-cost-per-million-tokens-guide.)\n",
       "\n",
       "Worked example B — High‑volume document generation (batch analytics). Assumptions: P95 TTFT can be multi‑second, QPS = 5 long jobs, median response = 2,048 tokens, batched to amortize per‑token cost.\n",
       "- Token demand: 5 * 2,048 = 10,240 tokens/sec; monthly ≈ 885M tokens/month. \n",
       "- For batch jobs, per‑token efficiency matters: choose a model with low ITL even if TTFT is ~4 s; batching reduces per‑token overhead. If a single GPU provides 5,000 tokens/sec in your stack, GPUs needed ≈ 10,240 / 5,000 ≈ 2.05 → 3 GPUs (adjust for utilization). \n",
       "- Here, deploying a larger optimized model on dedicated instances with aggressive batching and kernel optimizations typically gives better unit economics than routing to a high‑latency managed API unless the API price per 1k tokens is materially competitive. Use the same formulae above to compute monthly TCO and sensitivity ranges.\n",
       "\n",
       "Tradeoffs and mitigation patterns (operationally specific). The literature and production case studies document complementary strategies; we summarize maturity, tradeoffs, and when to prefer each (evidence links included):\n",
       "- Distillation: mature; good for classification/intent and short‑form tasks; provides reliable cost reductions after validation (https://arxiv.org/abs/1910.01108). \n",
       "- Quantization / low‑precision inference: medium–high maturity; GPTQ and related methods regularly reduce memory footprint and can improve throughput, but require per‑model tuning and validation (https://arxiv.org/abs/2306.13353; https://quantumzeitgeist.com/efficient-llm-inference-achieves-speedup/). \n",
       "- RAG: mature for knowledge‑grounded tasks, scales well for high QPS when index engineering is solid; reduces generation tokens and hallucination risk but adds retrieval latency and index freshness engineering (https://arxiv.org/abs/2005.11401; https://calmops.com/ai/building-production-llm-applications-rag-deployment/). Practitioner decision frameworks summarizing RAG vs fine‑tuning tradeoffs are common and useful as heuristics; treat them as starting points for pilots (examples: Medium/Reddit analyses referenced in practitioner literature). \n",
       "- Intelligent routing/ensembles and hybrid cloud/edge: medium maturity; effective at lowering costs by routing simple queries to cheaper models and complex queries to larger models, but add routing accuracy and ops complexity.\n",
       "\n",
       "Operational controls, KPIs, and pilot hygiene. To operationalize the decision framework and validate assumptions, instrument these KPIs during microbenchmarks and pilots: P95 TTFT, median/P95 ITL, tokens/sec (aggregate and per model), GPU utilization, cost per 1k tokens (managed and self‑hosted), hallucination/error rates by intent, and index freshness. Practical pilot design:\n",
       "1) Baseline microbenchmarks to measure candidate models and runtimes under representative payloads (use AnyScale/NVIDIA benchmarking methods: https://docs.anyscale.com/llm/serving/benchmarking/metrics; NVIDIA perf notes). \n",
       "2) Pilot matrix: run concurrent pilots for (a) managed API; (b) self‑hosted medium fine‑tuned model; (c) distilled+quantized specialist + RAG classifier. Measure the KPIs above and collect real utilization traces. \n",
       "3) TCO mapping: convert observed metrics into monthly cost under realistic utilization and sensitivity ranges; perform break‑even analysis between managed API and self‑hosted under low/medium/high throughput and latencies using the formulas above and published unit economics references (https://introl.com/blog/inference-unit-economics-true-cost-per-million-tokens-guide). \n",
       "4) Incremental rollout: start with routing that prefers cheaper paths and reserves larger models for escalations; embed canarying, SLO monitoring, and cost dashboards. \n",
       "\n",
       "Skills, runbooks, and governance alignment. Staffing and tooling matter: teams typically need ML engineers, SREs, data engineers, and product owners to run pilots and own TCO dashboards. Operational practices should include CI for model artifacts, automatic validation for quantized/distilled models, and reproducible deployment playbooks. Governance responsibilities (data retention, compliance) are discussed in the next section; here we emphasize technical controls that enable governance‑grade SLOs (e.g., avaliable audit trails for fine‑tuning data and RAG index change logs).\n",
       "\n",
       "Caveats and sensitivity: where evidence is weak or practitioner‑led, be explicit. Some recommendations (optimal GPU tokens/sec, exact $/M token break‑even points) depend on rapidly changing hardware, runtime, and vendor pricing. Where only practitioner blog guidance or listicles exist, we mark those items as practitioner heuristics and require pilot validation. For every prescriptive choice above, teams should run the lightweight sensitivity analysis shown in the worked examples to understand how conclusions change under alternative assumptions. Technical and operational fixes—when chosen with clear KPI‑driven decision rules and validated in pilots—make LLM deployments repeatable and cost‑predictable, but they do not eliminate model‑level risks or regulatory exposures. The next section therefore turns from engineering controls to governance: measurable policies, technical safeguards, and organizational processes that map hallucination, data‑leakage, bias, and compliance risks to SLOs, monitoring signals, and accountable roles.\n",
       "\n",
       "## Risks, Compliance, and Governance: Addressing Hallucinations, Data Leakage, and Regulation\n",
       "\n",
       "The prior section showed that engineering and operational controls reduce many deployment frictions, but they cannot fully eliminate model‑level and legal risks. This section therefore turns to governance: an operational, measurable set of policies, technical safeguards, and organizational processes that map specific LLM risk vectors (hallucination, data leakage, bias, regulatory exposures) to concrete mitigations, monitoring signals, SLOs, and accountable roles. Scope and operational definitions. For clarity in the recommendations that follow, this document uses “LLM” to mean transformer‑based autoregressive or encoder‑decoder models that are pretrained at scale and typically exceed ~100M parameters and are used with adaptation (fine‑tuning, instruction tuning, or retrieval augmentation). “Production‑scale” denotes deployments expected to serve sustained user traffic (e.g., 10k+ requests/day or business‑critical workflows) where availability, security, compliance, and reputational risk are material.\n",
       "\n",
       "High‑level approach and balancing narrative gaps. To avoid the prior unevenness between technical depth and applied guidance, the governance posture below explicitly maps risks to controls, measurable signals, roles, and cost/benefit considerations. The emphasis here is metrics‑first: adopt automated, repeatable evaluation frameworks (e.g., DeepEval) and observability platforms (e.g., Arize, Galileo, MLflow) to operationalize detection, alerting, and periodic auditor review (see DeepEval docs: https://deepeval.com/docs/metrics-hallucination; Arize monitoring: https://www.arize.com/; Future AGI evaluation survey: https://futureagi.substack.com/p/the-complete-guide-to-llm-evaluation). Later sections should harmonize by quantifying benefits and TCO using the same SLOs and benchmarks defined here (e.g., factuality error rates, tokens/sec, cost per 1M tokens) so leadership can weigh residual risk against expected value.\n",
       "\n",
       "Operational matrix: Risk → Concrete controls → Measurement & enforcement → Roles. Below we expand each risk with concrete, enforceable examples (validation tests, thresholds, provenance requirements, access controls), suggested monitoring intervals, and remediation actions.\n",
       "\n",
       "- Hallucination (confident but incorrect or fabricated outputs)\n",
       "  - Concrete controls: 1) Retrieval‑augmented generation (RAG) with signed/source‑verified retrieval results and citation embedding hashes saved to provenance logs; 2) unit tests for factual claims (claim→expected evidence check) implemented via test harnesses such as DeepEval or custom LLMTestCase suites (see https://deepeval.com/docs/metrics-hallucination); 3) prompt templates that force source citation and conservative generation (e.g., answer only when supported); 4) automated veracity checks that re‑query model as a judge or use an external fact‑checking model; 5) citation‑backstop: require that all high‑risk responses include at least one verified external source or route to human review.\n",
       "  - Concrete metrics and SLOs (examples): 1) Regulatory/high‑risk workflows: factuality error rate (false claims per 100k assertions) P95 <0.1%; provenance completeness ≥99% (for any information‑bearing response, recorded retrieval hits/dataset ids and source hashes); citation accuracy (fraction of citations that verifiably support the claim) ≥95% on sampled audits; detection latency for a factuality anomaly <1 hour; automated test pass rate ≥99% per release (n≥500 critical test cases). 2) Lower‑risk customer‑support workflows: factuality error rate P95 <0.5%; ≥90% responses include a verifiable citation when assertions are made.\n",
       "  - Measurement approach and intervals: 1) Real‑time inference logging (inputs, generated tokens, model version, retrieval ids/hashes, timestamps) persisted to immutable audit store; 2) automated LLM‑as‑judge tests run on every deployment and nightly population tests using DeepEval/LLMTestCase suites; 3) daily aggregated dashboards (factuality error rate, unsupported assertion frequency), weekly human sampling (statistically sized sample, e.g., 400 responses → 95% CI ±5%), quarterly third‑party audit. See reviews and taxonomy on factuality/fact‑checking: https://arxiv.org/html/2508.03860v1.\n",
       "  - Enforcement and incident steps: threshold breach triggers automated circuit breaker: degrade to human‑in‑the‑loop (HITL) or constrained safe model, isolate the model/version, notify stakeholders. Post‑mortem: replay failing cases through test harness, root cause, patch prompt/RAG/index or retrain, add failing cases to regression suite. Accountability: model owner sets SLOs; ML engineers implement RAG and unit tests; QA owns release gating.\n",
       "\n",
       "- Privacy and data leakage (memorization, extraction of sensitive training or input data)\n",
       "  - Concrete controls: 1) ingestion controls and data minimization: automated client‑side and server‑side redaction of PII/PHI based on classification rules before storage or model input; 2) RBAC for model and dataset access, just‑in‑time elevated privileges, and strict key rotation policies; 3) DP‑SGD or other DP mechanisms for fine‑tuning where utility permits, with documented epsilon/δ choices and acceptance tests (Abadi et al.: https://arxiv.org/abs/1607.00133); 4) confidential compute enclaves for sensitive training/inference; 5) synthetic extraction and membership inference tests included in CI (e.g., Carlini‑style extraction suites: https://arxiv.org/abs/2012.07805); 6) contractual and technical controls for vendor data handling and residency.\n",
       "  - Concrete metrics and SLOs (examples): confirmed data‑extraction incidents in production = 0 (tolerance = zero); PII/PHI leakage detection rate — automated detection of any PII/PHI in outputs triggers immediate containment; mean time to detection (MTTD) target <4 hours; mean time to containment (MTTC) <24 hours; DP tuning: target ε ≤1 for highly sensitive datasets where feasible, or documented trade‑off when higher ε is accepted.\n",
       "  - Measurement approach and intervals: 1) continual automated synthetic extraction probing (nightly) and anomaly detection on output streams (real‑time) using pattern matchers and ML detectors; 2) weekly log reviews of privileged accesses and anomalous export behavior; 3) quarterly privacy audits and annual DPIA refresh or immediately upon material change.\n",
       "  - Enforcement and incident steps: immediate key revocation and endpoint isolation → identify records and user impacts → notify per statutory timelines (GDPR/HIPAA) → remediate training data/model → update DPIA and contractual terms. Accountability: data steward enforces classification and redaction; security/CISO enforces RBAC/enclave use; legal manages breach notification and contractual obligations.\n",
       "\n",
       "- Bias, fairness, and explainability (systemic and disparate impacts)\n",
       "  - Concrete controls: 1) datasheets/model cards and slice‑based evaluation baked into CI for every release (Model Cards: https://arxiv.org/abs/1810.03993); 2) counterfactual and targeted remediation tests (reweighting, balanced fine‑tuning, or rule overrides) applied per failing slice; 3) mandatory human review for decisions above a defined business impact threshold; 4) explanation artifacts persisted with model outputs (e.g., top retrieval context, chain‑of‑thought redacted logs, and confidence bands).\n",
       "  - Concrete metrics and SLOs (examples): per‑slice parity thresholds set by product (example: demographic false negative rate delta ≤3% for hiring/crediting flows); drift detection: weekly change in slice representation >5% triggers investigation; scheduled fairness re‑evaluation every release and at least monthly for high‑risk features.\n",
       "  - Measurement approach and intervals: continuous logging of slice labels (where legal), weekly automated disparity reporting, monthly human audits of flagged cohorts, quarterly external audit. Accountability: data steward and fairness lead own tests and remediation; legal reviews downstream regulatory exposure; product owner approves acceptable tradeoffs.\n",
       "\n",
       "- Regulatory compliance and legal risk\n",
       "  - Concrete controls: 1) DPIA prior to deployment and on material change (GDPR Article 35: https://eur-lex.europa.eu/eli/reg/2016/679/oj); 2) contractual clauses for vendor/cloud providers specifying data residency, breach notification windows, and audit rights; 3) immutable provenance logging (inputs, model version, retrieval sources, response, timestamp, user id) with tamper‑evident storage and defined retention aligned to law (e.g., HIPAA retention considerations) and business needs; 4) defined HITL thresholds and expressly documented record of human overrides for high‑risk decisions; 5) runbooks that align to NIST incident guidance (https://csrc.nist.gov/publications/detail/sp/800-61/rev-2/final).\n",
       "  - Concrete metrics and SLOs (examples): DPIA completion and legal signoff before production; audit log health (ingest success rate ≥99.9%); time‑to‑respond for data subject access requests <30 days (or shorter if local law requires); contract SLA compliance metrics as part of vendor KPIs.\n",
       "  - Measurement approach and intervals: automated audit log integrity checks (hourly), weekly SLA dashboards for vendor obligations, annual DPIA refresh or immediate refresh on material change, scheduled legal reviews tied to release cadence. Accountability: legal/compliance own DPIAs and contracts; security and IT enforce data residency and logging; privacy officer signs off on retention.\n",
       "\n",
       "Operational governance, monitoring, and separation of responsibilities. To address coherence between engineering and governance, adopt explicit handoffs and measurable gates:\n",
       "- Engineering (ML/SRE): implement technical mitigations, automated test suites (DeepEval/LLMTestCase, synthetic extraction tests), observability pipelines (Arize/Galileo/MLflow integration), and enforce RBAC/enclave usage. Deploy nightly automated tests and maintain regression suites that block releases on failed SLO gates.\n",
       "- Governance/Policy (Legal/Compliance/CISO/Product): set acceptable SLO thresholds per workflow, define escalation criteria and incident severity mappings, sign off DPIAs and vendor contracts, and run cross‑functional red‑team reviews. Maintain an SLO catalogue that maps each feature to its required factuality, privacy, fairness, and availability thresholds.\n",
       "Cross‑functional processes (red‑teaming, monitoring, HITL) require both groups to collaborate; governance defines SLOs and escalation, engineering implements controls and observability. Use a metrics‑first platform to automate the gate: e.g., pre‑release block if automated DeepEval factuality on a representative sample falls below target or if extraction tests detect leakage.\n",
       "\n",
       "Measurement, benchmarks, and TCO evidence. Use representative workloads and the same evaluation tools used for SLOs. Operational recommendations:\n",
       "- Baseline and post‑optimization benchmarking using MLPerf/GenAI‑Perf and internal telemetry (tokens/sec, p95 latency, cost per 1M tokens). Vendor or blog numbers require validation with internal pilots (see NVIDIA GenAI‑Perf and practitioner guides referenced earlier).\n",
       "- Track governance operational cost: staff hours for red‑teaming, audit/forensics, and human reviewers per 1M responses; include the cost of fallback HITL capacity when SLO gates trip. Use these to compute break‑even points before wider rollout.\n",
       "- Maintain an evidence repository linking model versions, test results (DeepEval/Arize dashboards), and incident logs to support audits and regulatory inquiries.\n",
       "\n",
       "Red‑teaming, testing cadence, and evidence provenance. Structure red‑teaming and testing as repeatable pipelines:\n",
       "- Continuous: nightly automated factuality, extraction, and safety tests; real‑time anomaly detection on production outputs.\n",
       "- Pre‑release: full regression on a statistically representative sample (e.g., 10k queries) with DeepEval/LLMTestCase suites and human audit of a stratified sample (n based on CI requirements).\n",
       "- Periodic external audit: third‑party review annually or upon major release. Keep red‑team results auditable with privileged access controls.\n",
       "\n",
       "Putting it together (operational checklist). Before production rollout, require and verify:\n",
       "- DPIA and legal signoff; vendor contract clauses verified.\n",
       "- Model card and datasheet published internally with documented evaluation datasets and SLOs.\n",
       "- Automated red‑team and synthetic extraction tests passing on the release branch (nightly pass rate ≥99%).\n",
       "- RAG or citation modality enabled where factuality matters; provenance logging configured (store: input, retrieval ids/hashes, model version, response, timestamp, retention policy).\n",
       "- PII/PHI redaction in ingest pipelines and RBAC enforced for sensitive model access.\n",
       "- Monitoring and alerting configured for SLOs with defined alert thresholds and runbooks (example: >5 unsupported assertions per 10k responses or any confirmed PII leakage triggers sev‑1).\n",
       "- Runbook and incident response aligned with NIST SP 800‑61; DPIA and vendor contractual controls in place.\n",
       "\n",
       "Evidence and provenance. Prefer peer‑reviewed and neutral industry analyses for technical claims (Maynez et al. on faithfulness; Carlini et al. on extraction) while operationalizing newer evaluation tooling (DeepEval: https://deepeval.com/docs/metrics-hallucination; community piece on metrics‑first evaluation: https://community.deeplearning.ai/t/mitigating-llm-hallucinations-with-a-metrics-first-evaluation-framework/476691). Rely on mature observability solutions (Arize: https://www.arize.com/) and evaluation platforms (Future AGI survey: https://futureagi.substack.com/p/the-complete-guide-to-llm-evaluation) to scale testing and provide provenance for audits. Vendor claims should be validated via internal benchmarks (GenAI‑Perf, MLPerf) and the organization’s regression suites.\n",
       "\n",
       "This governance framework is designed to be operational: each mitigation maps to measurable signals, concrete SLOs and enforcement rules, and accountable roles so leadership can make trade‑off decisions grounded in concrete operational numbers and documented evidence. With these governance mechanisms, mappings, and measurable SLOs established, the logical next step is to weigh those protections against expected benefits and the real costs of adoption. The next section synthesizes trade‑offs—quantified benefits, hidden and recurring costs, and governance‑driven constraints—into a prioritized, actionable roadmap with roles, timelines, and measurable milestones.\n",
       "\n",
       "## Analysis and Discussion: Synthesizing Benefits, Costs, and Strategic Considerations\n",
       "\n",
       "With governance mechanisms, mappings, and measurable SLOs in place, the next step is to reconcile those protections with realistic benefit estimates and the full economic picture of LLM deployments. The following analysis synthesizes quantified benefits, explicit TCO scenarios, and operational SLO requirements into concrete decision levers, break‑even heuristics, staffing expectations, and a staged roadmap for choosing cloud, hybrid, or on‑prem patterns as well as off‑the‑shelf, fine‑tuned, or custom model strategies. This analysis harmonizes empirical benefit ranges with explicit TCO sensitivity analysis and governance SLOs so organizations can make defensible, measurable LLM investment decisions. We preserve the earlier taxonomy of benefits, direct and hidden costs, governance controls, and maturity stages, and add three explicit, illustrative TCO scenarios (conservative, baseline, optimistic/high‑volume) that reconcile vendor ROI claims with engineering realities. These scenarios are accompanied by assumptions and sensitivity ranges so readers can adapt to their inputs.\n",
       "\n",
       "Benefits (recap, quantified with uncertainty bounds)\n",
       "- Empirical uplift: industry case studies commonly show single‑ to low‑double‑digit improvements on focused KPIs where LLMs are embedded in workflows (e.g., 5–25% reductions in handle time or 5–20% throughput gains). Reported ranges depend on task, baseline maturity, and whether outputs are grounded (RAG) or free‑generation. Use conservative central and downside scenarios for planning (e.g., central = 10% throughput gain, downside = 3–5%).\n",
       "- Adoption runway: time‑to‑value typically ranges 3–12 months depending on integration complexity and governance effort.\n",
       "\n",
       "Direct and hidden costs (TCO components, reiterated)\n",
       "- Direct: inference and fine‑tuning compute, storage (vector indexes), networking, managed API licensing, and staffing for MLOps/SRE/data engineering. Managed APIs shift capital to OpEx (per‑token) but can be expensive at scale.\n",
       "- Hidden/ongoing: monitoring/observability, continuous labeling and data pipelines, drift mitigation, red‑teaming, incident response, and recurring compliance audits. Regulated domains amplify these costs through stricter logging, encryption, and legal review.\n",
       "- Pricing variability and benchmarking: per‑million token prices vary widely by provider and model; use published comparisons and calculators when modelling (examples: LLM Pricing: Top Providers Compared (2026): https://research.aimultiple.com/llm-pricing/; LLM pricing calculator: https://www.llm-prices.com/). Performance benchmarking tools (e.g., NVIDIA GenAI‑Perf) are essential to map throughput/latency to GPU and infrastructure costs for on‑prem planning (see NVIDIA: https://developer.nvidia.com/blog/llm-inference-benchmarking-how-much-does-your-llm-inference-cost/).\n",
       "\n",
       "Comparative TCO scenarios (conservative, baseline, optimistic)\n",
       "Below are three illustrative scenarios showing how token volume, per‑million API price, model choice, and ops assumptions affect monthly costs and the relative economics of cloud vs on‑prem. Use these as templates: replace numeric anchors with organization‑specific token counts, per‑million costs, amortized hardware figures, and staffing rates.\n",
       "\n",
       "Scenario assumptions common to all examples\n",
       "- Per‑million token API price: range low = $9, central = $15, high = $25 per million (reflects observed 2026 variability; see https://research.aimultiple.com/llm-pricing/ and https://www.llm-prices.com/).\n",
       "- Ops staffing and hidden costs: cloud ops staffing typically 1–2 FTEs initially; on‑prem requires larger MLOps/SRE teams. Hidden recurring costs (monitoring, labeling, compliance) modeled as separate line items.\n",
       "- Benefit anchor (operational value): assume an operational workload where a 10% throughput/handle‑time improvement produces measurable labor savings; specific dollar benefit will vary by enterprise.\n",
       "- Use NVIDIA benchmarking guidance to estimate amortized GPU/inference costs for on‑prem deployments rather than raw token multipliers (https://developer.nvidia.com/blog/llm-inference-benchmarking-how-much-does-your-llm-inference-cost/).\n",
       "\n",
       "1) Conservative / Pilot (low volume)\n",
       "- Tokens: 200M tokens/month (e.g., ~1k active users at ~200k tokens/user/month; see user‑level estimates: https://www.linkedin.com/pulse/hidden-economics-ai-saas-2026-tokens-costs-real-margins-pedro-guillen-metze).\n",
       "- Cloud per‑token cost (central): 200 * $15 = $3,000 / month. Range (low/high): $1,800–$5,000.\n",
       "- Ops & hidden costs (cloud): 1–2 ops FTEs + monitoring/labeling = $20k–$40k/mo.\n",
       "- Total cloud cost: ≈ $23k–$45k / month (central ≈ $26k/mo).\n",
       "- On‑prem amortized GPU + ops: typically not cost‑effective at this scale — amortized GPU + infra + ops easily $80k–$200k/mo; higher fixed costs dominate.\n",
       "- Example benefit comparison: if a small support team (10 FTEs @ $8k/mo each = $80k/mo) reduces handle time by 10%, savings ≈ $8k/mo — below cloud TCO in most grantable setups unless business value beyond labor reduction exists.\n",
       "- Recommendation: start with managed API off‑the‑shelf or lightweight fine‑tuning, validate KPI delta and governance checklist, and avoid on‑prem capital until tokens grow or regulatory needs demand isolation.\n",
       "\n",
       "2) Baseline / Growth (mid volume)\n",
       "- Tokens: 2B tokens/month (0.5–5B range where hybrid often becomes attractive).\n",
       "- Cloud per‑token cost (central): 2,000 * $15 = $30,000 / month. Range (low/high): $18k–$50k.\n",
       "- Ops & hidden costs (cloud): 2–4 ops FTEs + monitoring/red‑teaming/compliance = $50k–$120k/mo.\n",
       "- Total cloud cost: ≈ $68k–$170k / month (central ≈ $100k/mo).\n",
       "- On‑prem amortized GPU + ops: assume amortized GPU fleet + infra ≈ $150k–$300k/mo plus larger ops (4–8 FTEs) $60k–$120k/mo; total ≈ $210k–$420k/mo. Hybrid often reduces net OpEx by routing high‑sensitivity or high‑latency traffic to private infra and overflow to cloud (empirical hybrid savings ≈ 20–35% vs full cloud per market analyses; see Newline: https://www.newline.co/@zaoyang/hybrid-cloud-vs-on-premise-llm-deployment--74f51098).\n",
       "- Example benefit comparison: a medium support organization (100 FTEs @ $8k/mo = $800k/mo) with 10% handle‑time reduction saves ~$80k/mo. In this scenario, central cloud cost (~$100k/mo) is comparable to realized labor savings; ROI depends on accuracy uplift, customer retention impact, and non‑labor value (e.g., faster SLAs). Fine‑tuning that yields larger accuracy uplift (e.g., 15% or more) or caching/parameter‑efficient techniques will materially improve ROI.\n",
       "- Recommendation: model hybrid patterns (private for regulated/sensitive flows, cloud for bulk), invest in PEFT/fine‑tuning if labeled data and MLOps readiness exist, and run a multi‑year TCO model including governance recurring costs.\n",
       "\n",
       "3) Optimistic / High‑volume (scale)\n",
       "- Tokens: 20B tokens/month (multi‑billion high‑volume; >5–10B/month often cited as on‑prem break‑even region).\n",
       "- Cloud per‑token cost (central): 20,000 * $15 = $300,000 / month. Range (low/high): $180k–$500k.\n",
       "- Ops & hidden costs (cloud): $100k–$250k/mo.\n",
       "- Total cloud cost: ≈ $280k–$750k / month (central ≈ $400k/mo).\n",
       "- On‑prem amortized GPU + ops: amortized GPU fleet + infra ≈ $400k–$800k/mo, plus ops 100k–300k/mo (depending on scale), total ≈ $500k–$1.1M/mo. With amortization over several years and aggressive throughput optimization (batching, quantization, tensor parallelism), per‑token on‑prem cost often falls below managed API at these volumes; many market analyses place break‑even in this multi‑billion token regime (Swfte AI; Newline; arXiv analyses: https://www.swfte.com/zh/blog/yun-vs-bendi-ai-tco-fenxi, https://www.newline.co/@zaoyang/hybrid-cloud-vs-on-premise-llm-deployment--74f51098, https://arxiv.org/html/2509.18101v1).\n",
       "- Example benefit comparison: a large enterprise operation (1,000 FTEs @ $8k/mo = $8M/mo) with 10% productivity gain saves ~$800k/mo — in this context both cloud and on‑prem can be justified, and on‑prem may deliver the best long‑term TCO if demand is stable.\n",
       "- Recommendation: for predictable, sustained, high token volumes, build full TCO models with GPU benchmarking (e.g., NVIDIA GenAI‑Perf metrics) and include amortized hardware, power, and facilities. Consider custom model training if product differentiation or regulatory control is critical.\n",
       "\n",
       "Sensitivity analysis and practical modelling guidance\n",
       "- Key levers: per‑million API price, monthly token volume, amortized GPU and infra cost, ops FTE counts, and hidden recurring items (monitoring, labeling, compliance). Small changes in per‑million price or token volume can swing TCO dramatically; run best/central/worst scenarios.\n",
       "- Quick sketch to build your TCO: inputs = expected monthly tokens, per‑million API cost (low/central/high), amortized monthly GPU/infra (for on‑prem), ops FTE counts & costs, and recurring governance costs. Compute cost_cloud = (tokens/1M)*API_price + cloud_ops + storage/embedding; cost_onprem = amortized_GPU + onprem_ops + infra_maintenance + storage. Use scenario ranges and sensitivity sweeps for token volume and API price.\n",
       "- Use benchmarking tools and token‑counting guides: tokenization differences matter — consult model tokenizers and sample prompts from pricing tables (e.g., https://research.aimultiple.com/llm-pricing/ and https://www.llm-prices.com/) to match expected usage patterns and content types.\n",
       "\n",
       "Governance → operational SLOs and controls (operationalized)\n",
       "- Map risk to controls and SLOs: encrypted storage, isolated VPCs, least‑privilege access, and audit logs with retention matching regulation (12–36 months). For high‑risk outputs enforce deterministic grounding (RAG with provenance), and HITL verification sampling rates (e.g., 100% initial verification for clinically actionable outputs; 1–5% ongoing sampling otherwise). Define hallucination SLOs (e.g., <10 hallucination incidents per 10k responses for low‑risk support; near‑zero for clinical/legal).\n",
       "- Auditability: maintain provenance for RAG context, model metadata/model cards for deployed checkpoints, and immutable logs for the compliance retention period.\n",
       "- Cost impact: incorporate governance line items explicitly in TCO (additional storage/encryption, audit pipelines, HITL staffing). Regulated workloads often shift the optimal architecture toward hybrid or on‑prem due to data residency and auditability needs.\n",
       "\n",
       "Decision frameworks: off‑the‑shelf vs fine‑tuning vs custom (operational heuristics)\n",
       "- Off‑the‑shelf (managed API): choose when use‑cases are generic, sensitivity is low, latency needs are moderate, and time‑to‑market is primary. Pros: fastest launch, minimal infra staffing. Cons: per‑token costs at scale and limited control over internals.\n",
       "- Fine‑tuning / PEFT: choose when domain specificity materially improves KPIs and you have labeled examples + MLOps readiness. Heuristic: consider fine‑tuning if expected accuracy uplift >5–10 percentage points and you can supply thousands–tens of thousands of high‑quality examples.\n",
       "- Custom/retrained models: choose when product is strategic, IP/differentiation requires model behavior control, or regulation demands full isolation. Expect multi‑year timelines and multimillion dollar investments; model only if measurable business value outweighs capital and ongoing ops burden.\n",
       "\n",
       "Operational maturity and staffing (pilot → production)\n",
       "- Pilot (0–3 months): small product team (1 PM, 1 data engineer, 1 ML engineer contractor, 1 domain SME, fractional legal/security). Validate KPI delta and compute sample TCO.\n",
       "- Validation / Harden (3–9 months): add 1–2 MLOps/SRE, 1–2 data engineers, QA/observability engineer, dedicated security/compliance. Build CI/CD, monitoring dashboards, and initial HITL.\n",
       "- Scale / Operate (9–24+ months): central LLM CoE (2–4 senior engineers), distributed product leads, 3–6 operational SRE/MLOps/data engineers, labeling workforce, red‑team and compliance staff. Staffing scales with query volume, number of models, and governance strictness.\n",
       "\n",
       "Practical tradeoffs, success factors, and pitfalls\n",
       "- Tradeoffs: speed (cloud) vs control (on‑prem/custom); per‑token OpEx vs capital amortization; vendor dependence vs internal expertise. Governance frequently pushes architectures toward more controlled and costlier patterns.\n",
       "- Success factors: executive sponsorship with measurable KPIs, prioritized use cases with clear ROI, cross‑functional teams, strong data engineering, explicit SLOs (hallucinations/latency/availability), and phased cost controls (PEFT, caching, hybrid routing).\n",
       "- Pitfalls: failing to model recurring governance costs, underestimating labeling and drift mitigation budgets, launching without KPIs, and skipping red‑teaming or provenance logging.\n",
       "\n",
       "Synthesis into actionable decision levers\n",
       "- Short checklist: (1) Monthly token volume (low/mid/high); (2) Data sensitivity (public/internal/regulatory); (3) Latency/availability SLA; (4) Expected accuracy uplift from tuning and data readiness; (5) Strategic need for IP/control.\n",
       "- Example rule: if tokens <0.5B/mo AND sensitivity low → start with off‑the‑shelf API; if tokens 0.5–5B/mo OR mixed sensitivity → adopt hybrid; if tokens >5–10B/mo AND stability high → evaluate on‑prem with GPU benchmarking and multi‑year TCO modelling (sources: https://www.swfte.com/zh/blog/yun-vs-bendi-ai-tco-fenxi; https://www.newline.co/@zaoyang/hybrid-cloud-vs-on-premise-llm-deployment--74f51098; https://arxiv.org/html/2509.18101v1).\n",
       "\n",
       "Where to model further and references\n",
       "- Use provider pricing tables and token calculators to convert expected content types to tokens and cost (https://research.aimultiple.com/llm-pricing/; https://www.llm-prices.com/). Consider practitioner writeups for hidden economics of AI SaaS (https://www.linkedin.com/pulse/hidden-economics-ai-saas-2026-tokens-costs-real-margins-pedro-guillen-metze) and engineering benchmarking (NVIDIA inference benchmarking: https://developer.nvidia.com/blog/llm-inference-benchmarking-how-much-does-your-llm-inference-cost/). For practical TCO heuristics and hybrid/on‑prem analyses see Newline and Swfte AI (https://www.newline.co/@zaoyang/hybrid-cloud-vs-on-premise-llm-deployment--74f51098; https://www.swfte.com/zh/blog/yun-vs-bendi-ai-tco-fenxi).\n",
       "\n",
       "This section closes the logical gap between optimistic vendor ROI claims and engineering cost realities by (a) quantifying benefits with conservative bounds, (b) providing three concrete TCO scenarios with assumptions and sensitivity ranges, (c) mapping governance risks to SLOs and recurring costs, and (d) giving staffing and maturity guidance to move from pilot to production responsibly. The preceding synthesis converts governance SLOs and measured benefits into concrete decision levers, explicit TCO scenarios, and staffing/maturity guidance to inform deployment choices. Next we summarize the core findings and present the prioritized, staged roadmap—pilots, metrics, governance checkpoints, cost controls, and staffing—needed to operationalize responsible, measurable LLM adoption.\n",
       "\n",
       "## Conclusion and Recommendations: Roadmap for Responsible LLM Adoption\n",
       "\n",
       "The preceding synthesis converted governance SLOs and measured benefits into decision levers, explicit TCO scenarios, and staffing/maturity guidance; this section now integrates those elements into a validated decision framework, a time‑bound, role‑mapped roadmap, and sensitivity analyses to make recommendations operational and evidence‑grounded. Synthesis and evidence grounding\n",
       "- Verified adoption and market context: Recent industry and research sources show rapid, near‑ubiquitous enterprise uptake of LLMs and continued API demand growth. Multiple 2024–2026 surveys and analyst reports find that roughly two‑thirds to three‑quarters of organizations have adopted generative‑AI/LLM tooling (e.g., Iopex/Hostinger summary: ~67% adoption; McKinsey synthesis reporting enterprise AI usage at ~78% in 2024) (https://www.hostinger.com/tutorials/llm-statistics; https://www.typedef.ai/resources/llm-adoption-statistics). Gartner projects that by 2026 more than 30% of incremental API demand will come from AI and tools using LLMs, reinforcing the infrastructure and cost pressures described below (https://www.gartner.com/en/newsroom/press-releases/2024-03-20-gartner-predicts-more-than-30-percent-of-the-increase-in-demand-for-apis-will-come-from-ai-and-tools-using-llms-by-2026). These independent data points validate the premise that LLMs are moving from experiment to operational scale and that API/inference demand is a strategic cost driver.\n",
       "- Pilot failure and infrastructure gaps: Independent analyses also report high failure rates or slow scale‑up for generative‑AI pilots: summaries of academic and industry reviews indicate many pilots fail to reach production because of data readiness, integration complexity, and governance gaps (see roundup citing a high pilot attrition rate; https://www.typedef.ai/resources/llm-adoption-statistics). This supports conservative gating and rigorous piloting in our roadmap.\n",
       "- Primary value channels: Evidence across usage studies and practitioner reports shows recurring benefit channels—content generation and augmentation, developer productivity (code assistants), customer‑service automation, and decision‑support for knowledge work—consistent with synthesis in this document (https://www.secondtalent.com/resources/domain-generative-ai-llm-usage-statistics/; https://www.sciencedirect.com/science/article/pii/S2666389925002144). Where empirical uplift estimates exist, they are reported as ranges tied to task, prompt quality, and integration depth; therefore we continue to treat benefit estimates as distributions with explicit confidence bounds.\n",
       "Addressing previously unverified claims and sensitivity framing\n",
       "- Which claims were verified: pace of adoption (67–78% adoption signals), API demand growth (>30% of incremental API demand by 2026), and high pilot failure rates are now supported by the citations above (see Hostinger/Iopex, Gartner, typedef.ai) (https://www.hostinger.com/tutorials/llm-statistics; https://www.gartner.com/en/newsroom/press-releases/2024-03-20-gartner-predicts-more-than-30-percent-of-the-increase-in-demand-for-apis-will-come-from-ai-and-tools-using-llms-by-2026; https://www.typedef.ai/resources/llm-adoption-statistics).\n",
       "- Remaining uncertainty and transparent caveats: granular uplift numbers (e.g., “10–40% content throughput”) vary widely by organization and are still based on distributions of reported results; where such point ranges are used, we now label them as illustrative, cite the source of the range, and require pilots to measure local baselines. If independent evidence for a specific numerical claim is not available, we treat it as preliminary/anecdotal and show how conclusions change under alternate assumptions (sensitivity analysis below).\n",
       "- Sensitivity analysis (simple scenarios for decision thresholds):\n",
       "  - Optimistic scenario: median task improvement = 20% (IQR ±10%), per‑transaction gross benefit = $2.50, inference cost = $0.30 → benefit margin supports full pilot/scale if payback <9 months.\n",
       "  - Pessimistic scenario: median task improvement = 5% (IQR ±5%), per‑transaction gross benefit = $0.60, inference cost = $0.30 → inference consumes >50% of gross benefit; route to smaller/hybrid models, stronger caching, or shelve until costs fall. (Use Wallaroo guidance on cost‑sensitive routing) (https://wallaroo.ai/cost-effective-deployment-of-large-llms-overcoming-infrastructure-constraints/).\n",
       "  - Operational rule: require sensitivity checks in the TCO model—if any realistic scenario projects inference cost >25% of gross benefit, classify as cost‑sensitive and prefer RAG, model routing, or specialist smaller models.\n",
       "Decision framework (operational evaluation criteria and owners)\n",
       "- Business impact threshold (owner: Product sponsor/CPO): prioritize candidates expected to deliver measurable outcome changes in 6–12 months. Suggested thresholds: median relative improvement >10% in task accuracy or >15% reduction in handling time, or clear revenue/cost impact with payback in 6–12 months. Require pre‑registered evaluation plans and power calculations before pilot launch.\n",
       "- Cost sensitivity band (owner: Finance + ML infra lead): compute conservative and optimistic per‑transaction inference and maintenance costs, plus storage and annotation labor. Classify use cases as “cost‑sensitive” if inference cost >25% of projected gross benefit and prefer hybrid or distilled solutions in that band (https://wallaroo.ai/cost-effective-deployment-of-large-llms-overcoming-infrastructure-constraints/).\n",
       "- Compliance readiness (owner: Legal/Privacy officer): require documented data classification, PII handling rules, privacy impact assessment, and measurable access controls before production gating.\n",
       "- Evaluation checklist (owner: Pilot lead): pre‑registered metrics and stop/go criteria (accuracy, hallucination rate, p95 latency, cost per successful transaction, human override rate, NPS/task success).\n",
       "Mapping risks to controls, SLO examples, and role ownership\n",
       "- Data leakage / privacy risk\n",
       "  - Controls: end‑to‑end data classification, PII masking, encryption, strict RBAC, and comprehensive logging.\n",
       "  - SLO & owner: 100% data sources classified (Data Owner), quarterly PII exposure audit with target 0 incidents (Privacy Officer).\n",
       "- Hallucination / incorrect output\n",
       "  - Controls: retrieval‑augmented grounding, explicit verification steps, HITL gating for high‑risk outputs, automated hallucination detectors.\n",
       "  - SLO & owner: sampled hallucination rate ≤1–3% with daily sampling and CI (Evaluation Team); automated low‑confidence flagging ≥95%.\n",
       "- Bias / fairness\n",
       "  - Controls: pre‑deployment fairness testing, continuous sampling, remediation workflows.\n",
       "  - SLO & owner: disparate impact ratios within policy bounds; quarterly fairness report (Compliance Lead).\n",
       "- Availability / latency\n",
       "  - Controls: model routing, caching, autoscaling, SRE runbooks.\n",
       "  - SLO & owner: p95 latency <200–500 ms for synchronous customer flows (SRE), jurisdictional SLAs met.\n",
       "- Compliance & auditability\n",
       "  - Controls: model provenance/versioning, tamper‑evident logs, policy dashboards.\n",
       "  - SLO & owner: 100% of production requests logged with model version and prompt template (MLOps), audits every quarter.\n",
       "Prioritized, time‑bound roadmap with role assignments and concrete checkpoints\n",
       "- Stage 0 — Discovery & Prioritization (0–2 months)\n",
       "  - Key owners: Head of AI/Product sponsor (exec), Data Owner, VP Engineering.\n",
       "  - Activities: cross‑functional inventory; score candidates by impact, risk, data readiness, integration complexity using a documented scorecard; collect baseline KPIs and instrumentation spec.\n",
       "  - Deliverables: prioritized backlog (3–6 candidates), baseline KPIs, pre‑registered evaluation plan, go/no‑go scorecard signed by Sponsor + Compliance.\n",
       "  - Success criteria: ≥1 candidate meets business‑impact threshold and data access confirmed.\n",
       "  - Example checkpoint (Day 60): signed scorecard and instrumentation deployed to capture baseline metrics.\n",
       "- Stage 1 — Pilot / MVP (3–6 months)\n",
       "  - Key owners: Product Owner (pilot), ML Engineer, MLOps/SRE, Data Engineer, Legal/Compliance stakeholder (0.2–0.5 FTE), Domain Reviewers.\n",
       "  - Scope: 1–3 narrow pilots on non‑sensitive data where possible (e.g., RAG for knowledge retrieval, secure doc summarization in a shadow environment).\n",
       "  - Objectives & stop/go criteria: validate statistically significant accuracy uplift vs baseline, hallucination below agreed threshold (e.g., ≤2%), acceptable p95 latency (<250 ms for synchronous flows where required), integration feasibility, user acceptance, and positive unit economics. Stop triggers: no measurable improvement after predefined traffic and time, hallucination above safety threshold, or cost per successful transaction >25% of projected benefit.\n",
       "  - Metrics (measured continuously): task accuracy with CI, hallucination/incorrect‑response rate with CI, user satisfaction (NPS or task success), p95 latency, cost per successful transaction, human override rate, ROI uplift.\n",
       "  - Milestone: statistically significant improvement OR documented remediation plan signed by Sponsor + Compliance.\n",
       "  - Operational gate before promotion: completed privacy impact assessment, data classification, RBAC configured, monitoring pipelines live, and legal sign‑off.\n",
       "- Stage 2 — Productionization (6–12 months)\n",
       "  - Key owners: MLOps Lead, SRE, Security/Compliance, Product Manager.\n",
       "  - Activities: harden CI/CD for models, canary deployments, logging and monitoring dashboards (accuracy, drift, hallucination, cost), HITL workflows, incident response runbooks, and quarterly recertification processes.\n",
       "  - Model strategy: prefer RAG or prompt‑tuning until domain data volume and ROI justify fine‑tuning; validate fine‑tuning ROI with a break‑even analysis before committing (https://lumenalta.com/insights/9-llm-enterprise-applications-advancements-in-2026-for-cios-and-ctos; https://www.calsoftinc.com/blog/enterprise-ai-fine-tuning-service-roi).\n",
       "  - Governance gate to production (required signoffs): documented data classification and allowed uses; model provenance/versioning; RBAC and audit trails; bias/safety validation within thresholds; dashboards live; incident playbook tested; legal/compliance sign‑off.\n",
       "  - Success criteria: steady‑state KPIs within targets for two consecutive quarters and cost per effective transaction below the threshold established in discovery.\n",
       "- Stage 3 — Scale & Optimize (12+ months)\n",
       "  - Key owners: Head of AI program, Infrastructure lead, Finance.\n",
       "  - Activities: apply cost levers (quantization/distillation, model routing, token limits, caching, hybrid cloud spot instances), expand reviewer pools for HITL, and evaluate custom/fine‑tuned models only when scale/risk justify the investment (https://wallaroo.ai/cost-effective-deployment-of-large-llms-overcoming-infrastructure-constraints/).\n",
       "  - Milestone: scaled production flows with monitored KPIs, cost per effective transaction within target, and governance processes operating at enterprise scale.\n",
       "Minimum production gate checklist (owners in parentheses)\n",
       "- Comprehensive data classification and PII controls; privacy impact assessment completed (Privacy). \n",
       "- RBAC, immutable logging, and model provenance/version control enabled (Security / MLOps).\n",
       "- Monitoring pipelines streaming accuracy, hallucination, drift, latency, and cost to dashboards (MLOps / Evaluation team).\n",
       "- Incident response and rollback playbooks tested in drills (SRE / Ops).\n",
       "- Legal/compliance sign‑off and regulatory tracking mechanisms active (Legal).\n",
       "Cost‑control levers and engineering tactics (applied examples)\n",
       "- Route high‑volume, low‑risk requests to smaller specialist models; reserve largest models for high‑value or high‑risk responses.\n",
       "- Apply quantization/distillation and response caching where acceptable to reduce inference costs (MLOps). \n",
       "- Use token limits, hybrid execution (sensitive workloads on‑prem), and cloud spot instances for non‑critical batch work.\n",
       "- Recompute TCO monthly and run sensitivity scenarios to detect regressions early (Finance + MLOps).\n",
       "Staffing, training, and RACI (explicit short‑term through scale)\n",
       "- Minimum pilot core team: 1 ML engineer / model infra, 1 MLOps/SRE, 1 data engineer, 1 product owner, legal/compliance stakeholder (0.2–0.5 FTE), and 1–2 domain reviewers. Central AI program manager (0.5–1 FTE) to coordinate cross‑functional execution (https://pcpl.io/every-organization-needs-an-ai-adoption-roadmap-in-2026/).\n",
       "- Scale staffing (12+ months): add ML research/ops, additional data engineers, a dedicated evaluation team for continuous measurement, and expanded reviewer pools for HITL.\n",
       "- Training priorities: prompt engineering best practices, evaluation methodology and power calculations, reviewer calibration, incident response drills, and annual recertification for compliance owners.\n",
       "- RACI example: Pilot decisions (R: Product Owner; A: Head of AI; C: Legal/Compliance; I: VP Engineering).\n",
       "Future research and monitoring priorities\n",
       "- Standardized cost/latency benchmarks across model families and clouds to improve TCO comparability.\n",
       "- Robust hallucination detection and calibration methods and human–AI collaboration studies to quantify long‑term benefits and risks.\n",
       "- Multimodal integration evaluation and regulatory monitoring as capabilities and rules evolve (https://www.sciencedirect.com/science/article/pii/S2666389925002144; https://www.rtslabs.com/enterprise-ai-roadmap/).\n",
       "How this fixes previous issues\n",
       "- Missing evidence: core claims about adoption and API demand are now explicitly cited (Hostinger/Iopex, typedef.ai, Gartner) and pilot fragility is referenced; benefit estimates are treated as distributions with mandatory local baselines and sensitivity checks (https://www.hostinger.com/tutorials/llm-statistics; https://www.typedef.ai/resources/llm-adoption-statistics; https://www.gartner.com/en/newsroom/press-releases/2024-03-20-gartner-predicts-more-than-30-percent-of-the-increase-in-demand-for-apis-will-come-from-ai-and-tools-using-llms-by-2026).\n",
       "- Coherence and operationalization: the roadmap is expanded with explicit role assignments, time‑boxed checkpoints, numeric SLO examples, pre‑registered evaluation plans, and clear stop/go criteria to make the guidance actionable for business leaders, engineering teams, and risk/compliance owners.\n",
       "Practical next step\n",
       "- Convert this roadmap into pilot checklists, metric dashboard templates, and a vendor/comparison rubric (next deliverable) so teams can execute pilots repeatably and measure outcomes against the documented SLOs. With a validated, evidence‑backed roadmap, concrete SLOs, role assignments, and sensitivity checks established, the next deliverable will operationalize these elements into pilot checklists, metric dashboards, and vendor/comparison rubrics to enable repeatable execution and program governance.\n",
       "\n",
       "## References\n",
       "\n",
       "1. https://developer.nvidia.com/blog/llm-inference-benchmarking-how-much-does-your-llm-inference-cost/\n",
       "2. https://a16z.com/generative-ai-enterprise-2024/\n",
       "3. https://www.youtube.com/watch?v=K330W632TYY\n",
       "4. https://medium.com/@serdargoksu/the-3-biggest-risks-of-llms-going-into-2026-hallucination-hidden-costs-and-data-leakage-f952fcb94506\n",
       "5. https://wizr.ai/blog/large-language-models-transform-enterprise-workflows/\n",
       "6. https://xenoss.io/blog/total-cost-of-ownership-for-enterprise-ai\n",
       "7. https://docs.anyscale.com/llm/serving/benchmarking/metrics\n",
       "8. https://www.searchunify.com/resource-center/blog/building-the-business-case-for-advanced-llm-adoption-in-technical-support-a-roadmap-to-roi\n",
       "9. https://www.calsoftinc.com/blog/enterprise-ai-fine-tuning-service-roi\n",
       "10. https://www.llm-prices.com/\n",
       "11. https://support.microsoft.com/en-us\n",
       "12. https://medium.com/@puttt.spl/ai-performance-engineering-how-to-test-latency-throughput-cost-in-llm-pipelines-3b79d1e4de5a\n",
       "13. https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices\n",
       "14. https://www.lasso.security/blog/llm-data-privacy\n",
       "15. https://quantumzeitgeist.com/efficient-llm-inference-achieves-speedup/\n",
       "16. https://en.wikipedia.org/wiki/Attention\n",
       "17. https://research.aimultiple.com/llm-pricing/\n",
       "18. https://arxiv.org/html/2508.03860v1\n",
       "19. https://deepsense.ai/blog/llm-inference-optimization-how-to-speed-up-cut-costs-and-scale-ai-models/\n",
       "20. https://cookie-script.com/news/data-privacy-trends-2026\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(result[\"output\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation Harness Integration\n",
    "\n",
    "Once the manual test passes, uncomment and run the cells below for full evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation harness and metrics\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from evaluation import (\n",
    "    ExperimentHarness, \n",
    "    fact_recall, \n",
    "    citation_precision,\n",
    "    coherence_judge, \n",
    "    depth_judge, \n",
    "    relevance_judge,\n",
    "    minimum_sources_check\n",
    ")\n",
    "\n",
    "# Initialize harness with the golden test dataset\n",
    "harness = ExperimentHarness(\n",
    "    dataset_path=\"../data/deep_research_agent_test_dataset.yaml\",\n",
    "    langsmith_dataset_name=\"deep-research-golden-v2\"\n",
    ")\n",
    "\n",
    "print(\"Evaluation harness initialized successfully!\")\n",
    "print(f\"Dataset: {harness.dataset_path}\")\n",
    "print(f\"LangSmith dataset name: {harness.langsmith_dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Evaluation on All 20 Questions\n",
    "# Uncomment to run:\n",
    "\n",
    "# # Define comprehensive evaluator suite\n",
    "# evaluators = [\n",
    "#     fact_recall,              # Required facts coverage\n",
    "#     citation_precision,       # Citation URL validity\n",
    "#     minimum_sources_check,    # Minimum source count\n",
    "#     coherence_judge,          # Logical structure\n",
    "#     depth_judge,              # Analysis depth\n",
    "#     relevance_judge,          # Addresses question\n",
    "# ]\n",
    "# \n",
    "# # Run full evaluation\n",
    "# print(\"Starting FULL evaluation on all 20 questions...\")\n",
    "# print(\"Patch-Based Iterative Refinement Agent - this will take 1-2 hours.\")\n",
    "# print(\"=\" * 80 + \"\\n\")\n",
    "# \n",
    "# results = harness.run_evaluation(\n",
    "#     agent_fn=patch_refinement_agent,\n",
    "#     evaluators=evaluators,\n",
    "#     experiment_name=\"patch_refinement_v1\",\n",
    "#     monte_carlo_runs=1,  # Single run to reduce cost\n",
    "#     max_concurrency=2,   # Lower concurrency for stability\n",
    "#     description=\"Patch-Based Iterative Refinement paradigm evaluation\"\n",
    "# )\n",
    "# \n",
    "# # Display comprehensive results\n",
    "# print(\"\\n\" + \"=\" * 80)\n",
    "# print(\"FULL EVALUATION RESULTS\")\n",
    "# print(\"=\" * 80)\n",
    "# print(f\"Experiment: {results.experiment_name}\")\n",
    "# print(f\"Questions evaluated: {results.num_questions}\")\n",
    "# print(f\"Runs per question: {results.num_runs}\")\n",
    "# \n",
    "# print(f\"\\n{'Metric':<30} {'Mean':<10}\")\n",
    "# print(\"-\" * 40)\n",
    "# for metric_name in sorted(results.metrics.keys()):\n",
    "#     if not metric_name.endswith('_std'):\n",
    "#         value = results.metrics.get(metric_name, 0)\n",
    "#         print(f\"{metric_name:<30} {value:<10.3f}\")\n",
    "# \n",
    "# # Save results to file\n",
    "# import json\n",
    "# from datetime import datetime\n",
    "# \n",
    "# results_file = Path(\"../results\") / f\"patch_refinement_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "# results_file.parent.mkdir(exist_ok=True)\n",
    "# \n",
    "# with open(results_file, 'w') as f:\n",
    "#     json.dump({\n",
    "#         \"experiment_name\": results.experiment_name,\n",
    "#         \"num_questions\": results.num_questions,\n",
    "#         \"num_runs\": results.num_runs,\n",
    "#         \"metrics\": results.metrics,\n",
    "#         \"per_question\": results.per_question_results\n",
    "#     }, f, indent=2)\n",
    "# \n",
    "# print(f\"\\nResults saved to: {results_file}\")\n",
    "\n",
    "print(\"Full evaluation cell ready. Uncomment to run when ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Architecture Comparison\n",
    "\n",
    "### Original Approach (Full Regeneration)\n",
    "- Generate complete draft (~1500 tokens)\n",
    "- Critique and identify issues\n",
    "- Regenerate ENTIRE draft (~1500 tokens) per iteration\n",
    "- Token cost: ~1500 + 1500*N iterations\n",
    "\n",
    "### Patch-Based Approach\n",
    "- Generate skeleton (~300 tokens)\n",
    "- Expand each node (~300 tokens * 5-7 nodes = ~1800 tokens)\n",
    "- Critique identifies specific nodes needing fixes\n",
    "- Patch ONLY affected nodes (~300 tokens per patched node)\n",
    "- Token cost: 300 + 1800 + 300*patches per iteration\n",
    "\n",
    "### Benefits\n",
    "1. **Longer documents**: No output token limit since we generate in chunks\n",
    "2. **Token efficiency**: If 2/6 nodes need fixing, we regenerate ~600 tokens instead of 1800\n",
    "3. **Better targeting**: Claims registry tracks exactly what needs verification\n",
    "4. **Coherence**: Bridge sentences ensure smooth transitions\n",
    "5. **Transparency**: Skeleton shows document structure, claims show verification status"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
